#' Helper function for cleanbatch to identify subset of observations that are either "included" or a "temporary duplicate"
#'
#' @keywords internal
#' @noRd
valid <- function(df,
                  include.temporary.duplicates = F,
                  include.duplicates = F,
                  include.carryforward = F) {
  exclude <- if (is.data.frame(df))
    df$exclude
  else
    df
  return(
    exclude < 'Exclude'
    |
      include.temporary.duplicates &
      exclude == 'Exclude-Temporary-Duplicate'
    | include.duplicates & exclude == 'Exclude-Duplicate'
    |
      include.carryforward &
      exclude == 'Exclude-Carried-Forward'
  )
}

#' Helper function to treat NA values as FALSE
#'
#' @keywords internal
#' @noRd
na_as_false <- function(v) {
  v[is.na(v)] <- F
  v
}

#' Function for temporary duplicates (step 5):
#' 5.  Temporary duplicates: I use duplicates to refer to more than more than one recorded value for a parameter on the same day,
#'     and we need to select which one to include in our analysis. The overall strategy will be to select a measurement using a simple
#'     strategy that will be used temporarily, and select permanently in a later step after we have a somewhat cleaner dataset that
#'     can help us identify the best duplicate.
#' a.  For subjects/parameters with duplicates: Determine median_tbc*sd for both parameters: the median tbc*sd for each subject
#'     and parameter including only non-duplicate values with exc_*==0. The median of the same parameter as the duplicate will
#'     be referred to as median_tbc*sd, the median of the other parameter will be referred to as median_tbcOsd.
#' b.	For each subject/parameter with duplicates and at least one value for the subject/parameter on a day with no duplicates,
#'     select the value closest to the median_tbc*sd for temporary inclusion, and assign all other duplicates exc_*=2.
#'  i.	For each subject/parameter with duplicates and no values for the subject/parameter on a day with no duplicates, select the
#'     value closest to the median_tbcOsd for temporary inclusion, and assign all other duplicates exc_*=2.
#'     If median_tbcOsd is missing because there are no values for the other parameter, randomly choose one duplicate value for
#'     each subject/parameter/age to keep as exc_*=0 and replace exc_*=2 for all other duplicates for that subject/parameter/age.
#'
#' @keywords internal
#' @noRd
temporary_duplicates <- function(df) {
  # add subjid and param if needed (may be missing depending on where this is called from)
  if (is.null(df$subjid))
    df[, subjid := NA]
  if (is.null(df$param))
    df[, param := NA]
  # only operate on valid rows (but include rows that may have previously been flagged as a "temporary duplicate")
  valid.rows <- valid(df, include.temporary.duplicates = T)
  # make a small copy of df with only the fields we need for efficiency
  df <- df[j = .(tbc.sd), keyby = .(subjid, param, agedays, index)]
  # initialize some useful fields
  df[, `:=`(
    median.sd = as.double(NA),
    delta.median.sd = as.double(NA),
    duplicates.this.day = F,
    duplicate = F
  )]
  # determine on which days there are duplicate measurements (more than 1 valid measurement on that age day)
  df[valid.rows, duplicates.this.day := (.N > 1), by = .(subjid, param, agedays)]
  # calculate median of measurements on days where there is no duplicate
  df[valid.rows &
       !duplicates.this.day, median.sd := median(tbc.sd), by = .(subjid, param)]
  # distribute median to other valid or potential duplicate rows for same parameter for that subject
  df[valid.rows, median.sd := sort(median.sd)[1], by = .(subjid, param)]
  # take care of subject/parameters with more than one day with a valid observation
  # determine the absolute difference between the measurements sd score and the median for that parameter for each child
  df[valid.rows &
       duplicates.this.day, delta.median.sd := abs(tbc.sd - median.sd), by = .(subjid, param)]
  # identify subjects that have duplicates on all days of observation for that parameter (i.e. delta.median.sd undefined)
  subj.all.dups <- df[valid.rows &
                        duplicates.this.day &
                        is.na(delta.median.sd), unique(subjid)]
  df[valid.rows &
       subjid %in% subj.all.dups, delta.median.sd := (function(subj.df) {
         # iterate over parameters where delta.median.sd is not yet defined
         # pass 1: take median from other parameter(s)
         for (p in subj.df[is.na(delta.median.sd) &
                           duplicates.this.day, unique(param)]) {
           median.other.sd <- subj.df[param != p &
                                        !duplicates.this.day, median(tbc.sd)]
           subj.df[param == p, delta.median.sd := abs(tbc.sd - median.other.sd)]
         }
         return(subj.df$delta.median.sd)
       })(copy(.SD)), by = .(subjid)]
  # Final pass: take median as zero (i.e. if no measurements from a different parameter)
  # NOTE -- this is not exactly the same as taking a random parameter
  df[valid.rows &
       duplicates.this.day &
       is.na(delta.median.sd), delta.median.sd := abs(tbc.sd)]
  # flag any duplicate value on the same day that is not the minimum distance from the median sd score
  # NOTE: in the case of exact ducplicates, "which.min" will pick the first
  df[valid.rows &
       duplicates.this.day, duplicate := seq_along(delta.median.sd) != which.min(delta.median.sd), by =
       .(subjid, param, agedays)]
  # return the duplicated valid rows (i.e. the ones that should be temporarily excluded)
  return(df$duplicate & valid.rows)
}

#' Function to identify switches (step 6):
#'  7.  Identify switches (weight and height each recorded as the other parameter)
#' Utility function to find the value associated with the opposite parameter
#' For example, if param.1='WEIGHTKG' and param.2='HEIGHTCM', then a vector of height values observed on the same day as the weight values is returned and vice versa.
#'
#' @keywords internal
#' @noRd
swap_parameters <- function(param.1 = 'WEIGHTKG',
                            param.2 = 'HEIGHTCM',
                            field.name = 'tbc.sd',
                            df) {
  valid.rows <- valid(df)
  # copy swap field to a new value for convenience in the code below
  df$swap <- df[, field.name, with = F]
  # construct an indexed table with valid parameters for speed in swapping
  valid.other <- df[valid.rows, list(
    subjid.other = subjid,
    param.other = ifelse(
      param == param.1,
      param.2,
      ifelse(param == param.2, param.1, NA)
    ),
    agedays.other = agedays,
    swap
  )]
  setkey(valid.other, subjid.other, param.other, agedays.other)
  # clear swap field (should retain original type)
  df[, swap := NA]
  # use indexed table for speed -- assign all values in one statement
  df[valid.rows, swap := valid.other[list(subjid, param, agedays), swap]]
  return(df$swap)
}

#' Function to clean data (optionally in batches):
#' 4.  Dataset split (optional)
#' a.	In Stata, many of the rest of the steps require you to repeat steps as many times as there are observations for the subject with the highest number of
#'     observations. Therefore, in order to speed things up I separated the dataset at this point into three groups: short (subjects with <=10 total
#'     observations), medium (subjects with >10 and <=30 observations) and long (subjects with >30 observations). I performed all the rest of the steps
#'     separately for each of the 3 datasets and then re-combined them at the end. Note that all observations for a subject stay together. This step is
#'     definitely not necessary if it will not speed things up in whatever software program you're using.
#'
#' NOTE: to use mutliple processes in R we will process patients in batches using the plyr ddply function
#' set up function to process one patient.  Function to parallelize batches is below.
#'
#' @keywords internal
#' @noRd
cleanbatch <- function(data.df,
                       log.path,
                       quietly,
                       parallel,
                       measurement.to.z,
                       ewma.fields,
                       ewma.exp,
                       recover.unit.error,
                       include.carryforward,
                       sd.extreme,
                       z.extreme,
                       exclude.levels,
                       tanner.ht.vel,
                       who.ht.vel,
                       lt3.exclude.mode,
                       error.load.threshold,
                       error.load.mincount) {
  data.df <- data.table(data.df, key = c('subjid', 'param', 'agedays', 'index'))

  if (!quietly & parallel) {
    # use local directory as default for logs
    if (is.na(log.path)) {
      log.path <- "."
    }
    sink(
      sprintf(
        "%s/cleangrowth-%s-batch-%s.log",
        log.path,
        Sys.Date(),
        data.df$batch[1]
      )
    )
  }

  if (!quietly)
    cat(sprintf(
      "[%s] Processing Batch #%s...\n",
      Sys.time(),
      data.df$batch[1]
    ))

  # save a copy of all original measurement values before any transformation
  data.df[, v.orig := v]

  if (!quietly)
    cat(sprintf(
      "[%s] Preliminarily identify potential duplicates...\n",
      Sys.time()
    ))
  data.df$exclude[temporary_duplicates(data.df)] <- 'Exclude-Temporary-Duplicate'

  # capture a list of subjects with possible duplicates for efficiency later
  subj.dup <- data.df[exclude == 'Exclude-Temporary-Duplicate', unique(subjid)]

  # 7a.  For each day on which a subject had both a weight and a height recorded, calculate tbc*sd_sw: SD scores as if the weight had been recorded as the height
  #      and the height had been recorded as the weight, recentered using rcsd_*.  I intentionally did not allow values that were the first or last for a subject/parameter to be replaced as a switch.
  # NOTE: this additional constraint related to first and last values is implemented below via the code
  # swap.flag.1 := if(.N>2) c(F, rep(T, .N-2), F) else F, by=.(subjid,param)

  if (!quietly)
    cat(sprintf(
      "[%s] Identify potentially swapped measurements...\n",
      Sys.time()
    ))

  data.df[, v.sw := swap_parameters(field.name = 'v', df = data.df)]
  # calculate "standard deviation" score for the "swapped" parameter and recenter
  data.df[, tbc.sd.sw := measurement.to.z(param, agedays, sex, v.sw, T) - sd.median]

  # 7b.  Perform a EWMA calculation.
  #   i.	In addition to the standard all/bef/aft dewma_* variables calculate all/bef/aft dewma_*_sw variables by subtracting EWMASD from tbc*sd_sw
  data.df[, (ewma.fields) := as.double(NaN)]
  data.df[valid(exclude), (ewma.fields) := ewma(agedays, tbc.sd, ewma.exp, T), by =
            .(subjid, param)]


  # 7c.  Label pairs of height/weight measurements recorded on the same day as a switch if ALL of the following criteria are met for BOTH parameters:
  #   i.	For the weight value:
  #       exc_wt==0 & tbcwtsd>4 & |tbcwtsd_sw|<3 & dewma_wt>3 & dewma_wt_bef>2 & dewma_wt_aft>2
  #         & |dewma_wt_sw|<0.3 & |dewma_wt_sw_bef|<0.5 & |dewma_wt_sw_aft|<0.5
  #   ii.	For the height value:
  #       exc_ht==0 & tbchtsd<-7 & |tbchtsd_sw|<3 & dewma_ht<-6 & dewma_ht_bef<-5 & dewma_ht_aft<-5 & |dewma_ht_sw|<0.3 & |dewma_ht_sw_bef|<0.5 & |dewma_ht_sw_aft|<0.5
  #

  # initialize swap.flag.1
  data.df[, `:=`(valid.interior.measurement = F,
                 swap.flag.1 = F)]

  # flag interior measurements
  data.df[valid(data.df), valid.interior.measurement := if (.N > 2)
    c(F, rep(T, .N - 2), F)
    else
      F, by = .(subjid, param)]
  data.df[valid.interior.measurement &
            (
              param == 'WEIGHTKG' &
                tbc.sd > 4 &
                abs(tbc.sd.sw) < 3 &
                (tbc.sd - ewma.all) > 3 &
                (tbc.sd - ewma.before) > 2 &
                (tbc.sd - ewma.after) > 2
              &
                abs(tbc.sd.sw - ewma.all) < 0.3 &
                abs(tbc.sd.sw - ewma.before) < 0.5 &
                abs(tbc.sd.sw - ewma.after) < 0.5
              |
                param == 'HEIGHTCM' &
                tbc.sd < -7 &
                abs(tbc.sd.sw) < 3 &
                (tbc.sd - ewma.all) < -6 &
                (tbc.sd - ewma.before) < -5 &
                (tbc.sd - ewma.after) < -5
              &
                abs(tbc.sd.sw - ewma.all) < 0.3 &
                abs(tbc.sd.sw - ewma.before) < 0.5 &
                abs(tbc.sd.sw - ewma.after) < 0.5
            ), swap.flag.1 := T]


  # 7d.  For pairs of measurements that meet criteria for a switch, do the following:
  #   i.	Replace wt with the value that was originally recorded as the ht, and replace ht with the value that was originally recorded as the wt
  #   ii.	Replace tbc*sd with the values for tbc*sd_sw
  data.df$swap.flag.2 <- swap_parameters(field.name = 'swap.flag.1', df = data.df)
  data.df[swap.flag.1 &
            swap.flag.2, `:=`(v = v.sw,
                              tbc.sd = tbc.sd.sw,
                              exclude = 'Swapped-Measurements')]

  # look for possible unit errors
  # calculate additional z-scores if we will be attempting to correct unit errors
  if (recover.unit.error) {
    # 8.  Identify unit errors (weight/height recorded in wrong units)
    # a.  Generate variables transformed by conversion factors for kg/lbs and inches/cm:
    #    i.	  wt_d_2=wt/2.204622
    #    ii.	wt_t_2=wt*2.204622
    #    iii.	ht_d_2=ht/2.54
    #    iv.	ht_t_2=ht*2.54
    if (!quietly)
      cat(sprintf("[%s] Identify and recover unit errors...\n", Sys.time()))
    valid.rows <- valid(data.df)
    data.df[, `:=`(
      v.d = v / ifelse(param == 'WEIGHTKG', 2.204622, 2.54),
      v.t = v * ifelse(param == 'WEIGHTKG', 2.204622, 2.54)
    )]

    # 8b.  Calculate SD scores for each transformed variable and recenter these (tbc*sd_d_2 and tbc_*sd_t_2)
    data.df$tbc.sd.d <- with(data.df,
                             measurement.to.z(param, agedays, sex, v.d, T) - sd.median)
    data.df$tbc.sd.t <- with(data.df,
                             measurement.to.z(param, agedays, sex, v.t, T) - sd.median)

    # 8c.  Perform a EWMA calculation
    #    i.	  In addition to the standard all/bef/aft dewma_* variables calculate all/bef/aft variables for  dewma_*_d_2 and dewma_*_t_2  by subtracting EWMASD from
    #         tbc*sd_d_2 and tbc_*_sd_t_2
    # NOTE: the additional dewma fields are calculated in the tests below
    data.df[, (ewma.fields) := as.double(NaN)]
    data.df[valid(exclude), (ewma.fields) := ewma(agedays, tbc.sd, ewma.exp, T), by =
              .(subjid, param)]

    # 8d.  Also calculate d_prevsd_*=tbc*sd-tbc*sdprev and d_nextsd_*=tbc*sdi-tbc*sdnext. d_prevsd_* will be missing for the first value and d_nextsd_* should be
    #     missing for the last value. I intentionally did not allow values that were the first or last for a subject/parameter to be replaced as unit errors.
    # structure c(NA, field.name[-.N]) == get.prev
    # structure c(field.name[-1], NA) == get.next
    data.df[, `:=`(delta.prev.sd = as.double(NaN),
                   delta.next.sd = as.double(NaN))]
    data.df[valid.rows, `:=`(
      delta.prev.sd = tbc.sd - c(NA, tbc.sd[-.N]),
      delta.next.sd = tbc.sd - c(tbc.sd[-1], NA)
    ), by = .(subjid, param)]

    # 8e.	Identify a value as a unit error if one of the following sets of criteria are met:
    #    i.  	For wt_d_2: dewma_wt>3 & dewma_wt_bef>2 & dewma_wt_aft>2 & tbcwtsd>3 & d_nextsd_wt>2 & d_nextsd_wt is not missing & d_prevsd_wt>2 & d_prevsd_wt is not missing
    #        & abs(dewma_wt_d_2)<0.3 & abs(dewma_wt_d_2_bef)<0.5 & abs(dewma_wt_d_2_aft)<0.5 & abs(tbcwtsd_d_2)<3 & exc_wt==0
    #    ii.	For wt_t_2: dewma_wt<-3 & dewma_wt_bef<-2 & dewma_wt_aft<-2 & tbcwtsd<-3 & d_nextsd_wt<-2 & d_nextsd_wt is not missing & d_prevsd_wt<-2 & d_prevsd_wt is not missing
    #        & abs(dewma_wt_t_2)<0.3 & abs(dewma_wt_t_2_bef)<0.5 & abs(dewma_wt_t_2_aft)<0.5 & abs(tbcwtsd_t_2)<3 & exc_wt==0
    #    iii.	For ht_d_2: dewma_ht>5 & dewma_ht_bef>4 & dewma_ht_aft>4 & tbchtsd>7 & d_nextsd_ht>4 & d_nextsd_ht is not missing & d_prevsd_ht>4 & d_prevsd_ht is not missing
    #        & abs(dewma_ht_d_2)<0.3 & abs(dewma_ht_d_2_bef)<0.5 & abs(dewma_ht_d_2_aft)<0.5 & abs(tbchtsd_d_2)<3 & exc_ht==0
    #    iv.	For ht_t_2: dewma_ht<-5 & dewma_ht_bef<-4 & dewma_ht_aft<-4 & tbchtsd<-7 & d_nextsd_ht<-4 & d_nextsd_ht is not missing & d_prevsd_ht<-4 & d_prevsd_ht is not missing
    #        & abs(dewma_ht_t_2)<0.3 & abs(dewma_ht_t_2_bef)<0.5 & abs(dewma_ht_t_2_aft)<0.5 & abs(tbchtsd_t_2)<3 & exc_ht==0
    # f.  For values that are identified as unit errors by one of the sets of criteria above, replace wt or ht with the corresponding transformed value and
    #     replace tbc*sd with the recentered sd-score for the transformed value

    # process unit error low first (8.e.i and iii from above), re-factor slightly for efficiency
    data.df[valid(data.df) &
              abs(tbc.sd.d - ewma.all) < 0.3 &
              abs(tbc.sd.d - ewma.before) < 0.5 &
              abs(tbc.sd.d - ewma.after) < 0.5 & abs(tbc.sd.d) < 3
            &
              (
                param == 'WEIGHTKG' &
                  (tbc.sd - ewma.all) > 3 &
                  (tbc.sd - ewma.before) > 2 &
                  (tbc.sd - ewma.after) > 2 & tbc.sd > 3
                &
                  !is.na(delta.next.sd) &
                  delta.next.sd > 2 &
                  !is.na(delta.prev.sd) & delta.prev.sd > 2
                |
                  param == 'HEIGHTCM' &
                  (tbc.sd - ewma.all) > 5 &
                  (tbc.sd - ewma.before) > 4 &
                  (tbc.sd - ewma.after) > 4 & tbc.sd > 7
                &
                  !is.na(delta.next.sd) &
                  delta.next.sd > 4 &
                  !is.na(delta.prev.sd) & delta.prev.sd > 4
              ),
            `:=`(v = v.d,
                 tbc.sd = tbc.sd.d,
                 exclude = 'Unit-Error-High')]

    # process unit error high second (8.e.ii and iv from above), re-factor slightly for efficiency
    data.df[valid(data.df) &
              abs(tbc.sd.t - ewma.all) < 0.3 &
              abs(tbc.sd.t - ewma.before) < 0.5 &
              abs(tbc.sd.t - ewma.after) < 0.5 & abs(tbc.sd.t) < 3
            &
              (
                param == 'WEIGHTKG' &
                  (tbc.sd - ewma.all) < -3 &
                  (tbc.sd - ewma.before) < -2 &
                  (tbc.sd - ewma.after) < -2 & tbc.sd < -3
                &
                  !is.na(delta.next.sd) &
                  delta.next.sd < -2 &
                  !is.na(delta.prev.sd) & delta.prev.sd < -2
                |
                  param == 'HEIGHTCM' &
                  (tbc.sd - ewma.all) < -5 &
                  (tbc.sd - ewma.before) < -4 &
                  (tbc.sd - ewma.after) < -4 & tbc.sd < -7
                &
                  !is.na(delta.next.sd) &
                  delta.next.sd < -4 &
                  !is.na(delta.prev.sd) & delta.prev.sd < -4
              ),
            `:=`(v = v.t,
                 tbc.sd = tbc.sd.t,
                 exclude = 'Unit-Error-Low')]
  }

  # 9.  Exclude values that are carried forward. For the purposes of this analysis, any value that is identical to the preceding value for the same parameter
  #     and subject is considered carried forward. The chances of having identical measurements, even at an age/interval when little or no growth would be expected,
  #     is fairly small, and when this is the case the carried forward measurements provide little new information.
  # a.	Calculate d_prev_wt=wt-wtprev and d_prev_ht=ht-htprev. Use original measurements rather than transformed
  #     measurements (unit errors and switches).
  # b.	Unlike most steps, do this step for all duplicate values (exc_*==2) in addition to included values (exc_*==0), comparing all values for one day to all
  #     values from the prior day â€“ if there are any values with a d_prev*==0, the value on the latter day should be excluded.
  # c.	Replace exc_*=3 for all values with d_prev*==0 & (exc_*==0 OR exc_*==2)
  if (!include.carryforward) {
    if (!quietly)
      cat(sprintf(
        "[%s] Exclude measurements carried forward...\n",
        Sys.time()
      ))
    # for efficiency, bring get.prev and get.next inline here (working on valid rows within a single parameter for a single subject)
    # structure c(NA, field.name[-.N]) == get.prev
    data.df[, prev.v := as.double(NaN)]
    data.df[valid(data.df), prev.v := c(NA, v.orig[-.N]), by = .(subjid, param)]

    # optimize "carry forward" for children without duplicates.
    data.df[!(subjid %in% subj.dup) &
              v.orig == prev.v, exclude := 'Exclude-Carried-Forward']

    # need to handle children with duplicate measurements on same day separately
    data.df[subjid %in% subj.dup &
              valid(data.df, include.temporary.duplicates = T), exclude := (function(df) {
                setkey(df, agedays)
                ages = unique(agedays)
                # no point in looking for measurements carried forward if all measurements are from a single day of life
                if (length(ages) > 1) {
                  # iterate over each age
                  for (i in 2:length(ages)) {
                    # find the set of measurements from the previous age in days
                    all.prev.v = df[agedays == ages[i - 1], v.orig]
                    # if a measurement for the current age is found in the set of measurements from the previous age, then mark it as carried forward
                    df[agedays == ages[i] &
                         v.orig %in% all.prev.v, exclude := 'Exclude-Carried-Forward']
                  }
                }
                return(df$exclude)
              })(copy(.SD)), .SDcols = c('agedays', 'exclude', 'v.orig'), by = .(subjid, param)]
  }

  # 9d.  Replace exc_*=0 if exc_*==2 & redo step 5 (temporary duplicates)
  data.df[exclude == 'Exclude-Temporary-Duplicate', exclude := 'Include']
  data.df[temporary_duplicates(data.df), exclude := 'Exclude-Temporary-Duplicate']

  # 10.  Exclude extreme errors with SD cutoffs. For this, a cutoff of |SD|>25 is used. Because of differences in SD and z score, there are some very extreme values
  #      with a |z|>25 that are implausible with an |SD|<25, so both are used to exclude extreme errors. This works better than using a lower value for the limit
  #      for |SD|.
  # a.	Generally we only evaluate measurements where exc_*==0, but for this step we also need to evaluate measurements with exc_*==2
  # b.	Replace exc_*=4 if |tbc*sd|>25 & (exc_*==0 OR exc_*==2)
  # c.	Replace exc_*=4 if |*z|>25 & (exc_*==0 OR exc_*==2) & the value is not switched or transformed.
  if (!quietly)
    cat(sprintf(
      "[%s] Exclude extreme measurements based on SD...\n",
      Sys.time()
    ))
  data.df[na_as_false(
    valid(data.df, include.temporary.duplicates = T) & abs(tbc.sd) > sd.extreme
    |
      exclude %in% c('Include', 'Exclude-Temporary-Duplicate') &
      abs(z.orig) > z.extreme
  ),
  exclude := 'Exclude-SD-Cutoff']

  # 10d. Redo temporary duplicates as in step 5.
  data.df[exclude == 'Exclude-Temporary-Duplicate', exclude := 'Include']
  data.df[temporary_duplicates(data.df), exclude := 'Exclude-Temporary-Duplicate']

  # 11.  Exclude extreme errors with EWMA
  # a.	Erroneous measurements can distort the EWMA for measurements around them. Therefore, if the EWMA method identifies more than one value for a subject and
  #     parameter that meets criteria for exclusion, we will only exclude the value that deviates the most from expected in any given step. Then we will repeat the
  #     entire process until no measurements are identified that meet criteria for exclusion.
  # b.	Perform a EWMA calculation
  #   i.	Only use values where exc_*==0 to determine the EWMAs. However, calculate dewma_* variables for values where exc_*==0 or exc_*==2
  if (!quietly)
    cat(sprintf(
      "[%s] Exclude extreme measurements based on EWMA...\n",
      Sys.time()
    ))
  data.df <- data.df[, exclude := (function(df) {
    num.ewma.excluded <- 0
    # optimization: determine whether this subject has any duplicates
    has.duplicates <- subjid %in% subj.dup
    while (T) {
      df[, (ewma.fields) := as.double(NaN)]
      df[valid(exclude), (ewma.fields) := ewma(agedays, tbc.sd, ewma.exp, T)]

      # note: at this point, only one ewma exists per param on a given day for a subject, so sort(ewma.all)[1] will returns the non-missing ewma.all
      # restrict to children with possible duplicates for efficiency
      if (has.duplicates) {
        df[, `:=`(
          ewma.all = sort(ewma.all)[1],
          ewma.before = sort(ewma.before)[1],
          ewma.after = sort(ewma.after)[1]
        ), by = .(agedays)]
      }
      df[, `:=`(
        dewma.all = tbc.sd - ewma.all,
        dewma.before = tbc.sd - ewma.before,
        dewma.after = tbc.sd - ewma.after
      )]

      # 11c.  Identify all values that meet all of the following criteria as potential exclusions:
      #   i.	There are 3 or more measurements for that subject and parameter
      #   ii.	(dewma_*>3.5 & dewma_*_bef>3 & dewma_*_aft>3 & tbc*sd>3.5) OR (dewma_*<-3.5 & dewma_*_bef<-3 & d & dewma_*_aft<-3 & tbc*sd<-3.5)
      #   iii.	exc_*==0

      num.valid <- sum(valid(df))
      rep <- na_as_false(with(
        df,
        num.valid >= 3 & valid(df)
        &
          (
            dewma.all > 3.5 & dewma.before > 3 & dewma.after > 3 & tbc.sd > 3.5
            |
              dewma.all < -3.5 &
              dewma.before < -3 & dewma.after < -3 & tbc.sd < -3.5
          )
      ))
      num.exclude <- sum(rep)
      # 11d.  If there is only one potential exclusion identified in step 11c for a subject and parameter, replace exc_*=5 for that value
      if (num.exclude == 1)
        df[rep, exclude := 'Exclude-EWMA-Extreme']
      # 11e.  If there is more than one potential exclusion identified in step 11c for a subject and parameter, calculate abssum_*=|tbc*sd+dewma_*| for each exclusion and
      #     replace exc_*=5 for the value with the highest abssum_*
      if (num.exclude > 1) {
        # first order by decreasing abssum
        worst.row <- with(df, order(rep, abs(tbc.sd + (
          tbc.sd - ewma.all
        )), decreasing = T))[1]
        df[worst.row, exclude := 'Exclude-EWMA-Extreme']
      }

      # 11f.  For subjects/parameters with only 2 values, calculate abstbc*sd=|tbc*sd|
      # g.  Replace exc_*=6 for values that meet all of the following criteria
      #   i.	There are 2 measurements for that subject and parameter
      #   ii.	(dewma_*>3.5 & tbc*sd>3.5) OR (dewma_*<-3.5 & tbc*sd<-3.5)
      #   iii.	If there are 2 measurements for a subject/parameter that meet criteria ii, only replace exc_*=6 for the value with the larger abstbc*sd
      rep <- na_as_false(with(
        df,
        num.valid == 2 &
          (
            tbc.sd - ewma.all > 3.5 &
              tbc.sd > 3.5 |
              tbc.sd - ewma.all < -3.5 & tbc.sd < -3.5
          )
      ))
      num.exclude <- sum(rep)
      if (num.exclude == 1)
        df[rep, exclude := 'Exclude-EWMA-Extreme-Pair']
      if (num.exclude > 1) {
        # first order by decreasing abssum
        worst.row <- with(df, order(rep, abs(tbc.sd), decreasing = T))[1]
        df[worst.row, exclude := 'Exclude-EWMA-Extreme-Pair']
      }

      # 11h.  Recalculate temporary duplicates as in step 5
      # optimize: only perform these steps if this subject is known to have duplicate measurements
      if (has.duplicates) {
        df[exclude == 'Exclude-Temporary-Duplicate', exclude := 'Include']
        df[temporary_duplicates(df), exclude := 'Exclude-Temporary-Duplicate']
      }

      # 11i.  If there was at least one subject who had a potential exclusion identified in step 11c, repeat steps 11b-11g. If there were no subjects with potential
      #     exclusions identified in step 11c, move on to step 12.
      newly.excluded <- sum(df$exclude %in% c('Exclude-EWMA-Extreme', 'Exclude-EWMA-Extreme-Pair'))
      if (newly.excluded > num.ewma.excluded) {
        num.ewma.excluded <- newly.excluded
      } else {
        break
      }
    }
    return(df$exclude)
  })(copy(.SD)), by = .(subjid, param), .SDcols = c('index', 'sex', 'agedays', 'tbc.sd', 'exclude')]



  # 12. Redo duplicates using EWMA. This will be the final time duplicates are done.
  #     For some duplicates it is very difficult to tell which one is likely representative. If the duplicates are very similar to each
  #     other, we will select one. If it is very difficult to tell which one is correct and the duplicates are not very similar, we will
  #     exclude all duplicates for that subject/parameter on that day
  # a.	Replace exc_*=0 for all temporarily excluded duplicates (exc_*==0)
  if (!quietly)
    cat(sprintf("[%s] Exclude duplicates based on EWMA...\n", Sys.time()))
  data.df[exclude == 'Exclude-Temporary-Duplicate', exclude := 'Include']

  # 12b. Select which duplicate to include in EWMA calculations using the same criteria as in step 5. However, do not include values
  #      in these medians that were excluded in steps 9-11 (exc_*=3, 4, 5 or 6)
  #  i.  Determine median_tbc*sd and median_tbcOsd as in step 5.
  # ii.	 For each subject/parameter with duplicates and at least one non-duplicate value, select the value closest to the
  #      median_tbc*sd for inclusion in EWMA calculations.

  # This is functionally the same as re-doing the "temporary duplicate" step before doing the EWMA
  temp.dups <- temporary_duplicates(data.df)
  data.df[temp.dups, exclude := 'Exclude-Temporary-Duplicate']

  # prepare a list of valid rows and initialize variables for convenience
  valid.rows <- valid(data.df)
  data.df[, `:=`(
    ewma.all = as.double(NaN),
    abssum2 = as.double(NaN),
    median.other.sd = as.double(NaN),
    duplicate = F
  )]

  # 12c.	Calculate a EWMA step for all subjects/parameters with duplicates and at least one non-duplicate value with the
  #       following modifications
  #   i.	For calculating the EWMA, include only the duplicate selected in 12c
  #  ii.	Calculate dewma_* for all values of duplicates
  # iii.	You do not need to calculate EWMAbef or EWMAaft for this step

  # determine proportion of days with duplication for each parameter ahead of time for efficiency
  dup.ratio.df <- data.df[subjid %in% subj.dup &
                            (valid.rows |
                               temp.dups), list(dup = (.N > 1)), by = .(subjid, param, agedays)][j = list(dup.ratio =
                                                                                                            mean(dup)), keyby = .(subjid, param)]
  # identify subject/parameters where there isduplication but at least one day with no duplicates for that parameter
  subj.param.not.all.dups <- dup.ratio.df[dup.ratio < 1.0, list(subjid, param)]
  # identify subject/parameters where there is duplication for all days for that parameter
  subj.param.all.dups <- dup.ratio.df[dup.ratio == 1, list(subjid, param)]
  subj.all.dups <- subj.param.all.dups[, unique(subjid)]
  # perform ewma for subjects with duplicates
  data.df[subjid %in% subj.dup &
            valid.rows, ewma.all := ewma(agedays, tbc.sd, ewma.exp, ewma.adjacent =
                                           F), by = .(subjid, param)]
  # note: at this point, only one ewma.all exists per param on a given day for a subject, so sort(ewma.all)[1] will returns the non-missing ewma.all
  data.df[subjid %in% subj.dup, ewma.all := sort(ewma.all)[1], by = .(subjid, param, agedays)]

  # iv.  Calculate abssum2_*=|2*dewma_*|+|tbc*sd| (note that this is different from how we calculated abssum_* in step 11).
  # NOTE: only children with more than one ageday with valid measurements will have a valid ewma from above
  data.df[, abssum2 := 2 * abs(tbc.sd - ewma.all) + abs(tbc.sd)]

  # 12d.  For each subject/parameter/age with duplicates and at least one non-duplicate value:
  #   i.  Replace exc_*=7 for all values except the value that has the smallest abssum2_*.
  data.df[J(subj.param.not.all.dups), duplicate := seq_along(abssum2) != which.min(abssum2), by =
            .(subjid, param, agedays)]
  data.df[temp.dups, exclude := 'Include']
  data.df[(valid.rows |
             temp.dups) &
            duplicate, exclude := 'Exclude-Duplicate']
  #  ii.  Determine dup_tot_* (# of days with duplicates for that subject/parameter) and nodup_tot_* (# of days with nonexlcuded
  #       non-duplicates for that subject/parameter).
  # iii.  If dup_tot_*/(dup_tot_*+nodup_tot_*) is greater than 1/2, replace exc_*=7 for all duplicates for that subject/parameter
  #       for each age where the  largest measurement minus the smallest measurement for that subject/parameter/age is larger than
  #       the maximum difference (ht 3cm; wt 0-9.999 kg 0.25kg; wt 10-29.9999 kg 0.5 kg; wt 30kg and higher 1 kg).

  data.df[J(dup.ratio.df[dup.ratio > 1 / 2, list(subjid, param)]), exclude := (function(df) {
    df[, `:=`(tbc.sd.min = as.double(NaN),
              tbc.sd.max = as.double(NaN))]
    df[valid(
      exclude,
      include.duplicates = T,
      include.temporary.duplicates = T
    ), `:=`(tbc.sd.min = min(tbc.sd),
            tbc.sd.max = max(tbc.sd))]
    df[tbc.sd.max - tbc.sd.min > ifelse(param == 'HEIGHTCM',
                                        3,
                                        ifelse(
                                          param == 'WEIGHTKG',
                                          ifelse(tbc.sd.min < 10, 0.25, ifelse(tbc.sd.min < 30, 0.5, 1)),
                                          NA
                                        )),
       exclude := 'Exclude-Duplicate']
    return(df$exclude)
  })(copy(.SD)), .SDcols = c('exclude', 'tbc.sd'), by = .(subjid, param, agedays)]

  # 12e.	For each subject/parameter/age with duplicates and no nonduplicate values:
  #   i.	Replace exc_*=7 for all values except the value with the smallest |tbc*sd-median_tbcOsd|. If median_tbcOsd is missing because there are no values
  #       for the other parameter, randomly choose one duplicate value for each subject/parameter/age to keep as exc_*=0 and replace exc_*=7 for all other
  #       duplicates for that subject/parameter/age.
  #  ii.  If the largest measurement minus the smallest measurement for that subject/parameter/age is larger than the maximum difference
  #       (ht 3cm; wt 0-9.999 kg 0.25kg; wt 10-29.9999 kg 0.5 kg; wt 30kg and higher 1 kg)., replace exc_*=7 for all duplicates for that
  #       subject/parameter/age.

  # calculate median for other parameter (restrict to subjects with all duplication for at least one parameter)
  data.df[subjid %in% subj.all.dups, exclude := (function(subj.df) {
    # flag days that have duplicates / potentially valid parameters
    subj.df[, `:=`(
      duplicates.this.day = F,
      duplicate = F,
      tbc.sd.min = as.double(NaN),
      tbc.sd.max = as.double(NaN)
    )]
    valid.rows = valid(
      subj.df,
      include.duplicates = T,
      include.temporary.duplicates = T
    )
    subj.df[valid.rows, duplicates.this.day := (.N > 1), by = .(param, agedays)]
    for (p in subj.df[j = unique(param)]) {
      median.sd <- subj.df[param != p &
                             !duplicates.this.day, median(tbc.sd)]
      subj.df[param == p, median.other.sd := median.sd]
    }
    # safety check -- assign median.other.sd==0 to ensure "which.min" functions correctly below
    subj.df[is.na(median.other.sd), median.other.sd := 0]
    # identify rows as duplicate where |tbc*sd-median_tbcOsd| is not at the minimum value
    subj.df[duplicates.this.day == T, duplicate := (seq_along(median.other.sd) != which.min(abs(tbc.sd - median.other.sd))), by =
              .(param, agedays)]
    subj.df[duplicates.this.day &
              !duplicate, exclude := 'Include']
    subj.df[duplicates.this.day &
              duplicate, exclude := 'Exclude-Duplicate']
    subj.df[duplicates.this.day == T, `:=`(tbc.sd.min = min(tbc.sd),
                                           tbc.sd.max = max(tbc.sd)), by = .(param, agedays)]
    subj.df[tbc.sd.max - tbc.sd.min > ifelse(param == 'HEIGHTCM',
                                             3,
                                             ifelse(
                                               param == 'WEIGHTKG',
                                               ifelse(tbc.sd.min < 10, 0.25, ifelse(tbc.sd.min < 30, 0.5, 1)),
                                               NA
                                             )),
            exclude := 'Exclude-Duplicate']

    # identify kids who had an SD or EWMA extreme excluded that was a duplicate and re-label as "Exclude-Duplicate"
    subj.df[, duplicates.this.day := F]
    # consider any non-missing measurement when determining presence of duplicates
    subj.df[exclude != 'Missing', duplicates.this.day := (.N > 1), by =
              .(param, agedays)]
    subj.df[duplicates.this.day &
              exclude %in% c('Exclude-SD-Cutoff',
                             'Exclude-EWMA-Extreme',
                             'Exclude-EWMA-Extreme-Pair'), exclude := 'Exclude-Duplicate']

    return(subj.df$exclude)
  })(copy(.SD)), .SDcols = c('param', 'agedays', 'exclude', 'tbc.sd'), by =
    .(subjid)]

  # 12f.  For any values that were excluded with exc_*=4, 5, or 6 that are also duplicates, replace exc_*=7.
  data.df[subjid %in% subj.dup, exclude := (function(subj.df) {
    if (.N > 1) {
      subj.df[exclude %in% c('Exclude-SD-Cutoff',
                             'Exclude-EWMA-Extreme',
                             'Exclude-EWMA-Extreme-Pair'), exclude := 'Exclude-Duplicate']
    }
    return(subj.df$exclude)
  })(copy(.SD)), .SDcols = c('exclude'), by = .(subjid, param, agedays)]

  # 13.  Calculate plus/minus measurements with allowable errors and corresponding recentered SD scores
  # a.	In order to help determine if a deviation from EWMA could be explained by an allowable degree of error, calculate the following:
  #    i.	wt_plus=wt+0.05*wt
  #    ii.	wt_minus=wt-0.05*wt
  #    iii.	ht_plus=ht+1
  #    iv.	ht_minus=ht-1
  # b.	Foreach of the above calculated and then recenter the SD score for the new value, generating tbc*sd_plus and tbc*sd_minus
  data.df[, delta := ifelse(param == 'WEIGHTKG', .05 * v, 1)]
  data.df[, `:=`(v.minus = v - delta, v.plus = v + delta)]
  data.df[, `:=`(
    tbc.sd.minus = measurement.to.z(param, agedays, sex, v.minus, T),
    tbc.sd.plus = measurement.to.z(param, agedays, sex, v.plus, T)
  )]

  # 14.  Exclude moderate errors based on EWMA
  # a.	This step is similar to step 11, with repeated exclusions of 1 value at a time, but with different criteria than step 11. There are several criteria used
  #     as checks to make sure that values that have a large dewma_* are truly not likely to be representative.
  # b.	Perform a EWMA calculation
  #   i.	In addition to standard dewma_* variables, calculate dewma_*_plus and dewma_*_minus using the tbc*sd scores generated in step 13
  if (!quietly)
    cat(sprintf("[%s] Exclude moderate errors based on EWMA...\n", Sys.time()))
  data.df[, exclude := (function(subj.df) {
    num.ewma.excluded <- 0
    while (T) {
      valid.rows <- valid(subj.df)
      # initialize fields
      subj.df[, `:=`(
        ewma.all = as.double(NaN),
        ewma.before = as.double(NaN),
        ewma.after = as.double(NaN),
        tbc.sd.prev = as.double(NaN),
        tbc.sd.next = as.double(NaN),
        delta.agedays.prev = as.integer(NaN),
        delta.agedays.next = as.integer(NaN),
        abs.2ndlast.sd = as.double(NaN),
        tbc.other.sd = as.double(NaN)
      )]
      subj.df[valid.rows, (ewma.fields) := ewma(agedays, tbc.sd, ewma.exp, T), by =
                param]
      subj.df[, `:=`(
        dewma.all = tbc.sd - ewma.all,
        dewma.before = tbc.sd - ewma.before,
        dewma.after = tbc.sd - ewma.after
      )]

      # 14c.	Calculate d_prevsd=tbc*sd-tbc*sdprev; d_prevsd_minus=tbc*sd_minus-tbc*sdprev; d_prevsd_plus=tbc*sd_plus-tbc*sdprev
      #     and d_nextsd=tbc*sd-tbc*sdnext; d_nextsd_minus=tbc*sd_minus-tbc*sdnext; d_nextsd_plus=tbc*sd_plus-tbc*sdnext

      # for efficiency, bring get.prev and get.next inline here (working on valid rows within a single parameter for a single subject)
      # structure c(NA, field.name[-.N]) == get.prev
      # structure c(field.name[-1], NA) == get.next
      subj.df[valid.rows, `:=`(tbc.sd.prev = c(NA, tbc.sd[-.N]),
                               tbc.sd.next = c(tbc.sd[-1], NA)), by = param]
      subj.df[, `:=`(
        dprev.sd = tbc.sd - tbc.sd.prev,
        dprev.sd.minus = tbc.sd.minus - tbc.sd.prev,
        dprev.sd.plus = tbc.sd.plus - tbc.sd.prev,
        dnext.sd = tbc.sd - tbc.sd.next,
        dnext.sd.minus = tbc.sd.minus - tbc.sd.next,
        dnext.sd.plus = tbc.sd.plus - tbc.sd.next
      )]

      # 14d.	Calculate d_agedays_prev=agedays-agedaysprev and d_agedays_next=agedaysnext-agedays
      subj.df[valid.rows, `:=`(
        delta.agedays.prev = as.integer(agedays - c(NA, agedays[-.N])),
        delta.agedays.next = as.integer(agedays - c(agedays[-1], NA))
      ), by = param]

      # 14e.	Generate abs_2ndlast_sd=|tbc*sd| for the second-to-last measurement for a subject/parameter
      # assign this value (if defined) to all agedays for that subject
      subj.df[valid.rows, abs.2ndlast.sd := ifelse(.N >= 2, abs(tbc.sd[.N -
                                                                         1]), as.double(NaN)), by = param]

      # 14f.	Calculate tbcOsd which is the tbc*sd for the OTHER parameter for the same subject and ageday  with exc_*==0 (this may be missing).
      # NOTE: move this simplified version of swap_parameters here for efficiency
      valid.other <- subj.df[valid.rows, list(
        param.other = ifelse(param == 'WEIGHTKG', 'HEIGHTCM', 'WEIGHTKG'),
        agedays.other = agedays,
        tbc.sd
      )]
      setkey(valid.other, param.other, agedays.other)
      subj.df[valid.rows, tbc.other.sd := valid.other[list(param, agedays), tbc.sd]]

      subj.df[, exclude := (function(df) {
        # 14g.  Calculate median_tbcOsd which is the median_tbc*sd for the OTHER parameter for the same subject with exc_*==0 (this may be missing)
        # also assign param variable for use in subsequent steps
        median.tbc.other.sd <- median(df$tbc.other.sd, na.rm = T)
        num.valid <- sum(valid(df$exclude))

        # 14h.	Identify values for possible exclusion if they meet one of the following sets of criteria. Generate a temporary exclusion variable temp_exc_* equal
        #     to the number indicated to keep track of which set of criteria were met
        df$temp.exclude <-
          factor(NA, levels = exclude.levels, ordered = T)

        # 14h.i.	Replace temp_exc_*=8 if the value is one of 3 or more measurements for a subject/parameter AND the value is neither the first nor the last measurement
        #       AND one of the following sets of criteria are met
        #     1.	dewma_*>1 & dewma_*_bef>1 & dewma_*_aft>1 & d_nextsd_*>1 & d_prevsd_*>1 & d_prevsd_plus_*>1 & d_prevsd_minus_*>1 & d_nextsd_plus_*>1 & d_nextsd_minus_*>1
        #     2.	dewma_*<-1 & dewma_*_bef<-1 & dewma_*_aft<-1 & d_nextsd_*<-1 & d_prevsd_*<-1 & d_prevsd_plus_*<-1 & d_prevsd_minus_*<-1 & d_nextsd_plus_*<-1 & d_nextsd_minus_*<-1

        # NOTE: NA as treated as false in context of data.table, which takes care of the NA values that result when
        # evaluating the boolean expression for a first or last value of a parameter for a child,
        # which ensures the flag is only set for measurements that have both a preceding and subsequent measurement
        df[dewma.all > 1 &
             dewma.before > 1 &
             dewma.after > 1 &
             dprev.sd > 1 &
             dprev.sd.plus > 1 &
             dprev.sd.minus > 1 &
             dnext.sd > 1 &
             dnext.sd.plus > 1 & dnext.sd.minus > 1
           |
             dewma.all < -1 &
             dewma.before < -1 &
             dewma.after < -1 &
             dprev.sd < -1 &
             dprev.sd.plus < -1 &
             dprev.sd.minus < -1 &
             dnext.sd < -1 &
             dnext.sd.plus < -1 & dnext.sd.minus < -1,
           temp.exclude := 'Exclude-EWMA-8']

        # 14h.ii.	Replace temp_exc_*=9 if the value is the first of 3 or more measurements for a subject/parameter AND d_agedays_next<365.25
        #       AND one of the following sets of criteria are met
        #     1.	dewma_*>2 & dewma_*_aft>1 & d_nextsd_*>1 & d_nextsd_plus_*>1 & d_nextsd_minus_*>1
        #     2.	dewma_*<-2 & dewma_*_aft<-1 & d_nextsd_*<-1 & d_nextsd_plus_*<-1 & d_nextsd_minus_*<-1

        # take advantage of other variables we have calculated to infer that a row is the first of three or more valid measurements for a paramete
        df$first.of.three.or.more <- F
        df$last.of.three.or.more <- F
        df[is.na(delta.agedays.prev) &
             num.valid >= 3, first.of.three.or.more := T]
        df[is.na(delta.agedays.next) &
             num.valid >= 3, last.of.three.or.more := T]

        df[first.of.three.or.more & delta.agedays.next < 365.25
           &
             (
               dewma.all > 2 &
                 dewma.after > 1 &
                 dnext.sd > 1 &
                 dnext.sd.plus > 1 & dnext.sd.minus > 1
               |
                 dewma.all < -2 &
                 dewma.after < -1 &
                 dnext.sd < -1 &
                 dnext.sd.plus < -1 & dnext.sd.minus < -1
             ),
           temp.exclude := 'Exclude-EWMA-9']

        # 14h.iii.	Replace temp_exc_*=10 if the value is the first of 3 or more measurements for a subject/parameter AND d_agedays_next>365.25
        #         AND one of the following sets of criteria are met
        #     1.	dewma_*>3 & dewma_*_aft>1 & d_nextsd_*>1 & d_nextsd_plus_*>1 & d_nextsd_minus_*>1
        #     2.	dewma_*<-3 & dewma_*_aft<-1 & d_nextsd_*<-1 & d_nextsd_plus_*<-1 & d_nextsd_minus_*<-1

        # NOTE: changed age comparison to >= to ensure complete coverage of possible conditions in the event case fractional days are present in the dataset
        df[first.of.three.or.more & delta.agedays.next >= 365.25
           &
             (
               dewma.all > 3 &
                 dewma.after > 1 &
                 dnext.sd > 1 &
                 dnext.sd.plus > 1 & dnext.sd.minus > 1
               |
                 dewma.all < -3 &
                 dewma.after < -1 &
                 dnext.sd < -1 &
                 dnext.sd.plus < -1 & dnext.sd.minus < -1
             ),
           temp.exclude := 'Exclude-EWMA-10']

        # 14h.iv.	Replace temp_exc_*=11 if the value is the last of 3 or more measurements for a subject/parameter AND d_agedays_prev<730.5 AND abs_2ndlast_sd <2
        #       AND one of the following sets of criteria are met
        #     1.	dewma_*>2 & dewma_*_bef>1 & d_prevsd_*>1 & d_prevsd_plus_*>1 & d_prevsd_minus_*>1
        #     2.	dewma_*<-2 & dewma_*_bef<-1 & d_prevsd_*<-1 & d_prevsd_plus_*<-1 & d_prevsd_minus_*<-1
        df[last.of.three.or.more &
             delta.agedays.prev < 730.5 & abs.2ndlast.sd < 2
           &
             (
               dewma.all > 2 &
                 dewma.before > 1 &
                 dprev.sd > 1 &
                 dprev.sd.plus > 1 & dprev.sd.minus > 1
               |
                 dewma.all < -2 &
                 dewma.before < -1 &
                 dprev.sd < -1 &
                 dprev.sd.plus < -1 & dprev.sd.minus < -1
             ),
           temp.exclude := 'Exclude-EWMA-11']

        # 14h.v.	Replace temp_exc_*=12 if the value is the last of 3 or more measurements for a subject/parameter AND d_agedays_prev<730.5 AND abs_2ndlast_sd >=2
        #       AND one of the following sets of criteria are met
        #     1.	dewma_*>abs_2ndlast_sd & dewma_*_bef>1 & d_prevsd_*>1 & d_prevsd_plus_*>1 & d_prevsd_minus_*>1
        #     2.	dewma_*<(-1*abs_2ndlast_sd) & dewma_*_bef<-1 & d_prevsd_*<-1 & d_prevsd_plus_*<-1 & d_prevsd_minus_*<-1
        df[last.of.three.or.more &
             delta.agedays.prev < 730.5 & abs.2ndlast.sd >= 2
           &
             (
               dewma.all > abs.2ndlast.sd &
                 dewma.before > 1 &
                 dprev.sd > 1 &
                 dprev.sd.plus > 1 & dprev.sd.minus > 1
               |
                 dewma.all < -abs.2ndlast.sd &
                 dewma.before < -1 &
                 dprev.sd < -1 &
                 dprev.sd.plus < -1 & dprev.sd.minus < -1
             ),
           temp.exclude := 'Exclude-EWMA-12']

        # 14h.vi.	Replace temp_exc_*=13 if the value is the last of 3 or more measurements for a subject/parameter AND d_agedays_prev>730.5 AND  abs_2ndlast_sd <2
        #       AND one of the following sets of criteria are met
        #     1.	dewma_*>3 & dewma_*_bef>1 & d_prevsd_*>1 & d_prevsd_plus_*>1 & d_prevsd_minus_*>1 & ((tbc*sd-tbcOsd)>4
        #         OR ((tbc*sd-median_tbcOsd)>4 & tbcOsd is missing) OR median_tbcOsd is missing)
        #     2.	dewma_*<-3 & dewma_*_bef<-1 & d_prevsd_*<-1 & d_prevsd_plus_*<-1 & d_prevsd_minus_*<-1 & ((tbc*sd-tbcOsd)<-4
        #         OR ((tbc*sd-median_tbcOsd)<-4 & tbcOsd is missing) OR median_tbcOsd is missing)

        # NOTE: changed age comparison to >= to ensure complete coverage of possible conditions in the event case fractional days are present in the dataset
        df[last.of.three.or.more &
             delta.agedays.prev >= 730.5 & abs.2ndlast.sd < 2
           &
             (
               dewma.all > 3 &
                 dewma.before > 1 &
                 dprev.sd > 1 &
                 dprev.sd.plus > 1 & dprev.sd.minus > 1
               &
                 (
                   tbc.sd - tbc.other.sd > 4 |
                     is.na(tbc.other.sd) &
                     tbc.sd - median.tbc.other.sd > 4 |
                     is.na(median.tbc.other.sd)
                 )
               |
                 dewma.all < -3 &
                 dewma.before < -1 &
                 dprev.sd < -1 &
                 dprev.sd.plus < -1 & dprev.sd.minus < -1
               &
                 (
                   tbc.sd - tbc.other.sd < -4 |
                     is.na(tbc.other.sd) &
                     tbc.sd - median.tbc.other.sd < -4 |
                     is.na(median.tbc.other.sd)
                 )
             ),
           temp.exclude := 'Exclude-EWMA-13']

        # 14h.vii.	Replace temp_exc_*=14 if the value is the last of 3 or more measurements for a subject/parameter AND d_agedays_prev>730.5 AND  abs_2ndlast_sd>=2
        #         AND one of the following sets of criteria are met
        #     1.	dewma_*>(1+abs_2ndlast_sd) & dewma_*_bef>1 & d_prevsd_*>1 & d_prevsd_plus_*>1 & d_prevsd_minus_*>1 & ((tbc*sd-tbcOsd)>4
        #         OR ((tbc*sd-median_tbcOsd)>4 & tbcOsd is missing) OR median_tbcOsd is missing)
        #     2.	dewma_*<(-1-abs_2ndlast_sd) & dewma_*_bef<-1 & d_prevsd_*<-1 & d_prevsd_plus_*<-1 & d_prevsd_minus_*<-1 & ((tbc*sd-tbcOsd)<-4
        #         OR ((tbc*sd-median_tbcOsd)<-4 & tbcOsd is missing) OR median_tbcOsd is missing)
        df[last.of.three.or.more &
             delta.agedays.prev >= 730.5 & abs.2ndlast.sd >= 2
           &
             (
               dewma.all > 1 + abs.2ndlast.sd &
                 dewma.before > 1 &
                 dprev.sd > 1 &
                 dprev.sd.plus > 1 & dprev.sd.minus > 1
               &
                 (
                   tbc.sd - tbc.other.sd > 4 |
                     is.na(tbc.other.sd) &
                     tbc.sd - median.tbc.other.sd > 4 |
                     is.na(median.tbc.other.sd)
                 )
               |
                 dewma.all < -1 - abs.2ndlast.sd &
                 dewma.before < -1 &
                 dprev.sd < -1 &
                 dprev.sd.plus < -1 & dprev.sd.minus < -1
               &
                 (
                   tbc.sd - tbc.other.sd < -4 |
                     is.na(tbc.other.sd) &
                     tbc.sd - median.tbc.other.sd < -4 |
                     is.na(median.tbc.other.sd)
                 )
             ),
           temp.exclude := 'Exclude-EWMA-14']

        # 14i.	If there is only one potential exclusion identified in step 14h for a subject and parameter, replace exc_*=temp_exc_* for that value
        rep <- !is.na(df$temp.exclude)
        num.exclude <- sum(rep)
        if (num.exclude == 1)
          df[rep, exclude := temp.exclude]

        # 14j.	If there is more than one potential exclusion identified in step 14h for a subject and parameter, calculate abssum_*=|tbc*sd+dewma_*| for each exclusion
        #     and replace exc_*=temp_exc_* for the value with the highest abssum_*
        if (num.exclude > 1) {
          # first order by decreasing abssum  (where rep=T)
          worst.row <- with(df, order(rep, abs(tbc.sd + dewma.all), decreasing =
                                        T))[1]
          df[worst.row, exclude := temp.exclude]
        }

        return(df$exclude)
      })(copy(.SD)), by = param]

      # k.	If there was at least one subject who had a potential exclusion identified in step 14h, repeat steps 14b-14j. If there were no subjects with potential
      #     exclusions identified in step 14h, move on to step 15.
      newly.excluded <- sum(subj.df$exclude >= 'Exclude-EWMA-8' &
                              subj.df$exclude <= 'Exclude-EWMA-14')
      if (newly.excluded > num.ewma.excluded) {
        num.ewma.excluded <- newly.excluded
      } else {
        break
      }
    }

    return(subj.df$exclude)
  })(copy(.SD)), by = subjid, .SDcols = c(
    'index',
    'sex',
    'param',
    'agedays',
    'v',
    'tbc.sd',
    'v.minus',
    'v.plus',
    'tbc.sd.minus',
    'tbc.sd.plus',
    'exclude'
  )]

  # 15.  Exclude heights based on absolute differences in measurement. The key to this step is that once we identify pairs of measurements with implausible
  #      amounts of absolute difference between them, we have to determine which of the two values in the pair is less likely to be representative and should
  #      be excluded. For subjects/parameters with 3 or more measurements, this is done by looking at the dewma_* of each of the 2 values in a pair using a ewma that
  #      excludes the other value in the pair. For subjects/parameters with 2 or more measurements, this is done by looking at the absolute value of the tbc*sd.
  #      The Tanner height velocity reference is used for measurements taken at >2yo, WHO will be used for <2yo. For a few pairs of measurements either could be used;
  #      WHO will be used if difference between ages is < 9 months.
  if (!quietly)
    cat(sprintf(
      "[%s] Exclude heights based on growth velocity...\n",
      Sys.time()
    ))
  data.df[param == 'HEIGHTCM', exclude := (function(subj.df) {
    # assign some book keeping variables
    #subj.df[, `:=`(subjid = subjid, param='HEIGHTCM',index=1:.N)]
    subj.df[, index := 1:.N]

    num.height.excluded <- 0
    while (T) {
      # use a closure to discard all the extra fields added to df with each iteration
      subj.df[valid(exclude), exclude := (function (df) {
        # with each iteration we are operating only on valid rows to allow simplification of code below

        # initialize fields
        df[, (ewma.fields) := as.double(NaN)]
        df[, `:=`(
          v.prev = as.double(NaN),
          v.next = as.double(NaN),
          dewma.after.prev = as.double(NaN),
          dewma.before.next = as.double(NaN),
          abs.tbc.sd.prev = as.double(NaN),
          abs.tbc.sd.next = as.double(NaN),
          agedays.next = as.integer(NaN),
          abs.2ndlast.sd = as.double(NaN),
          mindiff.prev.ht = as.double(NaN),
          mindiff.next.ht = as.double(NaN),
          maxdiff.prev.ht = as.double(NaN),
          maxdiff.next.ht = as.double(NaN),
          pair.prev = F,
          pair.next = F
        )]

        # ewma fields are needed later -- calculate now for efficiency
        df[, (ewma.fields) := ewma(agedays, tbc.sd, ewma.exp, T)]

        # calculate some usefule values (e.g. dewma values and tbc.sd) for use in later steps
        df[, `:=`(
          dewma.all = tbc.sd - ewma.all,
          dewma.before = tbc.sd - ewma.before,
          dewma.after = tbc.sd - ewma.after,
          abs.tbc.sd = abs(tbc.sd)
        )]

        # 15a.  As with steps 11 and 14, only one value will be excluded per round, and the step will be repeated until there are no more values to exclude
        # b.  For each height, calculate the d_age=agedays of next value-agedays of current value
        # NOTE: obtain next measurement, ewma.before and abs.tbc.sd as well since they are needed later

        # for efficiency, bring get.next inline here (working on valid rows within a single parameter for a single subject)
        # structure c(field.name[-1], NA) == get.next
        df[, `:=`(
          agedays.next = c(agedays[-1], NA),
          v.next = c(v[-1], NA),
          dewma.before.next = c(dewma.before[-1], NA),
          abs.tbc.sd.next = c(abs.tbc.sd[-1], NA)
        )]
        df$delta.agedays.next <- with(df, agedays.next - agedays)

        # 15c.	For each height, calculate mid_agedays=0.5*(agedays of next value + agedays of current value)
        df$mid.agedays <- 0.5 * (df$agedays.next + df$agedays)

        # 15d.	Generate variable tanner_months= 6+12*(round(mid_agedays/365.25))
        # only calculate for rows that relate to height (may speed up subsequent processing)
        df$tanner.months <-
          with(df, 6 + 12 * (round(mid.agedays / 365.25)))

        # 15e.	Merge with dataset tanner_ht_vel using sex and tanner_months â€“ this will give you min_ht_vel and max_ht_vel
        setkey(df, sex, tanner.months)
        df <- tanner.ht.vel[df]

        # 15f.	Calculate the following:
        #   i.	mindiff_ht=0.5*min_ht_vel*(d_agedays/365.25)^2-3 if d_agedays<365.25
        #   ii.	replace mindiff_ht=0.5*min_ht_vel-3 if d_agedays>365.25
        df[, ht.exp := ifelse(delta.agedays.next < 365.25, 2, 0)]
        df[, `:=`(maxdiff.next.ht = as.double(NA),
                  mindiff.next.ht = as.double(NaN))]
        df[, mindiff.next.ht := 0.5 * min.ht.vel * (delta.agedays.next /
                                                      365.25) ^ ht.exp - 3]

        # 15f.iii.	maxdiff_ht=2*max_ht_vel*(d_agedays/365.25)^1.5+5.5 if d_agedays>365.25
        #   iv.	replace maxdiff_ht=2*max_ht_vel*(d_agedays/365.25)^0.33+5.5 if d_agedays<365.25
        df[, ht.exp := ifelse(delta.agedays.next < 365.25, 0.33, 1.5)]
        df[, maxdiff.next.ht := 2 * max.ht.vel * (delta.agedays.next /
                                                    365.25) ^ ht.exp + 5.5]

        # 15g.	Generate variable whoagegrp_ht=agedays/30.4375 rounded to the nearest integer
        df[, whoagegrp.ht := round(agedays / 30.4375)]

        # 15h.	Generate variable whoinc_age_ht based on values of d_agedays_ht according the the following table
        #   d_agedays_ht	whoinc_age_ht
        #   20-45	        1
        #   46-75	        2
        #   76-106	      3
        #   107-152	      4
        #   153-198	      6
        #   All others	  missing
        df[, whoinc.age.ht := ifelse(delta.agedays.next < 20 ,
                                     NA,
                                     ifelse(
                                       delta.agedays.next <= 45,
                                       1,
                                       ifelse(
                                         delta.agedays.next <= 75,
                                         2,
                                         ifelse(
                                           delta.agedays.next <= 106,
                                           3,
                                           ifelse(
                                             delta.agedays.next <= 152,
                                             4,
                                             ifelse(delta.agedays.next <= 198, 6, NA)
                                           )
                                         )
                                       )
                                     ))]

        # i.	Merge using sex and whoagegrp_ht using who_ht_vel_3sd and who_ht_maxvel_3sd; this will give you varaibles whoinc_i_ht and maxwhoinc_i_ht
        #     for various intervals where i is 1,2, 3,4, 6 and corresponds to whoinc_age_ht.
        setkey(df, sex, whoagegrp.ht)
        df <- who.ht.vel[df]

        # restore original sort order (ensures valid.rows variable applies to correct rows)
        setkey(df, index)

        # 15j.	Generate variable who_mindiff_ht=whoinc_i_ht according to the value if whoinc_age_ht; make who_mindiff_ht missing if whoinc_i_ht or whoinc_age_ht is missing.
        df[, who.mindiff.next.ht := ifelse(
          delta.agedays.next < 20 ,
          NA,
          ifelse(
            delta.agedays.next <= 45,
            whoinc.1.ht,
            ifelse(
              delta.agedays.next <= 75,
              whoinc.2.ht,
              ifelse(
                delta.agedays.next <= 106,
                whoinc.3.ht,
                ifelse(
                  delta.agedays.next <= 152,
                  whoinc.4.ht,
                  ifelse(delta.agedays.next <= 198, whoinc.6.ht, NA)
                )
              )
            )
          )
        )]

        # 15k.	Generate variable who_maxdiff_ht=max_whoinc_i_ht according to the value if whoinc_age_ht; make who_maxdiff_ht missing if max_whoinc_i_ht or
        #     whoinc_age_ht is missing.
        df[, who.maxdiff.next.ht := ifelse(
          delta.agedays.next < 20 ,
          NA,
          ifelse(
            delta.agedays.next <= 45,
            max.whoinc.1.ht,
            ifelse(
              delta.agedays.next <= 75,
              max.whoinc.2.ht,
              ifelse(
                delta.agedays.next <= 106,
                max.whoinc.3.ht,
                ifelse(
                  delta.agedays.next <= 152,
                  max.whoinc.4.ht,
                  ifelse(delta.agedays.next <= 198, max.whoinc.6.ht, NA)
                )
              )
            )
          )
        )]

        # 15l.	Scale allowed value based on d_agedays_ht:
        #   1.	replace who_mindiff_`p'=who_mindiff_`p'*d_agedays_`p'/(whoinc_age_`p'*30.4375) if d_agedays_`p'<(whoinc_age_`p'*30.4375)
        #   2.	replace who_maxdiff_`p'=who_maxdiff_`p'*d_agedays_`p'/(whoinc_age_`p'*30.4375) if d_agedays_`p'>(whoinc_age_`p'*30.4375)
        df[delta.agedays.next < whoinc.age.ht * 30.4375,
           `:=`(
             who.mindiff.next.ht = who.mindiff.next.ht * delta.agedays.next / (whoinc.age.ht *
                                                                                 30.4375),
             who.maxdiff.next.ht = who.maxdiff.next.ht * delta.agedays.next /
               (whoinc.age.ht * 30.4375)
           )]

        # 15m.	Replace mindiff_ht/maxdiff_ht with adjusted WHO value if Tanner value is missing or if both are present and age difference is < 9 months:
        #   1.	replace mindiff_`p'=0.5*who_mindiff_`p'-3 if who_mindiff_`p' is not missing & d_agedays_`p'<(9*30.4375)
        #   2.	replace maxdiff_`p'=2*who_maxdiff_`p'+3 if who_maxdiff_`p' is not missing & d_agedays_`p'<(9*30.4375)
        #   3.	replace mindiff_`p'=0.5*who_mindiff_`p'-3 if mindiff_`p' is missing & who_mindiff_`p' is not missing
        #   4.	replace maxdiff_`p'=2*who_maxdiff_`p'+3 if maxdiff_`p is missing & who_maxdiff_`p' is not missing

        # refactored logic slightly for efficiency
        df[!is.na(who.mindiff.next.ht) &
             (delta.agedays.next < 9 * 30.4375 |
                is.na(mindiff.next.ht)),
           `:=`(
             mindiff.next.ht = 0.5 * who.mindiff.next.ht - 3,
             maxdiff.next.ht = 2.0 * who.maxdiff.next.ht + 3
           )]

        # 15m.5.  replace mindiff_`p'=-3 if mindiff_`p' is missing
        df[is.na(mindiff.next.ht), mindiff.next.ht := -3]

        # 15n.	Determine the min/maxdiffs for the previous age: mindiff_prev_ht, maxdiff_prev_ht
        # NOTE: obtain previous height, ewma.after value and abs.tbc.sd as well since they are needed in next steps

        # for efficiency, bring get.prev inline here (working on valid rows within a single parameter for a single subject)
        # structure c(NA, tbc.sd[-.N]) == get.prev
        df[, `:=`(
          v.prev = c(NA, v[-.N]),
          dewma.after.prev = c(NA, dewma.after[-.N]),
          abs.tbc.sd.prev = c(NA, abs.tbc.sd[-.N]),
          mindiff.prev.ht = c(NA, mindiff.next.ht[-.N]),
          maxdiff.prev.ht = c(NA, maxdiff.next.ht[-.N])
        )]

        # 15o.	Determine d_prev_ht=ht-htprev (set to missing for the first value for a subject) and d_next_ht=htnext-ht (set to missing for the last value for a subject)
        df[, `:=`(delta.prev.ht = v - v.prev,
                  delta.next.ht = v.next - v)]

        # 15p.  Perform a EWMA calculation with the following modifications:
        #  i.	  Generate a variable pair=1 if (d_prev_ht<mindiff_prev_ht OR d_ht<mindiff_ht OR d_prev_ht>maxdiff_prev_ht  OR d_ht>maxdiff_ht) AND exc_ht==0
        df[, pair := na_as_false(
          delta.prev.ht < mindiff.prev.ht |
            delta.next.ht < mindiff.next.ht |
            delta.prev.ht > maxdiff.prev.ht |
            delta.next.ht > maxdiff.next.ht
        )]

        # for efficiency, bring get.prev and get.next inline here (working on valid rows within a single parameter for a single subject)
        # structure c(NA, field.name[-.N]) == get.prev
        # structure c(field.name[-1], NA) == get.next
        df[, `:=`(pair.prev = c(F, pair[-.N]),
                  pair.next = c(pair[-1], F))]

        #  ii.	Generate bef_g_aftm1=1 if |Î”ewma_htbef| for the value of interest is greater than |Î”ewma_htaft| for the previous value
        #       AND the value of interest is not the first height value for that subject AND pair==1 AND pair for the previous value==1

        #  iii.	Generate aft_g_befp1=1 if |Î”ewma_htaft| for the value of interest is greater than |Î”ewma_htbef| for the next value
        #       AND the value of interest is not the last height value for that subject AND pair==1 AND pair for the next value==1
        # NOTE: pair.next will be NA last height, which will result in a FALSE value below
        df[, `:=`(
          bef.g.aftm1 = na_as_false(
            abs(dewma.before) > abs(dewma.after.prev)  & pair & pair.prev
          ),
          aft.g.befp1 = na_as_false(
            abs(dewma.after)  > abs(dewma.before.next) & pair & pair.next
          )
        )]

        #  iv.	Determine tbchtsd for each value as well as the one before prev_tbchtsd and after next_tbchtsd it
        # NOTE: done previously for efficiency

        # 15p.v.  Determine the total number of ht values for each subject (tot_ht)
        # NOTE: all rows are valid due to constraint in subj.df[...] statement
        num.valid <- .N

        # 15q.	Identify a value for possible exclusion if one of the following sets of criteria are met. For values identified by each set of criteria determine
        #       the value of temp_diff using the formula given
        #   i.	d_prev_ht<mindiff_prev_ht & bef_g_aftm1_ht==1 & exc_ht==0 & mindiff_prev_ht is not missing
        #     a.  (temp_diff=|dewma_ht_bef|)
        df[, temp.diff := as.double(NaN)]
        df$temp.exclude <-
          factor(NA, levels = exclude.levels, ordered = T)
        df[delta.prev.ht < mindiff.prev.ht & bef.g.aftm1,
           `:=`(temp.diff = abs(dewma.before),
                temp.exclude = 'Exclude-Min-Height-Change')]

        #   ii.	d_ht<mindiff_ht & aft_g_befp1_ht==1 & exc_ht==0 & mindiff_ht is not missing
        #     a.	(temp_diff=|dewma_ht_aft|)
        df[delta.next.ht < mindiff.next.ht & aft.g.befp1,
           `:=`(temp.diff = abs(dewma.after),
                temp.exclude = 'Exclude-Min-Height-Change')]

        #   iii.	d_prev_ht>maxdiff_prev_ht & bef_g_aftm1_ht==1 & exc_ht==0 & mindiff_prev_ht is not missing
        #     a.  (temp_diff=|dewma_ht_bef|)
        df[delta.prev.ht > maxdiff.prev.ht & bef.g.aftm1,
           `:=`(temp.diff = abs(dewma.before),
                temp.exclude = 'Exclude-Max-Height-Change')]

        #   iv.	d_ht>maxdiff_ht & aft_g_befp1_ht==1 & exc_ht==0 & mindiff_ht is not missing
        #     a.  (temp_diff=|dewma_ht_aft|)
        df[delta.next.ht > maxdiff.next.ht & aft.g.befp1,
           `:=`(temp.diff = abs(dewma.after),
                temp.exclude = 'Exclude-Max-Height-Change')]

        #   v.	d_prev_ht<mindiff_prev_ht & tot_ht==2 & |tbchtsd|>|prev_tbchtsd|
        #     a. for v-viii temp_diff is kept as missing
        #   vi. d_ht<mindiff_ht & tot_ht==2 & |tbchtsd|>|next_tbchtsd|
        df[delta.prev.ht < mindiff.prev.ht &
             num.valid == 2 & abs.tbc.sd > abs.tbc.sd.prev
           |
             delta.next.ht < mindiff.next.ht &
             num.valid == 2 & abs.tbc.sd > abs.tbc.sd.next,
           temp.exclude := 'Exclude-Min-Height-Change']

        #   vii.	d_prev_ht>maxdiff_prev_ht & tot_ht==2 & |tbchtsd|>|prev_tbchtsd|
        #   viii. d_ht>maxdiff_ht & tot_ht==2 & |tbchtsd|>|next_tbchtsd|
        df[delta.prev.ht > maxdiff.prev.ht &
             num.valid == 2 & abs.tbc.sd > abs.tbc.sd.prev
           |
             delta.next.ht > maxdiff.next.ht &
             num.valid == 2 & abs.tbc.sd > abs.tbc.sd.next,
           temp.exclude := 'Exclude-Max-Height-Change']

        # r.  If there is only one potential exclusion identified in step 15j for a subject and parameter,
        #     replace exc_ht=15 for that value if it met criteria i, ii, v, or vi  and exc_ht=16 if it met criteria iii, iv, vii, or viii
        # NOTE: these exclusions are assigned in the code above as 'Exclude-Min-Height-Change' and 'Exclude-Max-Height-Change'
        rep <- !is.na(df$temp.exclude)
        num.exclude <- sum(rep)
        if (num.exclude == 1)
          df[rep, exclude := temp.exclude]

        # s.  If there is more than one potential exclusion identified in step 14h for a subject and parameter, determine which value has the largest temp_diff and
        #     replace exc_ht=15 for that value if it met criteria i, ii, v, or vi and exc_ht=16 for that value if it met criteria iii,  iv, vii, or viii.

        if (num.exclude > 1) {
          # first order by decreasing temp.diff (where rep=T)
          worst.row <- order(rep, df$temp.diff, decreasing = T)[1]
          df[worst.row, exclude := temp.exclude]
        }

        return(df$exclude)
      })(copy(.SD))]

      # t.  If there was at least one subject who had a potential exclusion identified in step 15q, repeat steps 15b-15q. If there were no subjects with potential
      #     exclusions identified in step 15q, move on to step 16.
      newly.excluded <- sum(
        subj.df$exclude %in% c(
          'Exclude-Min-Height-Change',
          'Exclude-Max-Height-Change'
        )
      )
      if (newly.excluded > num.height.excluded) {
        num.height.excluded <- newly.excluded
      } else {
        break
      }
    }

    setkey(subj.df, index)
    return(subj.df$exclude)
  })(copy(.SD)), by = .(subjid), .SDcols = c('sex', 'agedays', 'v', 'tbc.sd', 'exclude')]

  # 16.  Exclude measurements for subjects/parameters with only 1 or 2 measurements with exc_*=0. This step uses a variety of criteria,
  #      including the tbc*sd of the other parameter.
  if (!quietly)
    cat(sprintf(
      "[%s] Exclude single measurements and pairs...\n",
      Sys.time()
    ))
  data.df$tbc.other.sd <- swap_parameters(df = data.df)
  data.df[, exclude := (function(df) {
    valid.rows <- which(valid(df))
    # calculate median SD for other parameter (used in steps below)
    median.tbc.other.sd <- median(df$tbc.other.sd)
    # a.  Identify subjects/parameters with only 2 values with exc_*==0, and determine the following:
    #   1.  d_tbc*sd_other=the absolute value difference in tbc*sd
    #   2.	d_agedays_other=the difference in agedays between the 2 values
    #   3.	tbcOsd=the tbc*sd for the other parameter on the same ageday
    # NOTE: this is done as the first step below for efficiency
    #   4.	median_tbcOsd= the median tbc*sd for the other parameter
    #   5.	abs_d_*_O is the absolute value of the difference between tbc*sd and tbcOsd if tbcOsd is not missing;
    #       it is the absolute value of the difference between tbc*sd and median_tbcOsd if tbcOsd is missing but median_tbcOsd is not missing;
    #       and is missing if tbcOsd and median_tbcOsd are both missing
    if (length(valid.rows) == 2) {
      delta.tbc.sd.other <- abs(df$tbc.sd[valid.rows[1]] - df$tbc.sd[valid.rows[2]])
      delta.agedays.other <- abs(df$agedays[valid.rows[1]] - df$agedays[valid.rows[2]])
      abs.delta.other <- with(df, ifelse(
        !is.na(tbc.other.sd),
        abs(tbc.sd - tbc.other.sd),
        ifelse(
          !is.na(median.tbc.other.sd),
          abs(tbc.sd - median.tbc.other.sd),
          NA
        )
      ))

      # b.	For subjects/parameters with 2 values with exc_*==0, replace exc_*=17 if one of the following sets of criteria are met:
      #   1.	If |d_agedays_other|>365.25 and |d_tbc*sd_other|>3; replace exc_*=17 for the value of the pair that has the largest abs_d_*_O. If abs_d_*O is missing, replace exc_*=17 for the value of the pair with the higher |tbc*sd|
      #   2.	If |d_agedays_other|<365.25 and |d_tbc*sd_other|>2; replace exc_*=18 for the value of the pair that has the largest abs_d_*_O. If abs_d_*O is missing, replace exc_*=18 for the value of the pair with the higher |tbc*sd|
      #
      # MODE default
      #   1.  uses current behavior outlined in b, in else statement
      #
      # MODE flag.both
      #   1.  similar to b, except in cases where only HTs values exist or only WTs values exist, both are dropped instead of only 1 (first if)
      #
      #
      # check whether each value in double has other parameters (make sure they don't first)
      if (lt3.exclude.mode == "flag.both" &&
          (is.na(df$tbc.other.sd[valid.rows[1]])) &&
          (is.na(df$tbc.other.sd[valid.rows[2]])) &&
          is.na(abs.delta.other)) {
        # if they don't then do test
        # check default thresholds, if they fail then exclude both regardless of tbc.sd
        if (na_as_false(delta.agedays.other >= 365.25 &
                        delta.tbc.sd.other > 3)) {
          df$exclude[valid.rows[1]] = 'Exclude-Pair-Delta-19'
          df$exclude[valid.rows[2]] = 'Exclude-Pair-Delta-19'
        } else if (na_as_false(delta.agedays.other < 365.25 &
                               delta.tbc.sd.other > 2)) {
          df$exclude[valid.rows[1]] = 'Exclude-Pair-Delta-19'
          df$exclude[valid.rows[2]] = 'Exclude-Pair-Delta-19'
        }

      } else {
        # default method if other mode is not used or if other parameters exist
        worst.row <- order(valid(df),
                           abs(df$tbc.sd),
                           abs(abs.delta.other),
                           decreasing = T)[1]
        if (na_as_false(delta.agedays.other >= 365.25 &
                        delta.tbc.sd.other > 3)) {
          df$exclude[worst.row] <- 'Exclude-Pair-Delta-17'
        } else if (na_as_false(delta.agedays.other < 365.25 &
                               delta.tbc.sd.other > 2)) {
          df$exclude[worst.row] <- 'Exclude-Pair-Delta-18'
        }
      }
    }
    # c.	Identify subjects/parameters with exactly 1 value for which exc_*=0. This will include subjects/parameters for which a value was excluded in step 16b.
    #     Determine tbcOsd and median_tbcOsd as described in step 16a3 and 16a4 above.
    # d.	For subjects/parameters with 1 value for which exc_*=0,  replace exc_*=19 if one of the following sets of criteria are met:
    #   1.	|tbc*sd|>3 & |tbc*sd-tbcOsd|>5 & tbcOsd is not missing
    #   2.	|tbc*sd|>3 & |tbc*sd-median_tbcOsd|>5 & tbcOsd is missing & median_tbcOsd is not missing
    #   3.	|tbc*sd|>5 & tbcOsd is missing & median_tbcOsd is missing
    valid.rows <- valid(df)
    # NOTE: valid.rows is now a boolean.  was a row number in code above
    if (sum(valid.rows) == 1) {
      #cat("param = ", df[valid.rows, param])
      df[valid.rows &
           (
             abs(tbc.sd) > 3 &
               abs(
                 tbc.sd - ifelse(
                   !is.na(tbc.other.sd),
                   tbc.other.sd,
                   median.tbc.other.sd
                 )
               ) > 5 |
               abs(tbc.sd) > 5 &
               is.na(tbc.other.sd) & is.na(median.tbc.other.sd)
           ),
         exclude := 'Exclude-Single-Outlier']
    }

    # ensure factor levels didn't accidentally get mangled
    return(factor(df$exclude, levels = exclude.levels, ordered = T))

  })(copy(.SD)), by = .(subjid, param), .SDcols = c('agedays', 'tbc.sd', 'tbc.other.sd', 'exclude', 'param')] #added param here for debugging, i think it's dropped otherwise

  # 17.  Exclude measurements based on error load for the subject
  # a.	For each subject/parameter determine the following:
  #   1.	tot_exc_*=the total number of values for which exc_* is equal to 4, 5, 6, or 8-19
  #   2.	tot_inc_*=the total number of values for which exc_*=0
  # b.	For subjects/parameters where tot_exc_*>0.5 x tot_inc_*; replace all values where exc_* had been 0 to exc_*=20
  # c.	For subjects/parameters where tot_exc_*>tot_inc_*, replace all values for the OTHER parameter where exc_* had been 0 to exc_*=21
  #
  # CD e-mail 2/10/15: It looks like there are two problems. One is carried forward measurements. The other is that I forgot to include an
  # important part of the rule in the English - tot_exc_* has to be >=2. I'm forwarding an updated English version.
  # Also, because your exclusions are handled a little differently I wanted to specify that unit errors and swaps are not included
  # in tot_exc_* and are included in tot_inc_*, whereas carried forwards and duplicates are not included in either count.
  # NOTE: updated to include optional argument "include.carryforward=T" in the valid() function, and added the
  # constraints "exclude.count.this.parameter >= 2" and "exclude.count.other.parameter >= 2" in the code below

  if (!quietly)
    cat(
      sprintf(
        "[%s] Exclude all measurements if maximum threshold of errors is exceeded...\n",
        Sys.time()
      )
    )
  # NOTE: restrict analysis to non-missing rows
  data.df[exclude != 'Missing', exclude := (function(subj.df) {
    inc.exc <- subj.df[, j = list(exclude.count = sum(
      !valid(
        exclude,
        include.duplicates = T,
        include.carryforward = T
      )
    ),
    include.count = sum(valid(exclude))), keyby = param]
    for (p in unique(subj.df$param)) {
      exclude.count.this.parameter <- inc.exc[param == p, exclude.count]
      exclude.count.other.parameter <- sum(inc.exc[param != p, exclude.count])
      if (exclude.count.this.parameter > error.load.threshold * inc.exc[param ==
                                                                        p, include.count] &
          exclude.count.this.parameter >= error.load.mincount) {
        subj.df[param == p &
                  valid(exclude), exclude := 'Exclude-Too-Many-Errors']
      } else if (exclude.count.other.parameter > sum(inc.exc[param != p, include.count]) &
                 exclude.count.other.parameter >= error.load.mincount) {
        subj.df[param == p &
                  valid(exclude), exclude := 'Exclude-Too-Many-Errors-Other-Parameter']
      }
    }
    # ensure factor levels didn't accidentally get mangled
    return(factor(subj.df$exclude, levels = exclude.levels, ordered =
                    T))
  })(copy(.SD)), by = subjid, .SDcols = c('param', 'exclude')]

  if (!quietly)
    cat(sprintf("[%s] Completed Batch #%s...\n", Sys.time(), data.df$batch[1]))
  if (!quietly & parallel)
    sink()
  return(data.df[j = .(line, exclude, tbc.sd, tbc.other.sd, param)]) #debugging
}

#' Clean growth measurements
#'
#' @param subjid Vector of unique identifiers for each subject in the database.
#' @param param Vector identifying each measurement, may be 'WEIGHTKG', 'HEIGHTCM', or 'LENGTHCM'
#'   'HEIGHTCM' vs. 'LENGTHCM' only affects z-score calculations between ages 24 to 35 months (730 to 1095 days).
#'   All linear measurements below 731 days of life (age 0-23 months) are interpreted as supine length, and
#'   all linear measurements above 1095 days of life (age 36+ months) are interpreted as standing height.
#' @param agedays Numeric vector containing the age in days at each measurement.
#' @param sex Vector identifying the gender of the subject, may be 'M', 'm', or 0 for males, vs. 'F', 'f' or 1 for females.
#' @param measurement Numeric vector containing the actual measurement data.  Weight must be in
#'   kilograms (kg), and linear measurements (height vs. length) in centimeters (cm).
#' @param recover.unit.error Indicates whether the cleaning algorithm should
#' attempt to identify unit errors (I.e. inches vs. cm, lbs vs. kg). If unit
#' errors are identified, the value will be corrected and retained within the
#' cleaning algorithm as a valid measurement.  Defaults to FALSE.
#' @param sd.extreme Measurements more than sd.extreme standard deviations from
#' the mean (either above or below) will be flagged as invalid. Defaults to 25.
#' @param z.extreme Measurements with an absolute z-score greater than
#' z.extreme will be flagged as invalid. Defaults to 25.
#' @param lt3.exclude.mode Determines type of exclusion procedure to use for 1 or 2 measurements of one type without
#' matching same ageday measurements for the other parameter. Options include "default" (standard growthcleanr approach),
#' and "flag.both" (in case of two measurements of one type without matching values for the other parameter, flag both
#' for exclusion if beyond threshold)
#' @param height.tolerance.cm maximum decrease in height tolerated for sequential measurements
#' @param error.load.mincount minimum count of exclusions on parameter before
#' considering excluding all measurements. Defaults to 2.
#' @param error.load.threshold threshold of percentage of excluded measurement count to included measurement
#' count that must be exceeded before excluding all measurements of either parameter. Defaults to 0.5.
#' @param sd.recenter Data frame or table with median SD-scores per day of life
#' by gender and parameter. Columns in the table must include param, sex,
#' agedays, and sd.median.  If not supplied, the median values will be
#' calculated using the growth data that is being cleaned. Defaults to NA.
#' @param sdmedian.filename Name of file to save sd.median data calculated on the input dataset to as CSV.
#' Defaults to "", for which this data will not be saved. Use for extracting medians for parallel processing
#' scenarios other than the built-in parallel option.
#' @param sdrecentered.filename Name of file to save re-centered data to as CSV. Defaults to "", for which this
#' data will not be saved. Useful for post-processing and debugging.
#' @param include.carryforward Determines whether Carry-Forward values are kept in the output. Defaults to False.
#' @param ewma.exp Exponent to use for weighting measurements in the
#' exponentially weighted moving average calculations. Defaults to -1.5.
#' This exponent should be negative in order to weight growth measurements
#' closer to the measurement being evaluated more strongly. Exponents that are
#' further from zero (e.g. -3) will increase the relative influence of
#' measurements close in time to the measurement being evaluated compared to
#' using the default exponent.
#' @param ref.data.path Path to reference data. If not supplied, the year 2000
#' Centers for Disease Control (CDC) reference data will be used.
#' @param log.path Path to log file output when running in parallel (non-quiet mode). Default is ".". A new
#' directory will be created if necessary.
#' @param parallel Determines if function runs in parallel.  Defaults to FALSE.
#' @param num.batches Specify the number of batches to run in parallel. Only
#' applies if parallel is set to TRUE. Defaults to the number of workers
#' returned by the getDoParWorkers function in the foreach package.
#' @param quietly Determines if function messages are to be displayed and if log files (parallel only) are to be generated.
#' Defaults to TRUE
#'
#' @return Vector of exclusion codes for each of the input measurements.
#'
#'    Possible values for each code are:
#'
#' * 'Include', 'Unit-Error-High', 'Unit-Error-Low', 'Swapped-Measurements', 'Missing',
#' *  'Exclude-Carried-Forward', 'Exclude-SD-Cutoff', 'Exclude-EWMA-Extreme', 'Exclude-EWMA-Extreme-Pair',
#' *  'Exclude-Duplicate',
#' *  'Exclude-EWMA-8', 'Exclude-EWMA-9', 'Exclude-EWMA-10', 'Exclude-EWMA-11', 'Exclude-EWMA-12', 'Exclude-EWMA-13', 'Exclude-EWMA-14',
#' *  'Exclude-Min-Height-Change', 'Exclude-Max-Height-Change',
#' *  'Exclude-Pair-Delta-17', 'Exclude-Pair-Delta-18', 'Exclude-Pair-Delta-19',
#' *  'Exclude-Single-Outlier', 'Exclude-Too-Many-Errors', 'Exclude-Too-Many-Errors-Other-Parameter'
#' @md
#'
#' @export
#' @import data.table
#' @rawNamespace import(plyr, except = c(failwith, id, summarize, count, desc, mutate, arrange, rename, is.discrete, summarise, summarize))
#' @import foreach
#' @import doParallel
#' @import parallel
#' @examples
#' # Run calculation using a small subset of given data
#' df_stats <- as.data.frame(syngrowth)
#' df_stats <- df_stats[df_stats$subjid %in% unique(df_stats[, "subjid"])[1:5], ]
#'
#' clean_stats <-cleangrowth(subjid = df_stats$subjid,
#'                          param = df_stats$param,
#'                          agedays = df_stats$agedays,
#'                          sex = df_stats$sex,
#'                          measurement = df_stats$measurement)
#'
#' # Once processed you can filter data based on result value
#' df_stats <- cbind(df_stats, "clean_result" = clean_stats)
#' clean_df_stats <- df_stats[df_stats$clean_result == "Include",]
#'
#' # Parallel processing: run using 2 cores and batches
#' clean_stats <- cleangrowth(subjid = df_stats$subjid,
#'                            param = df_stats$param,
#'                            agedays = df_stats$agedays,
#'                            sex = df_stats$sex,
#'                            measurement = df_stats$measurement,
#'                            parallel = TRUE,
#'                            num.batches = 2)
cleangrowth <- function(subjid,
                        param,
                        agedays,
                        sex,
                        measurement,
                        recover.unit.error = F,
                        sd.extreme = 25,
                        z.extreme = 25,
                        lt3.exclude.mode = "default",
                        height.tolerance.cm = 2.5,
                        error.load.mincount = 2,
                        error.load.threshold = 0.5,
                        sd.recenter = NA,
                        sdmedian.filename = "",
                        sdrecentered.filename = "",
                        include.carryforward = F,
                        ewma.exp = -1.5,
                        ref.data.path = "",
                        log.path = ".",
                        parallel = F,
                        num.batches = NA,
                        quietly = T) {
  # organize data into a dataframe along with a line "index" so the original data order can be recovered
  data.all <- data.table(
    line = seq_along(measurement),
    subjid = as.factor(subjid),
    param,
    agedays = as.integer(agedays),
    v = ifelse(measurement == 0, NaN, measurement),
    sex = as.integer(ifelse(
      sex %in% c(0, 'm', 'M'), 0, ifelse(sex %in% c(1, 'f', 'F'), 1, NA)
    ))
  )

  # if parallel processing is desired, load additional modules
  if (parallel) {
    if (is.na(num.batches)) {
      num.batches <- getDoParWorkers()
    }
    # variables needed for parallel workers
    var_for_par <- c("temporary_duplicates", "valid", "swap_parameters",
                     "na_as_false")

    cl <- makeCluster(num.batches)
    clusterExport(cl = cl, varlist = var_for_par, envir = environment())
    registerDoParallel(cl)
  } else {
    if (is.na(num.batches))
      num.batches <- 1
  }

  setkey(data.all, subjid)
  subjid.unique <- data.all[j = unique(subjid)]
  batches.all <- data.table(
    subjid = subjid.unique,
    batch = sample(num.batches, length(subjid.unique), replace = T),
    key = 'subjid'
  )
  data.all <- batches.all[data.all]

  # load tanner height velocity data. sex variable is defined such that 0=male and 1=female
  # recode column names to match syntactic style ("." rather than "_" in variable names)
  tanner_ht_vel_path <- ifelse(
    ref.data.path == "",
    system.file("extdata/tanner_ht_vel.csv", package = "growthcleanr"),
    paste(ref.data.path, "tanner_ht_vel.csv", sep =
            "")
  )

  tanner.ht.vel <- fread(tanner_ht_vel_path)

  setnames(tanner.ht.vel,
           colnames(tanner.ht.vel),
           gsub('_', '.', colnames(tanner.ht.vel)))
  setkey(tanner.ht.vel, sex, tanner.months)
  # keep track of column names in the tanner data
  tanner.fields <- colnames(tanner.ht.vel)
  tanner.fields <- tanner.fields[!tanner.fields %in% c('sex', 'tanner.months')]

  who_max_ht_vel_path <- ifelse(
    ref.data.path == "",
    system.file("extdata/who_ht_maxvel_3sd.csv", package = "growthcleanr"),
    paste(ref.data.path, "who_ht_maxvel_3sd.csv", sep =
            "")
  )

  who_ht_vel_3sd_path <- ifelse(
    ref.data.path == "",
    system.file("extdata/who_ht_vel_3sd.csv", package = "growthcleanr"),
    paste(ref.data.path, "who_ht_vel_3sd.csv", sep =
            "")
  )
  who.max.ht.vel <- fread(who_max_ht_vel_path)
  who.ht.vel <- fread(who_ht_vel_3sd_path)
  setkey(who.max.ht.vel, sex, whoagegrp_ht)
  setkey(who.ht.vel, sex, whoagegrp_ht)
  who.ht.vel <- as.data.table(dplyr::full_join(who.ht.vel, who.max.ht.vel, by =
                                                c('sex', 'whoagegrp_ht')))

  setnames(who.ht.vel, colnames(who.ht.vel), gsub('_', '.', colnames(who.ht.vel)))
  setkey(who.ht.vel, sex, whoagegrp.ht)
  # keep track of column names in the who growth velocity data
  who.fields <- colnames(who.ht.vel)
  who.fields <- who.fields[!who.fields %in% c('sex', 'whoagegrp.ht')]

  # 1.  General principles
  # a.	All steps are done separately for each parameter unless otherwise noted
  # b.	All steps are done sorted by subject, parameter, and age (in days) for nonexcluded and nonmissing values only unless otherwise noted. This is very important.
  #     Sorting needs to be redone with each step to account for excluded and transformed  values.
  # c.	The next value refers to the value with the next highest age for the same parameter and the same subject, and the previous value refers to the value with the
  #     next lowest age for the same parameter and the same subject.
  # d.	You will need to set up a method for keeping track of whether a value is missing or excluded (and in what step). I use variables called exc_* that are =0
  #     if a value is to be included, =1 if missing, and =2 or higher if it is to be excluded, with each number indicating a different step. I also set up
  #     parameter-specific subjid_* variables that are = to subjid for included values and are blank if the value is missing or should be excluded. These subjid_*
  #     variables need to be updated with each step.
  # e.  All steps assume that data are sorted by subjid_*, parameter, and age (in days) for nonexcluded and nonmissing values only
  #     unless otherwise noted. Sorting needs to be redone after any transformations or exclusions to account for excluded and
  #     transformed values.
  # f.  The next value refers to the nonexcluded nonmissing value with the next highest age for the same parameter and the same
  #     subject, and the previous value refers to the nonexcluded nonmissing value with the next lowest age for the same parameter
  #     and the same subject.
  # g.  exc_* should only be replaced with a  higher value if exc_*==0 at the time of replacement, unless otherwise specified.


  # NOTE: in the R code below exclusion is documented as a series of factor levels, where all levels occuring before 'Exclude' in the sequence are considered
  # to be valid measurements.  We use the built in sorting of the data.table object and subsets rather than re-sorting at each step
  # to ensure that only valid measurements are used at the beginning of each step.
  # Also, unlike the Stata code, the measurement parameter (weight vs. height) is recorded as a factor in the data frame, rather than as a variable name

  # 2.  Data set-up
  # a.	I always code sex as 0=Male, 1=Female, so I recoded the variable sex that way and left a variable sexorigcode the way the data was sent to me (1=Female 2=Male)
  # b.	Remove rows that are duplicates for subjid, param, and measurement from further analysis
  #     NOTE: this step is not needed -- handled automatically by "temporary duplicate" step.
  # c.  I generated separate variables for weight (wt) and height (ht), as well as exc_* and subjid_* variables. Set exc_*=0 if value is not missing
  #     and exc_*=1 if value is missing. In all future steps, exc_* should only be changed if it is 0. This helps to keep track of which step excluded a value.
  #     I also kept the measurement variable there and untouched because sometimes wt and ht got transformed to something else.
  # d.	I made tables based on CDC growth curve parameters that include data for each day that I will send separately. The LMS parameters for each day are
  #     cubically interpolated from the values by month available on the CDC website. Create wt and ht z-scores for each value of each parameter (Z: WtZ and HtZ).
  # e.	There are variables in the table labelled cdc_*_csd_pos and cdc_*_csd_neg. For each age and sex, these correspond to Â½ of the absolute value of the
  #     median and the value with a z-score of +2 (csd_pos) and -2 (csd_neg). These can be created to generate a score similar to the z-score but with an
  #     important difference. The z-scores created using the LMS method account for skewness in the distribution of the parameters (particularly weight), which
  #     can lead to small changes in z-score with large changes in weight in subjects with very high weight, and relatively large changes in z-score for smaller
  #     changes in weight in subjects with low weights.  The score we will create can be called an SD-score (SDorig: WtSDorig and HtSDorig that is calculated by
  #     dividing the difference between the value and the median by the SD score (use csd_pos if the value is above the median, csd_neg if the value is below the
  #     median). These SD-scores, rather than z-scores, now form the basis for the algorithm.

  # calculate z scores
  if (!quietly)
    cat(sprintf("[%s] Calculating z-scores...\n", Sys.time()))
  measurement.to.z <- read_anthro(ref.data.path, cdc.only = T)
  data.all[, z.orig := measurement.to.z(param, agedays, sex, v)]

  # calculate "standard deviation" scores
  if (!quietly)
    cat(sprintf("[%s] Calculating SD-scores...\n", Sys.time()))
  data.all[, sd.orig := measurement.to.z(param, agedays, sex, v, T)]

  # sort by subjid, param, agedays
  setkey(data.all, subjid, param, agedays)

  # add a new convenience index for bookkeeping
  data.all[, index := 1:.N]

  # enumerate the different exclusion levels
  exclude.levels <- c(
    'Include',
    'Unit-Error-High',
    'Unit-Error-Low',
    'Unit-Error-Possible',
    'Swapped-Measurements',
    'Exclude',
    'Missing',
    'Exclude-Temporary-Duplicate',
    'Exclude-Carried-Forward',
    'Exclude-SD-Cutoff',
    'Exclude-EWMA-Extreme',
    'Exclude-EWMA-Extreme-Pair',
    'Exclude-Duplicate',
    'Exclude-EWMA-8',
    'Exclude-EWMA-9',
    'Exclude-EWMA-10',
    'Exclude-EWMA-11',
    'Exclude-EWMA-12',
    'Exclude-EWMA-13',
    'Exclude-EWMA-14',
    'Exclude-Min-Height-Change',
    'Exclude-Max-Height-Change',
    'Exclude-Pair-Delta-17',
    'Exclude-Pair-Delta-18',
    'Exclude-Pair-Delta-19',
    'Exclude-Single-Outlier',
    'Exclude-Too-Many-Errors',
    'Exclude-Too-Many-Errors-Other-Parameter'
  )

  # Mark missing values for exclusion
  data.all[, exclude := factor(with(data.all, ifelse(
    is.na(v) |
      agedays < 0, 'Missing', 'Include'
  )),
  levels = exclude.levels,
  ordered = T)]

  # after calculating z scores, for convenience, recategorize linear parameters as 'HEIGHTCM'
  data.all[param == 'LENGTHCM', param := 'HEIGHTCM']

  # define field names needed by helper functions
  ewma.fields <- c('ewma.all', 'ewma.before', 'ewma.after')

  # 3.  SD-score recentering: Because the basis of the method is comparing SD-scores over time, we need to account for the fact that
  #     the mean SD-score for the population changes with age.
  # a.	Determine the median cdc*sd for each parameter by year of age (with sexes combined): median*sd.
  # b.	The median*sd should be considered to apply to midyear-age, defined as the age in days with the same value as the integer
  #     portion of (365.25*year + 365.25/2).
  # c.	Linearly interpolate median*sd for each parameter between each midyear-age, naming the interpolated values rc*sd.
  # d.	For ages below the first midyear-age, let rc*sd equal the median*sd for the earliest year.
  #     For ages above the last midyear_age, let rc*sd equal the median*sd for the last year.
  # e.	Subtract rcsd_* from SDorig to create the recentered SD-score.  This recentered SD-score, labeled tbc*sd
  #     (stands for "to be cleaned") will be used for most of the rest of the analyses.
  # f.	In future steps I will sometimes refer to measprev and measnext which refer to the previous or next wt or ht measurement
  #     for which exc_*==0 for the subject and parameter, when the data are sorted by subject, parameter, and agedays. SDprev and SDnext refer to the tbc*sd of the previous or next measurement.

  if (!quietly)
    cat(sprintf("[%s] Re-centering data...\n", Sys.time()))

  # see function definition below for explanation of the re-centering process
  # returns a data table indexed by param, sex, agedays
  if (!is.data.table(sd.recenter)) {
    sd.recenter <- data.all[exclude < 'Exclude', sd_median(param, sex, agedays, sd.orig)]
    if (sdmedian.filename != "") {
      write.csv(sd.recenter, sdmedian.filename, row.names = F)
      if (!quietly)
        cat(
          sprintf(
            "[%s] Wrote re-centering medians to %s...\n",
            Sys.time(),
            sdmedian.filename
          )
        )
    }
  } else {
    # ensure passed-in medians are sorted correctly
    setkey(sd.recenter, param, sex, agedays)
  }
  # add sd.recenter to data, and recenter
  setkey(data.all, param, sex, agedays)
  data.all <- sd.recenter[data.all]
  setkey(data.all, subjid, param, agedays)
  data.all[, tbc.sd := sd.orig - sd.median]

  if (sdrecentered.filename != "") {
    write.csv(data.all, sdrecentered.filename, row.names = F)
    if (!quietly)
      cat(
        sprintf(
          "[%s] Wrote re-centered data to %s...\n",
          Sys.time(),
          sdrecentered.filename
        )
      )
  }

  # safety check: treat observations where tbc.sd cannot be calculated as missing
  data.all[is.na(tbc.sd), exclude := 'Missing']

  # NOTE: the rest of cleangrowth's steps are done through cleanbatch().

  # optionally process batches in parallel
  if (!quietly)
    cat(sprintf(
      "[%s] Cleaning growth data in %d batch(es)...\n",
      Sys.time(),
      num.batches
    ))
  if (num.batches == 1) {
    ret.df <- cleanbatch(data.all,
                         log.path = log.path,
                         quietly = quietly,
                         parallel = parallel,
                         measurement.to.z = measurement.to.z,
                         ewma.fields = ewma.fields,
                         ewma.exp = ewma.exp,
                         recover.unit.error = recover.unit.error,
                         include.carryforward = include.carryforward,
                         sd.extreme = sd.extreme,
                         z.extreme = z.extreme,
                         exclude.levels = exclude.levels,
                         tanner.ht.vel = tanner.ht.vel,
                         who.ht.vel = who.ht.vel,
                         lt3.exclude.mode = lt3.exclude.mode,
                         error.load.threshold = error.load.threshold,
                         error.load.mincount = error.load.mincount)
  } else {
    # create log directory if necessary
    if (!quietly)
      cat(sprintf("[%s] Writing batch logs to '%s'...\n", Sys.time(), log.path))
    ifelse(!dir.exists(log.path), dir.create(log.path), FALSE)

    ret.df <- ddply(
      data.all,
      .(batch),
      cleanbatch,
      .parallel = parallel,
      .paropts = list(.packages = "data.table"),
      log.path = log.path,
      quietly = quietly,
      parallel = parallel,
      measurement.to.z = measurement.to.z,
      ewma.fields = ewma.fields,
      ewma.exp = ewma.exp,
      recover.unit.error = recover.unit.error,
      include.carryforward = include.carryforward,
      sd.extreme = sd.extreme,
      z.extreme = z.extreme,
      exclude.levels = exclude.levels,
      tanner.ht.vel = tanner.ht.vel,
      who.ht.vel = who.ht.vel,
      lt3.exclude.mode = lt3.exclude.mode,
      error.load.threshold = error.load.threshold,
      error.load.mincount = error.load.mincount
    )
    stopCluster(cl)
  }
  if (!quietly)
    cat(sprintf("[%s] Done!\n", Sys.time()))

  return(ret.df$exclude[order(ret.df$line)])

}

#' Function to calculate z-scores and csd-scores based on anthro tables.
#'
#' @param path Path to supplied reference anthro data. Defaults to package anthro tables.
#' @param cdc.only Whether or not only CDC data should be used. Defaults to false.
#'
#' @return Function for calculating BMI based on measurement, age in days, sex, and measurement value.
#' @export
#' @import data.table
#' @examples
#' # Return calculating function with all defaults
#' afunc <- read_anthro()
#'
#' # Return calculating function while specifying a path and using only CDC data
#' afunc <- read_anthro(path = system.file("extdata", package = "growthcleanr"),
#'                      cdc.only = TRUE)
read_anthro <- function(path = "", cdc.only = F) {
  # set correct path based on input reference table path (if any)
  weianthro_path <- ifelse(
    path == "",
    system.file(file.path("extdata","weianthro.txt"), package = "growthcleanr"),
    file.path(path, "weianthro.txt")
  )
  lenanthro_path <- ifelse(
    path == "",
    system.file(file.path("extdata","lenanthro.txt"), package = "growthcleanr"),
    file.path(path, "lenanthro.txt")
  )
  bmianthro_path <- ifelse(
    path == "",
    system.file(file.path("extdata","bmianthro.txt"), package = "growthcleanr"),
    file.path(path, "bmianthro.txt")
  )
  growth_cdc_ext_path <- ifelse(
    path == "",
    system.file(file.path("extdata","growthfile_cdc_ext.csv"), package = "growthcleanr"),
    file.path(path, "growthfile_cdc_ext.csv")
  )


  growth_cdc_ext <- read.csv(growth_cdc_ext_path)

  l <- list(
    with(
      read.table(weianthro_path, header = T),
      data.frame(
        src = 'WHO',
        param = 'WEIGHTKG',
        sex = sex - 1,
        age,
        l,
        m,
        s,
        csdpos = as.double(NA),
        csdneg = as.double(NA)
      )
    ),
    with(
      read.table(lenanthro_path, header = T),
      data.frame(
        src = 'WHO',
        param = 'HEIGHTCM',
        sex = sex - 1,
        age,
        l,
        m,
        s,
        csdpos = as.double(NA),
        csdneg = as.double(NA)
      )
    ),
    with(
      read.table(bmianthro_path, header = T),
      data.frame(
        src = 'WHO',
        param = 'BMI',
        sex = sex - 1,
        age,
        l,
        m,
        s,
        csdpos = as.double(NA),
        csdneg = as.double(NA)
      )
    ),
    with(
      growth_cdc_ext,
      data.frame(
        src = 'CDC',
        param = 'WEIGHTKG',
        sex,
        age = agedays,
        l = cdc_wt_l,
        m = cdc_wt_m,
        s = cdc_wt_s,
        csdpos = cdc_wt_csd_pos,
        csdneg = cdc_wt_csd_neg
      )
    ),
    with(
      growth_cdc_ext,
      data.frame(
        src = 'CDC',
        param = 'HEIGHTCM',
        sex,
        age = agedays,
        l = cdc_ht_l,
        m = cdc_ht_m,
        s = cdc_ht_s,
        csdpos = cdc_ht_csd_pos,
        csdneg = cdc_ht_csd_neg
      )
    ),
    with(
      growth_cdc_ext,
      data.frame(
        src = 'CDC',
        param = 'BMI',
        sex,
        age = agedays,
        l = cdc_bmi_l,
        m = cdc_bmi_m,
        s = cdc_bmi_s,
        csdpos = cdc_bmi_csd_pos,
        csdneg = cdc_bmi_csd_neg
      )
    )
  )

  anthro <- rbindlist(l)


  setkey(anthro, src, param, sex, age)

  return(function(param, agedays, sex, measurement, csd = F) {
    # For now, we will only use CDC growth reference data, note that the cubically interpolated file
    # we are using has linear measurments derived from length data for children < 731 days, and height thereafter
    src <- ifelse(agedays < 731 & !cdc.only, 'WHO', 'CDC')

    # keep column sequence the same fo efficient join
    dt <- data.table(src, param, sex, agedays, measurement)
    dt <- anthro[dt]

    dt[, ret := as.double(NA)]
    if (csd) {
      dt[measurement < m, ret := (measurement - m) / csdneg]
      dt[measurement >= m, ret := (measurement - m) / csdpos]
    } else {
      dt[l == 0, ret := log(measurement / m) / s]
      dt[l != 0, ret := (((measurement / m) ^ l) - 1) / (l * s)]
    }

    return(dt$ret)
  })
}

#' Exponentially Weighted Moving Average (EWMA)
#'
#' \code{ewma} calculates the exponentially weighted moving average (EWMA) for a set of numeric observations over time.
#'
#' @param agedays Vector of age in days for each z score (potentially transformed to adjust weighting).
#'
#' @param z Input vector of numeric z-score data.
#'
#' @param ewma.exp Exponent to use for weighting.
#'
#' @param ewma.adjacent Specify whether EWMA values excluding adjacent measurements should be calculated.  Defaults to TRUE.
#'
#' @return Data frame with 3 variables:
#' * The first variable (ewma.all) contains the EWMA at observation time
#'   excluding only the actual observation for that time point.
#' * The second variable (ewma.before) contains the EWMA for each observation excluding both the actual observation
#'   and the immediate prior observation.
#' * The third variable (ewma.after) contains the EWMA for each observation excluding both the actual observation
#'   and the subsequent observation.
#'@md
#'
#' @export
#' @examples
#' # Run on 1 subject, 1 type of parameter
#' df_stats <- as.data.frame(syngrowth)
#' df_stats <- df_stats[df_stats$subjid == df_stats$subjid[1] &
#'                        df_stats$param == "HEIGHTCM", ]
#'
#' # Get the uncentered z-scores
#' measurement_to_z <- read_anthro(cdc.only = TRUE)
#' sd <- measurement_to_z(df_stats$param,
#'                        df_stats$agedays,
#'                        df_stats$sex,
#'                        df_stats$measurement,
#'                        TRUE)
#'
#' # Calculate exponentially weighted moving average
#' e_df <- ewma(df_stats$agedays, sd, ewma.exp = -1.5)
ewma <- function(agedays, z, ewma.exp, ewma.adjacent = T) {
  # 6.  EWMA calculation description: Most of the next steps will involve calculating the exponentially weighted moving average for each subject and parameter. I will
  #     describe how to calculate EWMASDs, and will describe how it needs to be varied in subsequent steps.
  # a.	The overall goal of the EWMASD calculation is to identify the difference between the SD-score and what we might predict that DS-score should be, in order to
  #     determine whether it should be excluded.
  # b.	Only nonmissing SD-scores for a parameter that are not designated for exclusion are included in the following calculations.
  # c.	For each SD-score SDi and associated agedaysi calculate the following for every other z-score (SDjâ€¦SDn) and associated agedays (agedaysjâ€¦agedaysn)  for the
  #     same subject and parameter
  #   i.	Î”Agej=agedaysj-agedaysi
  #   ii.	EWMAZ=SDi=[Î£jâ†’n(SDj*((5+Î”Agej)^-1.5))]/[ Î£jâ†’n((5+Î”Agej)^-1.5)]
  #   iii.	For most EWMASD calculations, there are 3 EWMASDs that need to be calculated. I will note if not all of these need to be done for a given step.
  #     1.	EWMASDall calculated as above
  #     2.	EWMAZbef calculated excluding the SD-score just before the SD-score of interest (sorted by agedays). For the first observation for a parameter for a
  #         subject, this should be identical to EWMASDall rather than missing.
  #     3.	EWMAZaft calculated excluding the z-score just after the SD-score of interest (sorted by agedays). For the lastobservation for a parameter for a subject,
  #         this should be identical to EWMASDall rather than missing.
  #   iv.	For each of the three EWMASDs, calculate the dewma_*=SD-EWMASD
  # d.	EWMASDs and Î”EWMASDs will change if a value is excluded or manipulated using one of the methods below, therefore EWMASDs and Î”EWMASDs be recalculated for each
  #     step where they are needed.
  # e.	For these calculations, use variables that allow for precise storage of numbers (in Stata this is called 'double') because otherwise rounding errors can cause
  #     problems in a few circumstances

  n <- length(agedays)
  # initialize response variables
  ewma.all <- ewma.before <- ewma.after <- vector('numeric', 0)
  if (n > 0) {
    # organize into data frame and sort into order of increasing age,
    # but retain original sort order information in index
    if (!all(agedays == cummax(agedays)))
      warning("EWMA ordering is not sorted; double check") #add in a check to make sure the inputs are already sorted (they should be)
    index <- order(agedays)


    # calculate matrix of differences in age, and add 5 to each delta per Daymont algorithm
    delta <- as_matrix_delta(agedays)
    delta <- ifelse(delta == 0, 0, (delta + 5) ^ ewma.exp)

    # calculate EWMAs, and return in order of original data
    ewma.all[index] <- delta %*% z / apply(delta, 1, sum)

    if (ewma.adjacent) {
      if (n > 2) {
        delta2 <- delta
        delta2[col(delta2) == row(delta2) - 1] <- 0
        ewma.before[index] <- delta2 %*% z / apply(delta2, 1, sum)
        delta3 <- delta
        delta3[col(delta3) == row(delta3) + 1] <- 0
        ewma.after[index] <- delta3 %*% z / apply(delta3, 1, sum)
      } else {
        ewma.before <- ewma.after <- ewma.all
      }
    }
  }
  # return all 3 EWMAs as a data frame
  return(if (ewma.adjacent)
    data.frame(ewma.all, ewma.before, ewma.after)
    else
      data.frame(ewma.all))
}

#' Function to convert vector of ages, in days, into matrix
#'
#' @keywords internal
#' @noRd
as_matrix_delta <- function(agedays) {
  n <- length(agedays)
  delta <- abs(matrix(rep(agedays, n), n, byrow = T) - agedays)

  return(delta)
}

#' Calculate median SD score by age for each parameter.
#'
#' @param param Vector identifying each measurement, may be 'WEIGHTKG', or 'HEIGHTCM'.
#' @param agedays Numeric vector containing the age in days at each measurement.
#' @param sex Vector identifying the gender of the subject, may be 'M', 'm', or 0 for males, vs. 'F', 'f' or 1 for females.
#' @param sd.orig Vector of previously calculated standard deviation (SD) scores for each measurement before re-centering.
#'
#' @return Table of data with median SD-scores per day of life by gender and parameter.
#'
#' @export
#' @import data.table
#' @examples
#' # Run on 1 subject
#' df_stats <- as.data.frame(syngrowth)
#' df_stats <- df_stats[df_stats$subjid == df_stats$subjid[1], ]
#'
#' # Get the original standard deviations
#' measurement_to_z <- read_anthro(cdc.only = TRUE)
#' sd.orig <- measurement_to_z(df_stats$param,
#'                        df_stats$agedays,
#'                        df_stats$sex,
#'                        df_stats$measurement,
#'                        TRUE)
#'
#' # Calculate median standard deviations
#' sd.m <- sd_median(df_stats$param,
#'                   df_stats$sex,
#'                   df_stats$agedays,
#'                   sd.orig)
sd_median <- function(param, sex, agedays, sd.orig) {
  # 3.  SD-score recentering: Because the basis of the method is comparing SD-scores over time, we need to account for the fact that
  #     the mean SD-score for the population changes with age.
  # a.  Determine the median cdc*sd for each parameter by year of age (with sexes combined): median*sd.
  # b.	The median*sd should be considered to apply to midyear-age, defined as the age in days with the same value as the integer
  #     portion of (365.25*year + 365.25/2).
  # c.	Linearly interpolate median*sd for each parameter between each midyear-age, naming the interpolated values rc*sd.
  # d.	For ages below the first midyear-age, let rc*sd equal the median*sd for the earliest year.
  #     For ages above the last midyear_age, let rc*sd equal the median*sd for the last year.
  # e.	Subtract rcsd_* from SDorig to create the recentered SD-score.  This recentered SD-score, labeled tbc*sd
  #     (stands for "to be cleaned") will be used for most of the rest of the analyses.
  # f.	In future steps I will sometimes refer to measprev and measnext which refer to the previous or next wt or ht measurement
  #     for which exc_*==0 for the subject and parameter, when the data are sorted by subject, parameter, and agedays. SDprev and SDnext refer to the tbc*sd of the previous or next measurement.
  dt <- data.table(param, sex, agedays, ageyears = floor(agedays / 365.25), sd.orig)
  setkey(dt, param, sex, agedays)
  # determine ages (in days) we need to cover from min to max age in years
  agedays.to.cover <- dt[, (floor(min(ageyears) * 365.25):floor(max(ageyears +
                                                                     1) * 365.25))]
  # group all measurements above age 19 together (copied from CD stata code e-mailed 4/3/15)
  dt[ageyears > 19, ageyears := 19]
  dt.median <- dt[!is.na(sd.orig), list(sex = c(0, 1), sd.median = rep(median(sd.orig), 2)), by =
                   .(param, ageyears)]
  dt.median <- dt.median[, list(
    agedays = agedays.to.cover,
    sd.median = approx(
      floor((ageyears + 0.5) * 365.25),
      sd.median,
      xout = agedays.to.cover,
      rule = 2
    )$y
  ), by = .(param, sex)]
  setkey(dt.median, param, sex, agedays)
  return(dt.median)
}
