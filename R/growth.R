#' Helper function for cleanbatch to identify subset of observations that are either "included" or a "temporary duplicate"
#'
#' @keywords internal
#' @noRd
valid <- function(df,
                  include.temporary.duplicates = FALSE,
                  include.duplicates = FALSE,
                  include.carryforward = FALSE) {
  exclude <- if (is.data.frame(df)) df$exclude else df
  return(
    exclude < "Exclude" |
      include.temporary.duplicates & exclude == "Exclude-Temporary-Duplicate" |
      include.duplicates & exclude == "Exclude-Duplicate" |
      include.carryforward &exclude == "Exclude-Carried-Forward"
  )
}

#' Helper function to treat NA values as FALSE
#'
#' @keywords internal
#' @noRd
na_as_false <- function(v) {
  v[is.na(v)] <- FALSE
  v
}

#' Function for temporary duplicates (step 5):
#' 5.  Temporary duplicates: I use duplicates to refer to more than more than one recorded value for a parameter on the same day,
#'     and we need to select which one to include in our analysis. The overall strategy will be to select a measurement using a simple
#'     strategy that will be used temporarily, and select permanently in a later step after we have a somewhat cleaner dataset that
#'     can help us identify the best duplicate.
#' a.  For subjects/parameters with duplicates: Determine median_tbc*sd for both parameters: the median tbc*sd for each subject
#'     and parameter including only non-duplicate values with exc_*==0. The median of the same parameter as the duplicate will
#'     be referred to as median_tbc*sd, the median of the other parameter will be referred to as median_tbcOsd.
#' b.	For each subject/parameter with duplicates and at least one value for the subject/parameter on a day with no duplicates,
#'     select the value closest to the median_tbc*sd for temporary inclusion, and assign all other duplicates exc_*=2.
#'  i.	For each subject/parameter with duplicates and no values for the subject/parameter on a day with no duplicates, select the
#'     value closest to the median_tbcOsd for temporary inclusion, and assign all other duplicates exc_*=2.
#'     If median_tbcOsd is missing because there are no values for the other parameter, randomly choose one duplicate value for
#'     each subject/parameter/age to keep as exc_*=0 and replace exc_*=2 for all other duplicates for that subject/parameter/age.
#'
#' @keywords internal
#' @noRd
temporary_duplicates <- function(df) {
  # add subjid and param if needed (may be missing depending on where this is called from)
  if (is.null(df$subjid)) {
    df[, subjid := NA]
  }
  if (is.null(df$param)) {
    df[, param := NA]
  }
  # only operate on valid rows (but include rows that may have previously been flagged as a "temporary duplicate")
  valid.rows <- valid(df, include.temporary.duplicates = TRUE)
  # make a small copy of df with only the fields we need for efficiency
  df <- df[j = .(tbc.sd), keyby = .(subjid, param, agedays, index)]
  # initialize some useful fields
  df[, `:=`(
    median.sd = as.double(NA),
    delta.median.sd = as.double(NA),
    duplicates.this.day = FALSE,
    duplicate = FALSE
  )]
  # determine on which days there are duplicate measurements (more than 1 valid measurement on that age day)
  df[valid.rows, duplicates.this.day := (.N > 1), by = .(subjid, param, agedays)]
  # calculate median of measurements on days where there is no duplicate
  df[valid.rows & !duplicates.this.day, median.sd := median(tbc.sd), by = .(subjid, param)]
  # distribute median to other valid or potential duplicate rows for same parameter for that subject
  df[valid.rows, median.sd := sort(median.sd)[1], by = .(subjid, param)]
  # take care of subject/parameters with more than one day with a valid observation
  # determine the absolute difference between the measurements sd score and the median for that parameter for each child
  df[valid.rows & duplicates.this.day, delta.median.sd := abs(tbc.sd - median.sd), by = .(subjid, param)]
  # identify subjects that have duplicates on all days of observation for that parameter (i.e. delta.median.sd undefined)
  subj.all.dups <- df[valid.rows & duplicates.this.day & is.na(delta.median.sd), unique(subjid)]
  df[
    valid.rows & subjid %in% subj.all.dups,
    delta.median.sd := (function(subj.df) {
      # iterate over parameters where delta.median.sd is not yet defined
      # pass 1: take median from other parameter(s)
      for (p in subj.df[is.na(delta.median.sd) & duplicates.this.day, unique(param)]) {
        median.other.sd <- subj.df[param != p & !duplicates.this.day, median(tbc.sd)]
        subj.df[param == p, delta.median.sd := abs(tbc.sd - median.other.sd)]
      }
      return(subj.df$delta.median.sd)
    })(copy(.SD)),
    by = .(subjid)
  ]
  # Final pass: take median as zero (i.e. if no measurements from a different parameter)
  # NOTE -- this is not exactly the same as taking a random parameter
  df[valid.rows & duplicates.this.day & is.na(delta.median.sd), delta.median.sd := abs(tbc.sd)]
  # flag any duplicate value on the same day that is not the minimum distance from the median sd score
  # NOTE: in the case of exact ducplicates, "which.min" will pick the first
  df[
    valid.rows & duplicates.this.day,
    duplicate := seq_along(delta.median.sd) != which.min(delta.median.sd),
    by = .(subjid, param, agedays)
  ]
  # return the duplicated valid rows (i.e. the ones that should be temporarily excluded)
  return(df$duplicate & valid.rows)
}

#' Function to identify switches (step 6):
#'  7.  Identify switches (weight and height each recorded as the other parameter)
#' Utility function to find the value associated with the opposite parameter
#' For example, if param.1='WEIGHTKG' and param.2='HEIGHTCM', then a vector of height values observed on the same day as the weight values is returned and vice versa.
#'
#' @keywords internal
#' @noRd
swap_parameters <- function(param.1 = "WEIGHTKG",
                            param.2 = "HEIGHTCM",
                            field.name = "tbc.sd",
                            df) {
  valid.rows <- valid(df)
  # copy swap field to a new value for convenience in the code below
  df$swap <- df[, field.name, with = FALSE]
  # construct an indexed table with valid parameters for speed in swapping
  valid.other <- df[valid.rows, list(
    subjid.other = subjid,
    param.other = ifelse(
      param == param.1,
      param.2,
      ifelse(param == param.2, param.1, NA)
    ),
    agedays.other = agedays,
    swap
  )]
  setkeyv(valid.other, c("subjid.other", "param.other", "agedays.other"))
  # clear swap field (should retain original type)
  df[, swap := NA]
  # use indexed table for speed -- assign all values in one statement
  df[valid.rows, swap := valid.other[list(subjid, param, agedays), swap]]
  return(df$swap)
}

#' Function to clean data (optionally in batches):
#' 4.  Dataset split (optional)
#' a.	In Stata, many of the rest of the steps require you to repeat steps as many times as there are observations for the subject with the highest number of
#'     observations. Therefore, in order to speed things up I separated the dataset at this point into three groups: short (subjects with <=10 total
#'     observations), medium (subjects with >10 and <=30 observations) and long (subjects with >30 observations). I performed all the rest of the steps
#'     separately for each of the 3 datasets and then re-combined them at the end. Note that all observations for a subject stay together. This step is
#'     definitely not necessary if it will not speed things up in whatever software program you're using.
#'
#' NOTE: to use mutliple processes in R we will process patients in batches using the plyr ddply function
#' set up function to process one patient.  Function to parallelize batches is below.
#'
#' @keywords internal
#' @noRd
cleanbatch <- function(data.df,
                       log.path,
                       quietly,
                       parallel,
                       measurement.to.z,
                       ewma.fields,
                       ewma.exp,
                       recover.unit.error,
                       include.carryforward,
                       sd.extreme,
                       z.extreme,
                       exclude.levels,
                       tanner.ht.vel,
                       who.ht.vel,
                       lt3.exclude.mode,
                       error.load.threshold,
                       error.load.mincount) {
  data.df <- data.table(data.df, key = c("subjid", "param", "agedays", "index"))

  if (!quietly & parallel) {
    # use local directory as default for logs
    if (is.na(log.path)) log.path <- "."
    sink(
      sprintf(
        "%s/cleangrowth-%s-batch-%s.log",
        log.path,
        Sys.Date(),
        data.df$batch[1]
      )
    )
  }

  if (!quietly) {
    cat(sprintf(
      "[%s] Processing Batch #%s...\n",
      Sys.time(),
      data.df$batch[1]
    ))
  }

  # save a copy of all original measurement values before any transformation
  data.df[, v.orig := v]

  if (!quietly) {
    cat(sprintf(
      "[%s] Preliminarily identify potential duplicates...\n",
      Sys.time()
    ))
  }
  data.df$exclude[temporary_duplicates(data.df)] <- "Exclude-Temporary-Duplicate"

  # capture a list of subjects with possible duplicates for efficiency later
  subj.dup <- data.df[exclude == "Exclude-Temporary-Duplicate", unique(subjid)]

  # 7a.  For each day on which a subject had both a weight and a height recorded, calculate tbc*sd_sw: SD scores as if the weight had been recorded as the height
  #      and the height had been recorded as the weight, recentered using rcsd_*.  I intentionally did not allow values that were the first or last for a subject/parameter to be replaced as a switch.
  # NOTE: this additional constraint related to first and last values is implemented below via the code
  # swap.flag.1 := if(.N>2) c(FALSE, rep(TRUE, .N-2), FALSE) else FALSE, by=.(subjid,param)

  if (!quietly) {
    cat(sprintf(
      "[%s] Identify potentially swapped measurements...\n",
      Sys.time()
    ))
  }

  data.df[, v.sw := swap_parameters(field.name = "v", df = data.df)]
  # calculate "standard deviation" score for the "swapped" parameter and recenter
  data.df[, tbc.sd.sw := measurement.to.z(param, agedays, sex, v.sw, TRUE) - sd.median]

  # 7b.  Perform a EWMA calculation.
  #   i.	In addition to the standard all/bef/aft dewma_* variables calculate all/bef/aft dewma_*_sw variables by subtracting EWMASD from tbc*sd_sw
  data.df[, (ewma.fields) := as.double(NaN)]
  data.df[valid(exclude), (ewma.fields) := ewma(agedays, tbc.sd, ewma.exp, TRUE), by = .(subjid, param)]


  # 7c.  Label pairs of height/weight measurements recorded on the same day as a switch if ALL of the following criteria are met for BOTH parameters:
  #   i.	For the weight value:
  #       exc_wt==0 & tbcwtsd>4 & |tbcwtsd_sw|<3 & dewma_wt>3 & dewma_wt_bef>2 & dewma_wt_aft>2
  #         & |dewma_wt_sw|<0.3 & |dewma_wt_sw_bef|<0.5 & |dewma_wt_sw_aft|<0.5
  #   ii.	For the height value:
  #       exc_ht==0 & tbchtsd<-7 & |tbchtsd_sw|<3 & dewma_ht<-6 & dewma_ht_bef<-5 & dewma_ht_aft<-5 & |dewma_ht_sw|<0.3 & |dewma_ht_sw_bef|<0.5 & |dewma_ht_sw_aft|<0.5
  #

  # initialize swap.flag.1
  data.df[, `:=`(valid.interior.measurement = FALSE, swap.flag.1 = FALSE)]

  # flag interior measurements
  data.df[
    valid(data.df),
    valid.interior.measurement := if (.N > 2) {
      c(FALSE, rep(TRUE, .N - 2), FALSE)
    } else {
      FALSE
    },
    by = .(subjid, param)
  ]
  data.df[
    valid.interior.measurement &
      (
        param == "WEIGHTKG" &
          tbc.sd > 4 &
          abs(tbc.sd.sw) < 3 &
          (tbc.sd - ewma.all) > 3 &
          (tbc.sd - ewma.before) > 2 &
          (tbc.sd - ewma.after) > 2
        &
          abs(tbc.sd.sw - ewma.all) < 0.3 &
          abs(tbc.sd.sw - ewma.before) < 0.5 &
          abs(tbc.sd.sw - ewma.after) < 0.5
        |
          param == "HEIGHTCM" &
            tbc.sd < -7 &
            abs(tbc.sd.sw) < 3 &
            (tbc.sd - ewma.all) < -6 &
            (tbc.sd - ewma.before) < -5 &
            (tbc.sd - ewma.after) < -5
          &
            abs(tbc.sd.sw - ewma.all) < 0.3 &
            abs(tbc.sd.sw - ewma.before) < 0.5 &
            abs(tbc.sd.sw - ewma.after) < 0.5
      ),
    swap.flag.1 := TRUE
  ]


  # 7d.  For pairs of measurements that meet criteria for a switch, do the following:
  #   i.	Replace wt with the value that was originally recorded as the ht, and replace ht with the value that was originally recorded as the wt
  #   ii.	Replace tbc*sd with the values for tbc*sd_sw
  data.df$swap.flag.2 <- swap_parameters(field.name = "swap.flag.1", df = data.df)
  data.df[
    swap.flag.1 &swap.flag.2,
    `:=`(v = v.sw, tbc.sd = tbc.sd.sw, exclude = "Swapped-Measurements")
  ]

  # look for possible unit errors
  # calculate additional z-scores if we will be attempting to correct unit errors
  if (recover.unit.error) {
    # 8.  Identify unit errors (weight/height recorded in wrong units)
    # a.  Generate variables transformed by conversion factors for kg/lbs and inches/cm:
    #    i.	  wt_d_2=wt/2.204622
    #    ii.	wt_t_2=wt*2.204622
    #    iii.	ht_d_2=ht/2.54
    #    iv.	ht_t_2=ht*2.54
    if (!quietly) {
      cat(sprintf("[%s] Identify and recover unit errors...\n", Sys.time()))
    }
    valid.rows <- valid(data.df)
    data.df[, `:=`(
      v.d = v / ifelse(param == "WEIGHTKG", 2.204622, 2.54),
      v.t = v * ifelse(param == "WEIGHTKG", 2.204622, 2.54)
    )]

    # 8b.  Calculate SD scores for each transformed variable and recenter these (tbc*sd_d_2 and tbc_*sd_t_2)
    data.df$tbc.sd.d <- with(
      data.df,
      measurement.to.z(param, agedays, sex, v.d, TRUE) - sd.median
    )
    data.df$tbc.sd.t <- with(
      data.df,
      measurement.to.z(param, agedays, sex, v.t, TRUE) - sd.median
    )

    # 8c.  Perform a EWMA calculation
    #    i.	  In addition to the standard all/bef/aft dewma_* variables calculate all/bef/aft variables for  dewma_*_d_2 and dewma_*_t_2  by subtracting EWMASD from
    #         tbc*sd_d_2 and tbc_*_sd_t_2
    # NOTE: the additional dewma fields are calculated in the tests below
    data.df[, (ewma.fields) := as.double(NaN)]
    data.df[valid(exclude), (ewma.fields) := ewma(agedays, tbc.sd, ewma.exp, TRUE), by = .(subjid, param)]

    # 8d.  Also calculate d_prevsd_*=tbc*sd-tbc*sdprev and d_nextsd_*=tbc*sdi-tbc*sdnext. d_prevsd_* will be missing for the first value and d_nextsd_* should be
    #     missing for the last value. I intentionally did not allow values that were the first or last for a subject/parameter to be replaced as unit errors.
    # structure c(NA, field.name[-.N]) == get.prev
    # structure c(field.name[-1], NA) == get.next
    data.df[, `:=`(delta.prev.sd = as.double(NaN), delta.next.sd = as.double(NaN))]
    data.df[
      valid.rows,
      `:=`(
        delta.prev.sd = tbc.sd - c(NA, tbc.sd[-.N]),
        delta.next.sd = tbc.sd - c(tbc.sd[-1], NA)
      ),
      by = .(subjid, param)
    ]

    # 8e.	Identify a value as a unit error if one of the following sets of criteria are met:
    #    i.  	For wt_d_2: dewma_wt>3 & dewma_wt_bef>2 & dewma_wt_aft>2 & tbcwtsd>3 & d_nextsd_wt>2 & d_nextsd_wt is not missing & d_prevsd_wt>2 & d_prevsd_wt is not missing
    #        & abs(dewma_wt_d_2)<0.3 & abs(dewma_wt_d_2_bef)<0.5 & abs(dewma_wt_d_2_aft)<0.5 & abs(tbcwtsd_d_2)<3 & exc_wt==0
    #    ii.	For wt_t_2: dewma_wt<-3 & dewma_wt_bef<-2 & dewma_wt_aft<-2 & tbcwtsd<-3 & d_nextsd_wt<-2 & d_nextsd_wt is not missing & d_prevsd_wt<-2 & d_prevsd_wt is not missing
    #        & abs(dewma_wt_t_2)<0.3 & abs(dewma_wt_t_2_bef)<0.5 & abs(dewma_wt_t_2_aft)<0.5 & abs(tbcwtsd_t_2)<3 & exc_wt==0
    #    iii.	For ht_d_2: dewma_ht>5 & dewma_ht_bef>4 & dewma_ht_aft>4 & tbchtsd>7 & d_nextsd_ht>4 & d_nextsd_ht is not missing & d_prevsd_ht>4 & d_prevsd_ht is not missing
    #        & abs(dewma_ht_d_2)<0.3 & abs(dewma_ht_d_2_bef)<0.5 & abs(dewma_ht_d_2_aft)<0.5 & abs(tbchtsd_d_2)<3 & exc_ht==0
    #    iv.	For ht_t_2: dewma_ht<-5 & dewma_ht_bef<-4 & dewma_ht_aft<-4 & tbchtsd<-7 & d_nextsd_ht<-4 & d_nextsd_ht is not missing & d_prevsd_ht<-4 & d_prevsd_ht is not missing
    #        & abs(dewma_ht_t_2)<0.3 & abs(dewma_ht_t_2_bef)<0.5 & abs(dewma_ht_t_2_aft)<0.5 & abs(tbchtsd_t_2)<3 & exc_ht==0
    # f.  For values that are identified as unit errors by one of the sets of criteria above, replace wt or ht with the corresponding transformed value and
    #     replace tbc*sd with the recentered sd-score for the transformed value

    # process unit error low first (8.e.i and iii from above), re-factor slightly for efficiency
    data.df[
      valid(data.df) &
        abs(tbc.sd.d - ewma.all) < 0.3 &
        abs(tbc.sd.d - ewma.before) < 0.5 &
        abs(tbc.sd.d - ewma.after) < 0.5 & abs(tbc.sd.d) < 3
      &
        (
          param == "WEIGHTKG" &
            (tbc.sd - ewma.all) > 3 &
            (tbc.sd - ewma.before) > 2 &
            (tbc.sd - ewma.after) > 2 & tbc.sd > 3
          &
            !is.na(delta.next.sd) &
            delta.next.sd > 2 &
            !is.na(delta.prev.sd) & delta.prev.sd > 2
          |
            param == "HEIGHTCM" &
              (tbc.sd - ewma.all) > 5 &
              (tbc.sd - ewma.before) > 4 &
              (tbc.sd - ewma.after) > 4 & tbc.sd > 7
            &
              !is.na(delta.next.sd) &
              delta.next.sd > 4 &
              !is.na(delta.prev.sd) & delta.prev.sd > 4
        ),
      `:=`(
        v = v.d,
        tbc.sd = tbc.sd.d,
        exclude = "Unit-Error-High"
      )
    ]

    # process unit error high second (8.e.ii and iv from above), re-factor slightly for efficiency
    data.df[
      valid(data.df) &
        abs(tbc.sd.t - ewma.all) < 0.3 &
        abs(tbc.sd.t - ewma.before) < 0.5 &
        abs(tbc.sd.t - ewma.after) < 0.5 & abs(tbc.sd.t) < 3
      &
        (
          param == "WEIGHTKG" &
            (tbc.sd - ewma.all) < -3 &
            (tbc.sd - ewma.before) < -2 &
            (tbc.sd - ewma.after) < -2 & tbc.sd < -3
          &
            !is.na(delta.next.sd) &
            delta.next.sd < -2 &
            !is.na(delta.prev.sd) & delta.prev.sd < -2
          |
            param == "HEIGHTCM" &
              (tbc.sd - ewma.all) < -5 &
              (tbc.sd - ewma.before) < -4 &
              (tbc.sd - ewma.after) < -4 & tbc.sd < -7
            &
              !is.na(delta.next.sd) &
              delta.next.sd < -4 &
              !is.na(delta.prev.sd) & delta.prev.sd < -4
        ),
      `:=`(
        v = v.t,
        tbc.sd = tbc.sd.t,
        exclude = "Unit-Error-Low"
      )
    ]
  }

  # 9.  Exclude values that are carried forward. For the purposes of this analysis, any value that is identical to the preceding value for the same parameter
  #     and subject is considered carried forward. The chances of having identical measurements, even at an age/interval when little or no growth would be expected,
  #     is fairly small, and when this is the case the carried forward measurements provide little new information.
  # a.	Calculate d_prev_wt=wt-wtprev and d_prev_ht=ht-htprev. Use original measurements rather than transformed
  #     measurements (unit errors and switches).
  # b.	Unlike most steps, do this step for all duplicate values (exc_*==2) in addition to included values (exc_*==0), comparing all values for one day to all
  #     values from the prior day – if there are any values with a d_prev*==0, the value on the latter day should be excluded.
  # c.	Replace exc_*=3 for all values with d_prev*==0 & (exc_*==0 OR exc_*==2)
  if (!include.carryforward) {
    if (!quietly) {
      cat(sprintf(
        "[%s] Exclude measurements carried forward...\n",
        Sys.time()
      ))
    }
    # for efficiency, bring get.prev and get.next inline here (working on valid rows within a single parameter for a single subject)
    # structure c(NA, field.name[-.N]) == get.prev
    data.df[, prev.v := as.double(NaN)]
    data.df[valid(data.df), prev.v := c(NA, v.orig[-.N]), by = .(subjid, param)]

    # optimize "carry forward" for children without duplicates.
    data.df[!(subjid %in% subj.dup) &
      v.orig == prev.v, exclude := "Exclude-Carried-Forward"]

    # need to handle children with duplicate measurements on same day separately
    data.df[
      subjid %in% subj.dup & valid(data.df, include.temporary.duplicates = TRUE),
      exclude := (function(df) {
        setkeyv(df, "agedays")
        ages <- unique(agedays)
        # no point in looking for measurements carried forward if all measurements are from a single day of life
        if (length(ages) > 1) {
          # iterate over each age
          for (i in 2:length(ages)) {
            # find the set of measurements from the previous age in days
            all.prev.v <- df[agedays == ages[i - 1], v.orig]
            # if a measurement for the current age is found in the set of measurements from the previous age, then mark it as carried forward
            df[agedays == ages[i] &
              v.orig %in% all.prev.v, exclude := "Exclude-Carried-Forward"]
          }
        }
        return(df$exclude)
      })(copy(.SD)),
      .SDcols = c("agedays", "exclude", "v.orig"),
      by = .(subjid, param)
    ]
  }

  # 9d.  Replace exc_*=0 if exc_*==2 & redo step 5 (temporary duplicates)
  data.df[exclude == "Exclude-Temporary-Duplicate", exclude := "Include"]
  data.df[temporary_duplicates(data.df), exclude := "Exclude-Temporary-Duplicate"]

  # 10.  Exclude extreme errors with SD cutoffs. For this, a cutoff of |SD|>25 is used. Because of differences in SD and z score, there are some very extreme values
  #      with a |z|>25 that are implausible with an |SD|<25, so both are used to exclude extreme errors. This works better than using a lower value for the limit
  #      for |SD|.
  # a.	Generally we only evaluate measurements where exc_*==0, but for this step we also need to evaluate measurements with exc_*==2
  # b.	Replace exc_*=4 if |tbc*sd|>25 & (exc_*==0 OR exc_*==2)
  # c.	Replace exc_*=4 if |*z|>25 & (exc_*==0 OR exc_*==2) & the value is not switched or transformed.
  if (!quietly) {
    cat(sprintf(
      "[%s] Exclude extreme measurements based on SD...\n",
      Sys.time()
    ))
  }
  data.df[
    na_as_false(
      valid(data.df, include.temporary.duplicates = TRUE) & abs(tbc.sd) > sd.extreme |
        exclude %in% c("Include", "Exclude-Temporary-Duplicate") & abs(z.orig) > z.extreme
    ),
    exclude := "Exclude-SD-Cutoff"
  ]

  # 10d. Redo temporary duplicates as in step 5.
  data.df[exclude == "Exclude-Temporary-Duplicate", exclude := "Include"]
  data.df[temporary_duplicates(data.df), exclude := "Exclude-Temporary-Duplicate"]

  # 11.  Exclude extreme errors with EWMA
  # a.	Erroneous measurements can distort the EWMA for measurements around them. Therefore, if the EWMA method identifies more than one value for a subject and
  #     parameter that meets criteria for exclusion, we will only exclude the value that deviates the most from expected in any given step. Then we will repeat the
  #     entire process until no measurements are identified that meet criteria for exclusion.
  # b.	Perform a EWMA calculation
  #   i.	Only use values where exc_*==0 to determine the EWMAs. However, calculate dewma_* variables for values where exc_*==0 or exc_*==2
  if (!quietly) {
    cat(sprintf(
      "[%s] Exclude extreme measurements based on EWMA...\n",
      Sys.time()
    ))
  }
  data.df <- data.df[,
    exclude := (function(df) {
      num.ewma.excluded <- 0
      # optimization: determine whether this subject has any duplicates
      has.duplicates <- subjid %in% subj.dup
      while (TRUE) {
        df[, (ewma.fields) := as.double(NaN)]
        df[valid(exclude), (ewma.fields) := ewma(agedays, tbc.sd, ewma.exp, TRUE)]

        # note: at this point, only one ewma exists per param on a given day for a subject, so sort(ewma.all)[1] will returns the non-missing ewma.all
        # restrict to children with possible duplicates for efficiency
        if (has.duplicates) {
          df[, `:=`(
            ewma.all = sort(ewma.all)[1],
            ewma.before = sort(ewma.before)[1],
            ewma.after = sort(ewma.after)[1]
          ), by = .(agedays)]
        }
        df[, `:=`(
          dewma.all = tbc.sd - ewma.all,
          dewma.before = tbc.sd - ewma.before,
          dewma.after = tbc.sd - ewma.after
        )]

        # 11c.  Identify all values that meet all of the following criteria as potential exclusions:
        #   i.	There are 3 or more measurements for that subject and parameter
        #   ii.	(dewma_*>3.5 & dewma_*_bef>3 & dewma_*_aft>3 & tbc*sd>3.5) OR (dewma_*<-3.5 & dewma_*_bef<-3 & d & dewma_*_aft<-3 & tbc*sd<-3.5)
        #   iii.	exc_*==0

        num.valid <- sum(valid(df))
        rep <- na_as_false(with(
          df,
          num.valid >= 3 & valid(df) &
            (
              dewma.all > 3.5 & dewma.before > 3 & dewma.after > 3 & tbc.sd > 3.5 |
                dewma.all < -3.5 & dewma.before < -3 & dewma.after < -3 & tbc.sd < -3.5
            )
        ))
        num.exclude <- sum(rep)
        # 11d.  If there is only one potential exclusion identified in step 11c for a subject and parameter, replace exc_*=5 for that value
        if (num.exclude == 1) {
          df[rep, exclude := "Exclude-EWMA-Extreme"]
        }
        # 11e.  If there is more than one potential exclusion identified in step 11c for a subject and parameter, calculate abssum_*=|tbc*sd+dewma_*| for each exclusion and
        #     replace exc_*=5 for the value with the highest abssum_*
        if (num.exclude > 1) {
          # first order by decreasing abssum
          worst.row <- with(df, order(rep, abs(tbc.sd + (tbc.sd - ewma.all)), decreasing = TRUE))[1]
          df[worst.row, exclude := "Exclude-EWMA-Extreme"]
        }

        # 11f.  For subjects/parameters with only 2 values, calculate abstbc*sd=|tbc*sd|
        # g.  Replace exc_*=6 for values that meet all of the following criteria
        #   i.	There are 2 measurements for that subject and parameter
        #   ii.	(dewma_*>3.5 & tbc*sd>3.5) OR (dewma_*<-3.5 & tbc*sd<-3.5)
        #   iii.	If there are 2 measurements for a subject/parameter that meet criteria ii, only replace exc_*=6 for the value with the larger abstbc*sd
        rep <- na_as_false(with(
          df,
          num.valid == 2 &
            (
              tbc.sd - ewma.all > 3.5 & tbc.sd > 3.5 |
                tbc.sd - ewma.all < -3.5 & tbc.sd < -3.5
            )
        ))
        num.exclude <- sum(rep)
        if (num.exclude == 1) {
          df[rep, exclude := "Exclude-EWMA-Extreme-Pair"]
        }
        if (num.exclude > 1) {
          # first order by decreasing abssum
          worst.row <- with(df, order(rep, abs(tbc.sd), decreasing = TRUE))[1]
          df[worst.row, exclude := "Exclude-EWMA-Extreme-Pair"]
        }

        # 11h.  Recalculate temporary duplicates as in step 5
        # optimize: only perform these steps if this subject is known to have duplicate measurements
        if (has.duplicates) {
          df[exclude == "Exclude-Temporary-Duplicate", exclude := "Include"]
          df[temporary_duplicates(df), exclude := "Exclude-Temporary-Duplicate"]
        }

        # 11i.  If there was at least one subject who had a potential exclusion identified in step 11c, repeat steps 11b-11g. If there were no subjects with potential
        #     exclusions identified in step 11c, move on to step 12.
        newly.excluded <- sum(df$exclude %in% c("Exclude-EWMA-Extreme", "Exclude-EWMA-Extreme-Pair"))
        if (newly.excluded > num.ewma.excluded) {
          num.ewma.excluded <- newly.excluded
        } else {
          break
        }
      }
      return(df$exclude)
    })(copy(.SD)),
    by = .(subjid, param),
    .SDcols = c("index", "sex", "agedays", "tbc.sd", "exclude")
  ]



  # 12. Redo duplicates using EWMA. This will be the final time duplicates are done.
  #     For some duplicates it is very difficult to tell which one is likely representative. If the duplicates are very similar to each
  #     other, we will select one. If it is very difficult to tell which one is correct and the duplicates are not very similar, we will
  #     exclude all duplicates for that subject/parameter on that day
  # a.	Replace exc_*=0 for all temporarily excluded duplicates (exc_*==0)
  if (!quietly) {
    cat(sprintf("[%s] Exclude duplicates based on EWMA...\n", Sys.time()))
  }
  data.df[exclude == "Exclude-Temporary-Duplicate", exclude := "Include"]

  # 12b. Select which duplicate to include in EWMA calculations using the same criteria as in step 5. However, do not include values
  #      in these medians that were excluded in steps 9-11 (exc_*=3, 4, 5 or 6)
  #  i.  Determine median_tbc*sd and median_tbcOsd as in step 5.
  # ii.	 For each subject/parameter with duplicates and at least one non-duplicate value, select the value closest to the
  #      median_tbc*sd for inclusion in EWMA calculations.

  # This is functionally the same as re-doing the "temporary duplicate" step before doing the EWMA
  temp.dups <- temporary_duplicates(data.df)
  data.df[temp.dups, exclude := "Exclude-Temporary-Duplicate"]

  # prepare a list of valid rows and initialize variables for convenience
  valid.rows <- valid(data.df)
  data.df[, `:=`(
    ewma.all = as.double(NaN),
    abssum2 = as.double(NaN),
    median.other.sd = as.double(NaN),
    duplicate = FALSE
  )]

  # 12c.	Calculate a EWMA step for all subjects/parameters with duplicates and at least one non-duplicate value with the
  #       following modifications
  #   i.	For calculating the EWMA, include only the duplicate selected in 12c
  #  ii.	Calculate dewma_* for all values of duplicates
  # iii.	You do not need to calculate EWMAbef or EWMAaft for this step

  # determine proportion of days with duplication for each parameter ahead of time for efficiency
  dup.ratio.df <- data.df[
    subjid %in% subj.dup &(valid.rows |temp.dups),
    list(dup = (.N > 1)),
    by = .(subjid, param, agedays)
  ][,
    list(dup.ratio = mean(dup)),
    keyby = .(subjid, param)
  ]
  # identify subject/parameters where there isduplication but at least one day with no duplicates for that parameter
  subj.param.not.all.dups <- dup.ratio.df[dup.ratio < 1.0, list(subjid, param)]
  # identify subject/parameters where there is duplication for all days for that parameter
  subj.param.all.dups <- dup.ratio.df[dup.ratio == 1, list(subjid, param)]
  subj.all.dups <- subj.param.all.dups[, unique(subjid)]
  # perform ewma for subjects with duplicates
  data.df[
    subjid %in% subj.dup & valid.rows,
    ewma.all := ewma(agedays, tbc.sd, ewma.exp, ewma.adjacent = FALSE),
    by = .(subjid, param)
  ]
  # note: at this point, only one ewma.all exists per param on a given day for a subject, so sort(ewma.all)[1] will returns the non-missing ewma.all
  data.df[subjid %in% subj.dup, ewma.all := sort(ewma.all)[1], by = .(subjid, param, agedays)]

  # iv.  Calculate abssum2_*=|2*dewma_*|+|tbc*sd| (note that this is different from how we calculated abssum_* in step 11).
  # NOTE: only children with more than one ageday with valid measurements will have a valid ewma from above
  data.df[, abssum2 := 2 * abs(tbc.sd - ewma.all) + abs(tbc.sd)]

  # 12d.  For each subject/parameter/age with duplicates and at least one non-duplicate value:
  #   i.  Replace exc_*=7 for all values except the value that has the smallest abssum2_*.
  data.df[
    J(subj.param.not.all.dups),
    duplicate := seq_along(abssum2) != which.min(abssum2),
    by =.(subjid, param, agedays)
  ]
  data.df[temp.dups, exclude := "Include"]
  data.df[(valid.rows | temp.dups) & duplicate, exclude := "Exclude-Duplicate"]
  #  ii.  Determine dup_tot_* (# of days with duplicates for that subject/parameter) and nodup_tot_* (# of days with nonexlcuded
  #       non-duplicates for that subject/parameter).
  # iii.  If dup_tot_*/(dup_tot_*+nodup_tot_*) is greater than 1/2, replace exc_*=7 for all duplicates for that subject/parameter
  #       for each age where the  largest measurement minus the smallest measurement for that subject/parameter/age is larger than
  #       the maximum difference (ht 3cm; wt 0-9.999 kg 0.25kg; wt 10-29.9999 kg 0.5 kg; wt 30kg and higher 1 kg).

  data.df[
    J(dup.ratio.df[dup.ratio > 1 / 2, list(subjid, param)]),
    exclude := (function(df) {
      df[, `:=`(tbc.sd.min = as.double(NaN), tbc.sd.max = as.double(NaN))]
      df[
        valid(
          exclude,
          include.duplicates = TRUE,
          include.temporary.duplicates = TRUE
        ),
        `:=`(
          tbc.sd.min = min(tbc.sd),
          tbc.sd.max = max(tbc.sd)
        )
      ]
      df[
        tbc.sd.max - tbc.sd.min > ifelse(param == "HEIGHTCM",
          3,
          ifelse(
            param == "WEIGHTKG",
            ifelse(tbc.sd.min < 10, 0.25, ifelse(tbc.sd.min < 30, 0.5, 1)),
            NA
          )
        ),
        exclude := "Exclude-Duplicate"
      ]
      return(df$exclude)
    })(copy(.SD)),
    .SDcols = c("exclude", "tbc.sd"),
    by = .(subjid, param, agedays)
  ]

  # 12e.	For each subject/parameter/age with duplicates and no nonduplicate values:
  #   i.	Replace exc_*=7 for all values except the value with the smallest |tbc*sd-median_tbcOsd|. If median_tbcOsd is missing because there are no values
  #       for the other parameter, randomly choose one duplicate value for each subject/parameter/age to keep as exc_*=0 and replace exc_*=7 for all other
  #       duplicates for that subject/parameter/age.
  #  ii.  If the largest measurement minus the smallest measurement for that subject/parameter/age is larger than the maximum difference
  #       (ht 3cm; wt 0-9.999 kg 0.25kg; wt 10-29.9999 kg 0.5 kg; wt 30kg and higher 1 kg)., replace exc_*=7 for all duplicates for that
  #       subject/parameter/age.

  # calculate median for other parameter (restrict to subjects with all duplication for at least one parameter)
  data.df[
    subjid %in% subj.all.dups,
    exclude := (function(subj.df) {
      # flag days that have duplicates / potentially valid parameters
      subj.df[, `:=`(
        duplicates.this.day = FALSE,
        duplicate = FALSE,
        tbc.sd.min = as.double(NaN),
        tbc.sd.max = as.double(NaN)
      )]
      valid.rows <- valid(
        subj.df,
        include.duplicates = TRUE,
        include.temporary.duplicates = TRUE
      )
      subj.df[valid.rows, duplicates.this.day := (.N > 1), by = .(param, agedays)]
      for (p in subj.df[j = unique(param)]) {
        median.sd <- subj.df[param != p & !duplicates.this.day, median(tbc.sd)]
        subj.df[param == p, median.other.sd := median.sd]
      }
      # safety check -- assign median.other.sd==0 to ensure "which.min" functions correctly below
      subj.df[is.na(median.other.sd), median.other.sd := 0]
      # identify rows as duplicate where |tbc*sd-median_tbcOsd| is not at the minimum value
      subj.df[
        duplicates.this.day == TRUE,
        duplicate := seq_along(median.other.sd) != which.min(abs(tbc.sd - median.other.sd)),
        by = .(param, agedays)
      ]
      subj.df[duplicates.this.day & !duplicate, exclude := "Include"]
      subj.df[duplicates.this.day & duplicate, exclude := "Exclude-Duplicate"]
      subj.df[
        duplicates.this.day == TRUE,
        `:=`(
          tbc.sd.min = min(tbc.sd),
          tbc.sd.max = max(tbc.sd)
        ),
        by = .(param, agedays)
      ]
      subj.df[
        tbc.sd.max - tbc.sd.min > ifelse(param == "HEIGHTCM",
          3,
          ifelse(
            param == "WEIGHTKG",
            ifelse(tbc.sd.min < 10, 0.25, ifelse(tbc.sd.min < 30, 0.5, 1)),
            NA
          )
        ),
        exclude := "Exclude-Duplicate"
      ]

      # identify kids who had an SD or EWMA extreme excluded that was a duplicate and re-label as "Exclude-Duplicate"
      subj.df[, duplicates.this.day := FALSE]
      # consider any non-missing measurement when determining presence of duplicates
      subj.df[
        exclude != "Missing",
        duplicates.this.day := (.N > 1),
        by = .(param, agedays)
      ]
      subj.df[
        duplicates.this.day &
          exclude %in% c(
            "Exclude-SD-Cutoff",
            "Exclude-EWMA-Extreme",
            "Exclude-EWMA-Extreme-Pair"
          ),
        exclude := "Exclude-Duplicate"
      ]

      return(subj.df$exclude)
    })(copy(.SD)),
    .SDcols = c("param", "agedays", "exclude", "tbc.sd"),
    by = .(subjid)
  ]

  # 12f.  For any values that were excluded with exc_*=4, 5, or 6 that are also duplicates, replace exc_*=7.
  data.df[
    subjid %in% subj.dup,
    exclude := (function(subj.df) {
      if (.N > 1) {
        subj.df[exclude %in% c(
          "Exclude-SD-Cutoff",
          "Exclude-EWMA-Extreme",
          "Exclude-EWMA-Extreme-Pair"
        ), exclude := "Exclude-Duplicate"]
      }
      return(subj.df$exclude)
    })(copy(.SD)),
    .SDcols = c("exclude"),
    by = .(subjid, param, agedays)
  ]

  # 13.  Calculate plus/minus measurements with allowable errors and corresponding recentered SD scores
  # a.	In order to help determine if a deviation from EWMA could be explained by an allowable degree of error, calculate the following:
  #    i.	wt_plus=wt+0.05*wt
  #    ii.	wt_minus=wt-0.05*wt
  #    iii.	ht_plus=ht+1
  #    iv.	ht_minus=ht-1
  # b.	Foreach of the above calculated and then recenter the SD score for the new value, generating tbc*sd_plus and tbc*sd_minus
  data.df[, delta := ifelse(param == "WEIGHTKG", .05 * v, 1)]
  data.df[, `:=`(v.minus = v - delta, v.plus = v + delta)]
  data.df[, `:=`(
    tbc.sd.minus = measurement.to.z(param, agedays, sex, v.minus, TRUE),
    tbc.sd.plus = measurement.to.z(param, agedays, sex, v.plus, TRUE)
  )]

  # 14.  Exclude moderate errors based on EWMA
  # a.	This step is similar to step 11, with repeated exclusions of 1 value at a time, but with different criteria than step 11. There are several criteria used
  #     as checks to make sure that values that have a large dewma_* are truly not likely to be representative.
  # b.	Perform a EWMA calculation
  #   i.	In addition to standard dewma_* variables, calculate dewma_*_plus and dewma_*_minus using the tbc*sd scores generated in step 13
  if (!quietly) {
    cat(sprintf("[%s] Exclude moderate errors based on EWMA...\n", Sys.time()))
  }
  data.df[,
    exclude := (function(subj.df) {
      num.ewma.excluded <- 0
      while (TRUE) {
        valid.rows <- valid(subj.df)
        # initialize fields
        subj.df[, `:=`(
          ewma.all = as.double(NaN),
          ewma.before = as.double(NaN),
          ewma.after = as.double(NaN),
          tbc.sd.prev = as.double(NaN),
          tbc.sd.next = as.double(NaN),
          delta.agedays.prev = as.integer(NaN),
          delta.agedays.next = as.integer(NaN),
          abs.2ndlast.sd = as.double(NaN),
          tbc.other.sd = as.double(NaN)
        )]
        subj.df[
          valid.rows,
          (ewma.fields) := ewma(agedays, tbc.sd, ewma.exp, TRUE),
          by =param
        ]
        subj.df[, `:=`(
          dewma.all = tbc.sd - ewma.all,
          dewma.before = tbc.sd - ewma.before,
          dewma.after = tbc.sd - ewma.after
        )]

        # 14c.	Calculate d_prevsd=tbc*sd-tbc*sdprev; d_prevsd_minus=tbc*sd_minus-tbc*sdprev; d_prevsd_plus=tbc*sd_plus-tbc*sdprev
        #     and d_nextsd=tbc*sd-tbc*sdnext; d_nextsd_minus=tbc*sd_minus-tbc*sdnext; d_nextsd_plus=tbc*sd_plus-tbc*sdnext

        # for efficiency, bring get.prev and get.next inline here (working on valid rows within a single parameter for a single subject)
        # structure c(NA, field.name[-.N]) == get.prev
        # structure c(field.name[-1], NA) == get.next
        subj.df[valid.rows, `:=`(
          tbc.sd.prev = c(NA, tbc.sd[-.N]),
          tbc.sd.next = c(tbc.sd[-1], NA)
        ), by = param]
        subj.df[, `:=`(
          dprev.sd = tbc.sd - tbc.sd.prev,
          dprev.sd.minus = tbc.sd.minus - tbc.sd.prev,
          dprev.sd.plus = tbc.sd.plus - tbc.sd.prev,
          dnext.sd = tbc.sd - tbc.sd.next,
          dnext.sd.minus = tbc.sd.minus - tbc.sd.next,
          dnext.sd.plus = tbc.sd.plus - tbc.sd.next
        )]

        # 14d.	Calculate d_agedays_prev=agedays-agedaysprev and d_agedays_next=agedaysnext-agedays
        subj.df[valid.rows, `:=`(
          delta.agedays.prev = as.integer(agedays - c(NA, agedays[-.N])),
          delta.agedays.next = as.integer(agedays - c(agedays[-1], NA))
        ), by = param]

        # 14e.	Generate abs_2ndlast_sd=|tbc*sd| for the second-to-last measurement for a subject/parameter
        # assign this value (if defined) to all agedays for that subject
        subj.df[
          valid.rows,
          abs.2ndlast.sd := ifelse(.N >= 2, abs(tbc.sd[.N - 1]), as.double(NaN)),
          by = param
        ]

        # 14f.	Calculate tbcOsd which is the tbc*sd for the OTHER parameter for the same subject and ageday  with exc_*==0 (this may be missing).
        # NOTE: move this simplified version of swap_parameters here for efficiency
        valid.other <- subj.df[valid.rows, list(
          param.other = ifelse(param == "WEIGHTKG", "HEIGHTCM", "WEIGHTKG"),
          agedays.other = agedays,
          tbc.sd
        )]
        setkeyv(valid.other, c("param.other", "agedays.other"))
        subj.df[valid.rows, tbc.other.sd := valid.other[list(param, agedays), tbc.sd]]

        subj.df[,
          exclude := (function(df) {
            # 14g.  Calculate median_tbcOsd which is the median_tbc*sd for the OTHER parameter for the same subject with exc_*==0 (this may be missing)
            # also assign param variable for use in subsequent steps
            median.tbc.other.sd <- median(df$tbc.other.sd, na.rm = TRUE)
            num.valid <- sum(valid(df$exclude))

            # 14h.	Identify values for possible exclusion if they meet one of the following sets of criteria. Generate a temporary exclusion variable temp_exc_* equal
            #     to the number indicated to keep track of which set of criteria were met
            df$temp.exclude <- factor(NA, levels = exclude.levels, ordered = TRUE)

            # 14h.i.	Replace temp_exc_*=8 if the value is one of 3 or more measurements for a subject/parameter AND the value is neither the first nor the last measurement
            #       AND one of the following sets of criteria are met
            #     1.	dewma_*>1 & dewma_*_bef>1 & dewma_*_aft>1 & d_nextsd_*>1 & d_prevsd_*>1 & d_prevsd_plus_*>1 & d_prevsd_minus_*>1 & d_nextsd_plus_*>1 & d_nextsd_minus_*>1
            #     2.	dewma_*<-1 & dewma_*_bef<-1 & dewma_*_aft<-1 & d_nextsd_*<-1 & d_prevsd_*<-1 & d_prevsd_plus_*<-1 & d_prevsd_minus_*<-1 & d_nextsd_plus_*<-1 & d_nextsd_minus_*<-1

            # NOTE: NA as treated as false in context of data.table, which takes care of the NA values that result when
            # evaluating the boolean expression for a first or last value of a parameter for a child,
            # which ensures the flag is only set for measurements that have both a preceding and subsequent measurement
            df[
              (
                dewma.all > 1 &
                dewma.before > 1 &
                dewma.after > 1 &
                dprev.sd > 1 &
                dprev.sd.plus > 1 &
                dprev.sd.minus > 1 &
                dnext.sd > 1 &
                dnext.sd.plus > 1 &
                dnext.sd.minus > 1
              ) |
              (
                dewma.all < -1 &
                dewma.before < -1 &
                dewma.after < -1 &
                dprev.sd < -1 &
                dprev.sd.plus < -1 &
                dprev.sd.minus < -1 &
                dnext.sd < -1 &
                dnext.sd.plus < -1 &
                dnext.sd.minus < -1
              ),
              temp.exclude := "Exclude-EWMA-8"
            ]

            # 14h.ii.	Replace temp_exc_*=9 if the value is the first of 3 or more measurements for a subject/parameter AND d_agedays_next<365.25
            #       AND one of the following sets of criteria are met
            #     1.	dewma_*>2 & dewma_*_aft>1 & d_nextsd_*>1 & d_nextsd_plus_*>1 & d_nextsd_minus_*>1
            #     2.	dewma_*<-2 & dewma_*_aft<-1 & d_nextsd_*<-1 & d_nextsd_plus_*<-1 & d_nextsd_minus_*<-1

            # take advantage of other variables we have calculated to infer that a row is the first of three or more valid measurements for a paramete
            df$first.of.three.or.more <- FALSE
            df$last.of.three.or.more <- FALSE
            df[is.na(delta.agedays.prev) & num.valid >= 3, first.of.three.or.more := TRUE]
            df[is.na(delta.agedays.next) & num.valid >= 3, last.of.three.or.more := TRUE]

            df[
              first.of.three.or.more & delta.agedays.next < 365.25 &
                (
                  (
                    dewma.all > 2 &
                    dewma.after > 1 &
                    dnext.sd > 1 &
                    dnext.sd.plus > 1 &
                    dnext.sd.minus > 1
                  ) | (
                    dewma.all < -2 &
                    dewma.after < -1 &
                    dnext.sd < -1 &
                    dnext.sd.plus < -1 &
                    dnext.sd.minus < -1
                  )
                ),
              temp.exclude := "Exclude-EWMA-9"
            ]

            # 14h.iii.	Replace temp_exc_*=10 if the value is the first of 3 or more measurements for a subject/parameter AND d_agedays_next>365.25
            #         AND one of the following sets of criteria are met
            #     1.	dewma_*>3 & dewma_*_aft>1 & d_nextsd_*>1 & d_nextsd_plus_*>1 & d_nextsd_minus_*>1
            #     2.	dewma_*<-3 & dewma_*_aft<-1 & d_nextsd_*<-1 & d_nextsd_plus_*<-1 & d_nextsd_minus_*<-1

            # NOTE: changed age comparison to >= to ensure complete coverage of possible conditions in the event case fractional days are present in the dataset
            df[
              first.of.three.or.more & delta.agedays.next >= 365.25 &
                (
                  (
                    dewma.all > 3 &
                    dewma.after > 1 &
                    dnext.sd > 1 &
                    dnext.sd.plus > 1 &
                    dnext.sd.minus > 1
                  ) | (
                    dewma.all < -3 &
                    dewma.after < -1 &
                    dnext.sd < -1 &
                    dnext.sd.plus < -1 &
                    dnext.sd.minus < -1
                  )
                ),
              temp.exclude := "Exclude-EWMA-10"
            ]

            # 14h.iv.	Replace temp_exc_*=11 if the value is the last of 3 or more measurements for a subject/parameter AND d_agedays_prev<730.5 AND abs_2ndlast_sd <2
            #       AND one of the following sets of criteria are met
            #     1.	dewma_*>2 & dewma_*_bef>1 & d_prevsd_*>1 & d_prevsd_plus_*>1 & d_prevsd_minus_*>1
            #     2.	dewma_*<-2 & dewma_*_bef<-1 & d_prevsd_*<-1 & d_prevsd_plus_*<-1 & d_prevsd_minus_*<-1
            df[
              last.of.three.or.more & delta.agedays.prev < 730.5 & abs.2ndlast.sd < 2 &
                (
                  (
                    dewma.all > 2 &
                    dewma.before > 1 &
                    dprev.sd > 1 &
                    dprev.sd.plus > 1 &
                    dprev.sd.minus > 1
                  ) | (
                    dewma.all < -2 &
                    dewma.before < -1 &
                    dprev.sd < -1 &
                    dprev.sd.plus < -1 &
                    dprev.sd.minus < -1
                  )
                ),
              temp.exclude := "Exclude-EWMA-11"
            ]

            # 14h.v.	Replace temp_exc_*=12 if the value is the last of 3 or more measurements for a subject/parameter AND d_agedays_prev<730.5 AND abs_2ndlast_sd >=2
            #       AND one of the following sets of criteria are met
            #     1.	dewma_*>abs_2ndlast_sd & dewma_*_bef>1 & d_prevsd_*>1 & d_prevsd_plus_*>1 & d_prevsd_minus_*>1
            #     2.	dewma_*<(-1*abs_2ndlast_sd) & dewma_*_bef<-1 & d_prevsd_*<-1 & d_prevsd_plus_*<-1 & d_prevsd_minus_*<-1
            df[
              last.of.three.or.more & delta.agedays.prev < 730.5 & abs.2ndlast.sd >= 2 &
                (
                  (
                    dewma.all > abs.2ndlast.sd &
                    dewma.before > 1 &
                    dprev.sd > 1 &
                    dprev.sd.plus > 1 &
                    dprev.sd.minus > 1
                  ) | (
                    dewma.all < -abs.2ndlast.sd &
                    dewma.before < -1 &
                    dprev.sd < -1 &
                    dprev.sd.plus < -1 &
                    dprev.sd.minus < -1
                  )
                ),
              temp.exclude := "Exclude-EWMA-12"
            ]

            # 14h.vi.	Replace temp_exc_*=13 if the value is the last of 3 or more measurements for a subject/parameter AND d_agedays_prev>730.5 AND  abs_2ndlast_sd <2
            #       AND one of the following sets of criteria are met
            #     1.	dewma_*>3 & dewma_*_bef>1 & d_prevsd_*>1 & d_prevsd_plus_*>1 & d_prevsd_minus_*>1 & ((tbc*sd-tbcOsd)>4
            #         OR ((tbc*sd-median_tbcOsd)>4 & tbcOsd is missing) OR median_tbcOsd is missing)
            #     2.	dewma_*<-3 & dewma_*_bef<-1 & d_prevsd_*<-1 & d_prevsd_plus_*<-1 & d_prevsd_minus_*<-1 & ((tbc*sd-tbcOsd)<-4
            #         OR ((tbc*sd-median_tbcOsd)<-4 & tbcOsd is missing) OR median_tbcOsd is missing)

            # NOTE: changed age comparison to >= to ensure complete coverage of possible conditions in the event case fractional days are present in the dataset
            df[
              last.of.three.or.more & delta.agedays.prev >= 730.5 & abs.2ndlast.sd < 2 &
                (
                  (
                    dewma.all > 3 &
                    dewma.before > 1 &
                    dprev.sd > 1 &
                    dprev.sd.plus > 1 &
                    dprev.sd.minus > 1 &
                    (
                      tbc.sd - tbc.other.sd > 4 |
                      is.na(tbc.other.sd) & tbc.sd - median.tbc.other.sd > 4 |
                      is.na(median.tbc.other.sd)
                    )
                  ) | (
                    dewma.all < -3 &
                    dewma.before < -1 &
                    dprev.sd < -1 &
                    dprev.sd.plus < -1 &
                    dprev.sd.minus < -1 &
                    (
                      tbc.sd - tbc.other.sd < -4 |
                      is.na(tbc.other.sd) & tbc.sd - median.tbc.other.sd < -4 |
                      is.na(median.tbc.other.sd)
                    )
                  )
                ),
              temp.exclude := "Exclude-EWMA-13"
            ]

            # 14h.vii.	Replace temp_exc_*=14 if the value is the last of 3 or more measurements for a subject/parameter AND d_agedays_prev>730.5 AND  abs_2ndlast_sd>=2
            #         AND one of the following sets of criteria are met
            #     1.	dewma_*>(1+abs_2ndlast_sd) & dewma_*_bef>1 & d_prevsd_*>1 & d_prevsd_plus_*>1 & d_prevsd_minus_*>1 & ((tbc*sd-tbcOsd)>4
            #         OR ((tbc*sd-median_tbcOsd)>4 & tbcOsd is missing) OR median_tbcOsd is missing)
            #     2.	dewma_*<(-1-abs_2ndlast_sd) & dewma_*_bef<-1 & d_prevsd_*<-1 & d_prevsd_plus_*<-1 & d_prevsd_minus_*<-1 & ((tbc*sd-tbcOsd)<-4
            #         OR ((tbc*sd-median_tbcOsd)<-4 & tbcOsd is missing) OR median_tbcOsd is missing)
            df[
              last.of.three.or.more & delta.agedays.prev >= 730.5 & abs.2ndlast.sd >= 2 &
                (
                  (
                    dewma.all > 1 + abs.2ndlast.sd &
                    dewma.before > 1 &
                    dprev.sd > 1 &
                    dprev.sd.plus > 1 &
                    dprev.sd.minus > 1 &
                    (
                      tbc.sd - tbc.other.sd > 4 |
                      is.na(tbc.other.sd) & tbc.sd - median.tbc.other.sd > 4 |
                      is.na(median.tbc.other.sd)
                    )
                  ) | (
                    dewma.all < -1 - abs.2ndlast.sd &
                    dewma.before < -1 &
                    dprev.sd < -1 &
                    dprev.sd.plus < -1 &
                    dprev.sd.minus < -1 &
                    (
                      tbc.sd - tbc.other.sd < -4 |
                      is.na(tbc.other.sd) & tbc.sd - median.tbc.other.sd < -4 |
                      is.na(median.tbc.other.sd)
                    )
                  )
                ),
              temp.exclude := "Exclude-EWMA-14"
            ]

            # 14i.	If there is only one potential exclusion identified in step 14h for a subject and parameter, replace exc_*=temp_exc_* for that value
            rep <- !is.na(df$temp.exclude)
            num.exclude <- sum(rep)
            if (num.exclude == 1) {
              df[rep, exclude := temp.exclude]
            }

            # 14j.	If there is more than one potential exclusion identified in step 14h for a subject and parameter, calculate abssum_*=|tbc*sd+dewma_*| for each exclusion
            #     and replace exc_*=temp_exc_* for the value with the highest abssum_*
            if (num.exclude > 1) {
              # first order by decreasing abssum  (where rep=TRUE)
              worst.row <- with(df, order(rep, abs(tbc.sd + dewma.all), decreasing = TRUE))[1]
              df[worst.row, exclude := temp.exclude]
            }

            return(df$exclude)
          })(copy(.SD)),
          by = param
        ]

        # k.	If there was at least one subject who had a potential exclusion identified in step 14h, repeat steps 14b-14j. If there were no subjects with potential
        #     exclusions identified in step 14h, move on to step 15.
        newly.excluded <- sum(subj.df$exclude >= "Exclude-EWMA-8" & subj.df$exclude <= "Exclude-EWMA-14")
        if (newly.excluded > num.ewma.excluded) {
          num.ewma.excluded <- newly.excluded
        } else {
          break
        }
      }

      return(subj.df$exclude)
    })(copy(.SD)),
    by = subjid,
    .SDcols = c(
      "index",
      "sex",
      "param",
      "agedays",
      "v",
      "tbc.sd",
      "v.minus",
      "v.plus",
      "tbc.sd.minus",
      "tbc.sd.plus",
      "exclude"
    )
  ]

  # 15.  Exclude heights based on absolute differences in measurement. The key to this step is that once we identify pairs of measurements with implausible
  #      amounts of absolute difference between them, we have to determine which of the two values in the pair is less likely to be representative and should
  #      be excluded. For subjects/parameters with 3 or more measurements, this is done by looking at the dewma_* of each of the 2 values in a pair using a ewma that
  #      excludes the other value in the pair. For subjects/parameters with 2 or more measurements, this is done by looking at the absolute value of the tbc*sd.
  #      The Tanner height velocity reference is used for measurements taken at >2yo, WHO will be used for <2yo. For a few pairs of measurements either could be used;
  #      WHO will be used if difference between ages is < 9 months.
  if (!quietly) {
    cat(sprintf(
      "[%s] Exclude heights based on growth velocity...\n",
      Sys.time()
    ))
  }
  data.df[param == "HEIGHTCM", exclude := (function(subj.df) {
    # assign some book keeping variables
    # subj.df[, `:=`(subjid = subjid, param='HEIGHTCM',index=1:.N)]
    subj.df[, index := 1:.N]

    num.height.excluded <- 0
    while (TRUE) {
      # use a closure to discard all the extra fields added to df with each iteration
      subj.df[valid(exclude), exclude := (function(df) {
        # with each iteration we are operating only on valid rows to allow simplification of code below

        # initialize fields
        df[, (ewma.fields) := as.double(NaN)]
        df[, `:=`(
          v.prev = as.double(NaN),
          v.next = as.double(NaN),
          dewma.after.prev = as.double(NaN),
          dewma.before.next = as.double(NaN),
          abs.tbc.sd.prev = as.double(NaN),
          abs.tbc.sd.next = as.double(NaN),
          agedays.next = as.integer(NaN),
          abs.2ndlast.sd = as.double(NaN),
          mindiff.prev.ht = as.double(NaN),
          mindiff.next.ht = as.double(NaN),
          maxdiff.prev.ht = as.double(NaN),
          maxdiff.next.ht = as.double(NaN),
          pair.prev = FALSE,
          pair.next = FALSE
        )]

        # ewma fields are needed later -- calculate now for efficiency
        df[, (ewma.fields) := ewma(agedays, tbc.sd, ewma.exp, TRUE)]

        # calculate some usefule values (e.g. dewma values and tbc.sd) for use in later steps
        df[, `:=`(
          dewma.all = tbc.sd - ewma.all,
          dewma.before = tbc.sd - ewma.before,
          dewma.after = tbc.sd - ewma.after,
          abs.tbc.sd = abs(tbc.sd)
        )]

        # 15a.  As with steps 11 and 14, only one value will be excluded per round, and the step will be repeated until there are no more values to exclude
        # b.  For each height, calculate the d_age=agedays of next value-agedays of current value
        # NOTE: obtain next measurement, ewma.before and abs.tbc.sd as well since they are needed later

        # for efficiency, bring get.next inline here (working on valid rows within a single parameter for a single subject)
        # structure c(field.name[-1], NA) == get.next
        df[, `:=`(
          agedays.next = c(agedays[-1], NA),
          v.next = c(v[-1], NA),
          dewma.before.next = c(dewma.before[-1], NA),
          abs.tbc.sd.next = c(abs.tbc.sd[-1], NA)
        )]
        df$delta.agedays.next <- with(df, agedays.next - agedays)

        # 15c.	For each height, calculate mid_agedays=0.5*(agedays of next value + agedays of current value)
        df$mid.agedays <- 0.5 * (df$agedays.next + df$agedays)

        # 15d.	Generate variable tanner_months= 6+12*(round(mid_agedays/365.25))
        # only calculate for rows that relate to height (may speed up subsequent processing)
        df$tanner.months <- with(df, 6 + 12 * (round(mid.agedays / 365.25)))

        # 15e.	Merge with dataset tanner_ht_vel using sex and tanner_months – this will give you min_ht_vel and max_ht_vel
        setkeyv(df, c("sex", "tanner.months"))
        df <- tanner.ht.vel[df]

        # 15f.	Calculate the following:
        #   i.	mindiff_ht=0.5*min_ht_vel*(d_agedays/365.25)^2-3 if d_agedays<365.25
        #   ii.	replace mindiff_ht=0.5*min_ht_vel-3 if d_agedays>365.25
        df[, ht.exp := ifelse(delta.agedays.next < 365.25, 2, 0)]
        df[, `:=`(
          maxdiff.next.ht = as.double(NA),
          mindiff.next.ht = as.double(NaN)
        )]
        df[, mindiff.next.ht := 0.5 * min.ht.vel * (delta.agedays.next /
          365.25)^ht.exp - 3]

        # 15f.iii.	maxdiff_ht=2*max_ht_vel*(d_agedays/365.25)^1.5+5.5 if d_agedays>365.25
        #   iv.	replace maxdiff_ht=2*max_ht_vel*(d_agedays/365.25)^0.33+5.5 if d_agedays<365.25
        df[, ht.exp := ifelse(delta.agedays.next < 365.25, 0.33, 1.5)]
        df[, maxdiff.next.ht := 2 * max.ht.vel * (delta.agedays.next /
          365.25)^ht.exp + 5.5]

        # 15g.	Generate variable whoagegrp_ht=agedays/30.4375 rounded to the nearest integer
        df[, whoagegrp.ht := round(agedays / 30.4375)]

        # 15h.	Generate variable whoinc_age_ht based on values of d_agedays_ht according the the following table
        #   d_agedays_ht	whoinc_age_ht
        #   20-45	        1
        #   46-75	        2
        #   76-106	      3
        #   107-152	      4
        #   153-198	      6
        #   All others	  missing
        df[, whoinc.age.ht := ifelse(delta.agedays.next < 20,
          NA,
          ifelse(
            delta.agedays.next <= 45,
            1,
            ifelse(
              delta.agedays.next <= 75,
              2,
              ifelse(
                delta.agedays.next <= 106,
                3,
                ifelse(
                  delta.agedays.next <= 152,
                  4,
                  ifelse(delta.agedays.next <= 198, 6, NA)
                )
              )
            )
          )
        )]

        # i.	Merge using sex and whoagegrp_ht using who_ht_vel_3sd and who_ht_maxvel_3sd; this will give you varaibles whoinc_i_ht and maxwhoinc_i_ht
        #     for various intervals where i is 1,2, 3,4, 6 and corresponds to whoinc_age_ht.
        setkeyv(df, c("sex", "whoagegrp.ht"))
        df <- who.ht.vel[df]

        # restore original sort order (ensures valid.rows variable applies to correct rows)
        setkeyv(df, "index")

        # 15j.	Generate variable who_mindiff_ht=whoinc_i_ht according to the value if whoinc_age_ht; make who_mindiff_ht missing if whoinc_i_ht or whoinc_age_ht is missing.
        df[, who.mindiff.next.ht := ifelse(
          delta.agedays.next < 20,
          NA,
          ifelse(
            delta.agedays.next <= 45,
            whoinc.1.ht,
            ifelse(
              delta.agedays.next <= 75,
              whoinc.2.ht,
              ifelse(
                delta.agedays.next <= 106,
                whoinc.3.ht,
                ifelse(
                  delta.agedays.next <= 152,
                  whoinc.4.ht,
                  ifelse(delta.agedays.next <= 198, whoinc.6.ht, NA)
                )
              )
            )
          )
        )]

        # 15k.	Generate variable who_maxdiff_ht=max_whoinc_i_ht according to the value if whoinc_age_ht; make who_maxdiff_ht missing if max_whoinc_i_ht or
        #     whoinc_age_ht is missing.
        df[, who.maxdiff.next.ht := ifelse(
          delta.agedays.next < 20,
          NA,
          ifelse(
            delta.agedays.next <= 45,
            max.whoinc.1.ht,
            ifelse(
              delta.agedays.next <= 75,
              max.whoinc.2.ht,
              ifelse(
                delta.agedays.next <= 106,
                max.whoinc.3.ht,
                ifelse(
                  delta.agedays.next <= 152,
                  max.whoinc.4.ht,
                  ifelse(delta.agedays.next <= 198, max.whoinc.6.ht, NA)
                )
              )
            )
          )
        )]

        # 15l.	Scale allowed value based on d_agedays_ht:
        #   1.	replace who_mindiff_`p'=who_mindiff_`p'*d_agedays_`p'/(whoinc_age_`p'*30.4375) if d_agedays_`p'<(whoinc_age_`p'*30.4375)
        #   2.	replace who_maxdiff_`p'=who_maxdiff_`p'*d_agedays_`p'/(whoinc_age_`p'*30.4375) if d_agedays_`p'>(whoinc_age_`p'*30.4375)
        df[
          delta.agedays.next < whoinc.age.ht * 30.4375,
          `:=`(
            who.mindiff.next.ht = who.mindiff.next.ht * delta.agedays.next / (whoinc.age.ht * 30.4375),
            who.maxdiff.next.ht = who.maxdiff.next.ht * delta.agedays.next / (whoinc.age.ht * 30.4375)
          )
        ]

        # 15m.	Replace mindiff_ht/maxdiff_ht with adjusted WHO value if Tanner value is missing or if both are present and age difference is < 9 months:
        #   1.	replace mindiff_`p'=0.5*who_mindiff_`p'-3 if who_mindiff_`p' is not missing & d_agedays_`p'<(9*30.4375)
        #   2.	replace maxdiff_`p'=2*who_maxdiff_`p'+3 if who_maxdiff_`p' is not missing & d_agedays_`p'<(9*30.4375)
        #   3.	replace mindiff_`p'=0.5*who_mindiff_`p'-3 if mindiff_`p' is missing & who_mindiff_`p' is not missing
        #   4.	replace maxdiff_`p'=2*who_maxdiff_`p'+3 if maxdiff_`p is missing & who_maxdiff_`p' is not missing

        # refactored logic slightly for efficiency
        df[
          !is.na(who.mindiff.next.ht) &
            (delta.agedays.next < 9 * 30.4375 | is.na(mindiff.next.ht)),
          `:=`(
            mindiff.next.ht = 0.5 * who.mindiff.next.ht - 3,
            maxdiff.next.ht = 2.0 * who.maxdiff.next.ht + 3
          )
        ]

        # 15m.5.  replace mindiff_`p'=-3 if mindiff_`p' is missing
        df[is.na(mindiff.next.ht), mindiff.next.ht := -3]

        # 15n.	Determine the min/maxdiffs for the previous age: mindiff_prev_ht, maxdiff_prev_ht
        # NOTE: obtain previous height, ewma.after value and abs.tbc.sd as well since they are needed in next steps

        # for efficiency, bring get.prev inline here (working on valid rows within a single parameter for a single subject)
        # structure c(NA, tbc.sd[-.N]) == get.prev
        df[, `:=`(
          v.prev = c(NA, v[-.N]),
          dewma.after.prev = c(NA, dewma.after[-.N]),
          abs.tbc.sd.prev = c(NA, abs.tbc.sd[-.N]),
          mindiff.prev.ht = c(NA, mindiff.next.ht[-.N]),
          maxdiff.prev.ht = c(NA, maxdiff.next.ht[-.N])
        )]

        # 15o.	Determine d_prev_ht=ht-htprev (set to missing for the first value for a subject) and d_next_ht=htnext-ht (set to missing for the last value for a subject)
        df[, `:=`(
          delta.prev.ht = v - v.prev,
          delta.next.ht = v.next - v
        )]

        # 15p.  Perform a EWMA calculation with the following modifications:
        #  i.	  Generate a variable pair=1 if (d_prev_ht<mindiff_prev_ht OR d_ht<mindiff_ht OR d_prev_ht>maxdiff_prev_ht  OR d_ht>maxdiff_ht) AND exc_ht==0
        df[, pair := na_as_false(
          delta.prev.ht < mindiff.prev.ht |
            delta.next.ht < mindiff.next.ht |
            delta.prev.ht > maxdiff.prev.ht |
            delta.next.ht > maxdiff.next.ht
        )]

        # for efficiency, bring get.prev and get.next inline here (working on valid rows within a single parameter for a single subject)
        # structure c(NA, field.name[-.N]) == get.prev
        # structure c(field.name[-1], NA) == get.next
        df[, `:=`(
          pair.prev = c(FALSE, pair[-.N]),
          pair.next = c(pair[-1], FALSE)
        )]

        #  ii.	Generate bef_g_aftm1=1 if |Δewma_htbef| for the value of interest is greater than |Δewma_htaft| for the previous value
        #       AND the value of interest is not the first height value for that subject AND pair==1 AND pair for the previous value==1

        #  iii.	Generate aft_g_befp1=1 if |Δewma_htaft| for the value of interest is greater than |Δewma_htbef| for the next value
        #       AND the value of interest is not the last height value for that subject AND pair==1 AND pair for the next value==1
        # NOTE: pair.next will be NA last height, which will result in a FALSE value below
        df[, `:=`(
          bef.g.aftm1 = na_as_false(abs(dewma.before) > abs(dewma.after.prev) & pair & pair.prev),
          aft.g.befp1 = na_as_false(abs(dewma.after) > abs(dewma.before.next) & pair & pair.next)
        )]

        #  iv.	Determine tbchtsd for each value as well as the one before prev_tbchtsd and after next_tbchtsd it
        # NOTE: done previously for efficiency

        # 15p.v.  Determine the total number of ht values for each subject (tot_ht)
        # NOTE: all rows are valid due to constraint in subj.df[...] statement
        num.valid <- .N

        # 15q.	Identify a value for possible exclusion if one of the following sets of criteria are met. For values identified by each set of criteria determine
        #       the value of temp_diff using the formula given
        #   i.	d_prev_ht<mindiff_prev_ht & bef_g_aftm1_ht==1 & exc_ht==0 & mindiff_prev_ht is not missing
        #     a.  (temp_diff=|dewma_ht_bef|)
        df[, temp.diff := as.double(NaN)]
        df$temp.exclude <- factor(NA, levels = exclude.levels, ordered = TRUE)
        df[
          delta.prev.ht < mindiff.prev.ht & bef.g.aftm1,
          `:=`(
            temp.diff = abs(dewma.before),
            temp.exclude = "Exclude-Min-Height-Change"
          )
        ]

        #   ii.	d_ht<mindiff_ht & aft_g_befp1_ht==1 & exc_ht==0 & mindiff_ht is not missing
        #     a.	(temp_diff=|dewma_ht_aft|)
        df[
          delta.next.ht < mindiff.next.ht & aft.g.befp1,
          `:=`(
            temp.diff = abs(dewma.after),
            temp.exclude = "Exclude-Min-Height-Change"
          )
        ]

        #   iii.	d_prev_ht>maxdiff_prev_ht & bef_g_aftm1_ht==1 & exc_ht==0 & mindiff_prev_ht is not missing
        #     a.  (temp_diff=|dewma_ht_bef|)
        df[
          delta.prev.ht > maxdiff.prev.ht & bef.g.aftm1,
          `:=`(
            temp.diff = abs(dewma.before),
            temp.exclude = "Exclude-Max-Height-Change"
          )
        ]

        #   iv.	d_ht>maxdiff_ht & aft_g_befp1_ht==1 & exc_ht==0 & mindiff_ht is not missing
        #     a.  (temp_diff=|dewma_ht_aft|)
        df[
          delta.next.ht > maxdiff.next.ht & aft.g.befp1,
          `:=`(
            temp.diff = abs(dewma.after),
            temp.exclude = "Exclude-Max-Height-Change"
          )
        ]

        #   v.	d_prev_ht<mindiff_prev_ht & tot_ht==2 & |tbchtsd|>|prev_tbchtsd|
        #     a. for v-viii temp_diff is kept as missing
        #   vi. d_ht<mindiff_ht & tot_ht==2 & |tbchtsd|>|next_tbchtsd|
        df[
          delta.prev.ht < mindiff.prev.ht & num.valid == 2 & abs.tbc.sd > abs.tbc.sd.prev |
          delta.next.ht < mindiff.next.ht & num.valid == 2 & abs.tbc.sd > abs.tbc.sd.next,
          temp.exclude := "Exclude-Min-Height-Change"
        ]

        #   vii.	d_prev_ht>maxdiff_prev_ht & tot_ht==2 & |tbchtsd|>|prev_tbchtsd|
        #   viii. d_ht>maxdiff_ht & tot_ht==2 & |tbchtsd|>|next_tbchtsd|
        df[
          delta.prev.ht > maxdiff.prev.ht & num.valid == 2 & abs.tbc.sd > abs.tbc.sd.prev |
            delta.next.ht > maxdiff.next.ht & num.valid == 2 & abs.tbc.sd > abs.tbc.sd.next,
          temp.exclude := "Exclude-Max-Height-Change"
        ]

        # r.  If there is only one potential exclusion identified in step 15j for a subject and parameter,
        #     replace exc_ht=15 for that value if it met criteria i, ii, v, or vi  and exc_ht=16 if it met criteria iii, iv, vii, or viii
        # NOTE: these exclusions are assigned in the code above as 'Exclude-Min-Height-Change' and 'Exclude-Max-Height-Change'
        rep <- !is.na(df$temp.exclude)
        num.exclude <- sum(rep)
        if (num.exclude == 1) {
          df[rep, exclude := temp.exclude]
        }

        # s.  If there is more than one potential exclusion identified in step 14h for a subject and parameter, determine which value has the largest temp_diff and
        #     replace exc_ht=15 for that value if it met criteria i, ii, v, or vi and exc_ht=16 for that value if it met criteria iii,  iv, vii, or viii.

        if (num.exclude > 1) {
          # first order by decreasing temp.diff (where rep=TRUE)
          worst.row <- order(rep, df$temp.diff, decreasing = TRUE)[1]
          df[worst.row, exclude := temp.exclude]
        }

        return(df$exclude)
      })(copy(.SD))]

      # t.  If there was at least one subject who had a potential exclusion identified in step 15q, repeat steps 15b-15q. If there were no subjects with potential
      #     exclusions identified in step 15q, move on to step 16.
      newly.excluded <- sum(
        subj.df$exclude %in% c(
          "Exclude-Min-Height-Change",
          "Exclude-Max-Height-Change"
        )
      )
      if (newly.excluded > num.height.excluded) {
        num.height.excluded <- newly.excluded
      } else {
        break
      }
    }

    setkeyv(subj.df, "index")
    return(subj.df$exclude)
  })(copy(.SD)), by = .(subjid), .SDcols = c("sex", "agedays", "v", "tbc.sd", "exclude")]

  # 16.  Exclude measurements for subjects/parameters with only 1 or 2 measurements with exc_*=0. This step uses a variety of criteria,
  #      including the tbc*sd of the other parameter.
  if (!quietly) {
    cat(sprintf(
      "[%s] Exclude single measurements and pairs...\n",
      Sys.time()
    ))
  }
  data.df$tbc.other.sd <- swap_parameters(df = data.df)
  data.df[,
    exclude := (function(df) {
      valid.rows <- which(valid(df))
      # calculate median SD for other parameter (used in steps below)
      median.tbc.other.sd <- median(df$tbc.other.sd)
      # a.  Identify subjects/parameters with only 2 values with exc_*==0, and determine the following:
      #   1.  d_tbc*sd_other=the absolute value difference in tbc*sd
      #   2.	d_agedays_other=the difference in agedays between the 2 values
      #   3.	tbcOsd=the tbc*sd for the other parameter on the same ageday
      # NOTE: this is done as the first step below for efficiency
      #   4.	median_tbcOsd= the median tbc*sd for the other parameter
      #   5.	abs_d_*_O is the absolute value of the difference between tbc*sd and tbcOsd if tbcOsd is not missing;
      #       it is the absolute value of the difference between tbc*sd and median_tbcOsd if tbcOsd is missing but median_tbcOsd is not missing;
      #       and is missing if tbcOsd and median_tbcOsd are both missing
      if (length(valid.rows) == 2) {
        delta.tbc.sd.other <- abs(df$tbc.sd[valid.rows[1]] - df$tbc.sd[valid.rows[2]])
        delta.agedays.other <- abs(df$agedays[valid.rows[1]] - df$agedays[valid.rows[2]])
        abs.delta.other <- with(df, ifelse(
          !is.na(tbc.other.sd),
          abs(tbc.sd - tbc.other.sd),
          ifelse(
            !is.na(median.tbc.other.sd),
            abs(tbc.sd - median.tbc.other.sd),
            NA
          )
        ))

        # b.	For subjects/parameters with 2 values with exc_*==0, replace exc_*=17 if one of the following sets of criteria are met:
        #   1.	If |d_agedays_other|>365.25 and |d_tbc*sd_other|>3; replace exc_*=17 for the value of the pair that has the largest abs_d_*_O. If abs_d_*O is missing, replace exc_*=17 for the value of the pair with the higher |tbc*sd|
        #   2.	If |d_agedays_other|<365.25 and |d_tbc*sd_other|>2; replace exc_*=18 for the value of the pair that has the largest abs_d_*_O. If abs_d_*O is missing, replace exc_*=18 for the value of the pair with the higher |tbc*sd|
        #
        # MODE default
        #   1.  uses current behavior outlined in b, in else statement
        #
        # MODE flag.both
        #   1.  similar to b, except in cases where only HTs values exist or only WTs values exist, both are dropped instead of only 1 (first if)
        #
        #
        # check whether each value in double has other parameters (make sure they don't first)
        if (
          lt3.exclude.mode == "flag.both" &&
            is.na(df$tbc.other.sd[valid.rows[1]]) &&
            is.na(df$tbc.other.sd[valid.rows[2]]) &&
            is.na(abs.delta.other)
        ) {
          # if they don't then do test
          # check default thresholds, if they fail then exclude both regardless of tbc.sd
          if (na_as_false(delta.agedays.other >= 365.25 & delta.tbc.sd.other > 3)) {
            df$exclude[valid.rows[1]] <- "Exclude-Pair-Delta-19"
            df$exclude[valid.rows[2]] <- "Exclude-Pair-Delta-19"
          } else if (na_as_false(delta.agedays.other < 365.25 & delta.tbc.sd.other > 2)) {
            df$exclude[valid.rows[1]] <- "Exclude-Pair-Delta-19"
            df$exclude[valid.rows[2]] <- "Exclude-Pair-Delta-19"
          }
        } else {
          # default method if other mode is not used or if other parameters exist
          worst.row <- order(valid(df),
            abs(df$tbc.sd),
            abs(abs.delta.other),
            decreasing = TRUE
          )[1]
          if (na_as_false(delta.agedays.other >= 365.25 & delta.tbc.sd.other > 3)) {
            df$exclude[worst.row] <- "Exclude-Pair-Delta-17"
          } else if (na_as_false(delta.agedays.other < 365.25 & delta.tbc.sd.other > 2)) {
            df$exclude[worst.row] <- "Exclude-Pair-Delta-18"
          }
        }
      }
      # c.	Identify subjects/parameters with exactly 1 value for which exc_*=0. This will include subjects/parameters for which a value was excluded in step 16b.
      #     Determine tbcOsd and median_tbcOsd as described in step 16a3 and 16a4 above.
      # d.	For subjects/parameters with 1 value for which exc_*=0,  replace exc_*=19 if one of the following sets of criteria are met:
      #   1.	|tbc*sd|>3 & |tbc*sd-tbcOsd|>5 & tbcOsd is not missing
      #   2.	|tbc*sd|>3 & |tbc*sd-median_tbcOsd|>5 & tbcOsd is missing & median_tbcOsd is not missing
      #   3.	|tbc*sd|>5 & tbcOsd is missing & median_tbcOsd is missing
      valid.rows <- valid(df)
      # NOTE: valid.rows is now a boolean.  was a row number in code above
      if (sum(valid.rows) == 1) {
        # cat("param = ", df[valid.rows, param])
        df[
          valid.rows &
            (
              abs(tbc.sd) > 3 &
                abs(
                  tbc.sd - ifelse(
                    !is.na(tbc.other.sd),
                    tbc.other.sd,
                    median.tbc.other.sd
                  )
                ) > 5 |
                abs(tbc.sd) > 5 & is.na(tbc.other.sd) & is.na(median.tbc.other.sd)
            ),
          exclude := "Exclude-Single-Outlier"
        ]
      }

      # ensure factor levels didn't accidentally get mangled
      return(factor(df$exclude, levels = exclude.levels, ordered = TRUE))
    })(copy(.SD)),
    by = .(subjid, param),
    .SDcols = c("agedays", "tbc.sd", "tbc.other.sd", "exclude", "param")
  ] # added param here for debugging, i think it's dropped otherwise

  # 17.  Exclude measurements based on error load for the subject
  # a.	For each subject/parameter determine the following:
  #   1.	tot_exc_*=the total number of values for which exc_* is equal to 4, 5, 6, or 8-19
  #   2.	tot_inc_*=the total number of values for which exc_*=0
  # b.	For subjects/parameters where tot_exc_*>0.5 x tot_inc_*; replace all values where exc_* had been 0 to exc_*=20
  # c.	For subjects/parameters where tot_exc_*>tot_inc_*, replace all values for the OTHER parameter where exc_* had been 0 to exc_*=21
  #
  # CD e-mail 2/10/15: It looks like there are two problems. One is carried forward measurements. The other is that I forgot to include an
  # important part of the rule in the English - tot_exc_* has to be >=2. I'm forwarding an updated English version.
  # Also, because your exclusions are handled a little differently I wanted to specify that unit errors and swaps are not included
  # in tot_exc_* and are included in tot_inc_*, whereas carried forwards and duplicates are not included in either count.
  # NOTE: updated to include optional argument "include.carryforward=TRUE" in the valid() function, and added the
  # constraints "exclude.count.this.parameter >= 2" and "exclude.count.other.parameter >= 2" in the code below

  if (!quietly) {
    cat(
      sprintf(
        "[%s] Exclude all measurements if maximum threshold of errors is exceeded...\n",
        Sys.time()
      )
    )
  }
  # NOTE: restrict analysis to non-missing rows
  data.df[
    exclude != "Missing",
    exclude := (function(subj.df) {
      inc.exc <- subj.df[,
        list(
          exclude.count = sum(
            !valid(
              exclude,
              include.duplicates = TRUE,
              include.carryforward = TRUE
            )
          ),
          include.count = sum(valid(exclude))
        ),
        keyby = param
      ]
      for (p in unique(subj.df$param)) {
        exclude.count.this.parameter <- inc.exc[param == p, exclude.count]
        exclude.count.other.parameter <- sum(inc.exc[param != p, exclude.count])
        if (
          exclude.count.this.parameter > error.load.threshold * inc.exc[param == p, include.count] &
            exclude.count.this.parameter >= error.load.mincount
        ) {
          subj.df[param == p & valid(exclude), exclude := "Exclude-Too-Many-Errors"]
        } else if (
          exclude.count.other.parameter > sum(inc.exc[param != p, include.count]) &
            exclude.count.other.parameter >= error.load.mincount
        ) {
          subj.df[param == p & valid(exclude), exclude := "Exclude-Too-Many-Errors-Other-Parameter"]
        }
      }
      # ensure factor levels didn't accidentally get mangled
      return(factor(subj.df$exclude, levels = exclude.levels, ordered = TRUE))
    })(copy(.SD)),
    by = subjid,
    .SDcols = c("param", "exclude")
  ]

  if (!quietly) {
    cat(sprintf("[%s] Completed Batch #%s...\n", Sys.time(), data.df$batch[1]))
  }
  if (!quietly & parallel) {
    sink()
  }
  return(data.df[j = .(line, exclude, tbc.sd, tbc.other.sd, param)]) # debugging
}

#' Clean growth measurements
#'
#' @param subjid Vector of unique identifiers for each subject in the database.
#' @param param Vector identifying each measurement, may be 'WEIGHTKG', 'HEIGHTCM', or 'LENGTHCM'
#'   'HEIGHTCM' vs. 'LENGTHCM' only affects z-score calculations between ages 24 to 35 months (730 to 1095 days).
#'   All linear measurements below 731 days of life (age 0-23 months) are interpreted as supine length, and
#'   all linear measurements above 1095 days of life (age 36+ months) are interpreted as standing height.
#' @param agedays Numeric vector containing the age in days at each measurement.
#' @param sex Vector identifying the gender of the subject, may be 'M', 'm', or 0 for males, vs. 'F', 'f' or 1 for females.
#' @param measurement Numeric vector containing the actual measurement data.  Weight must be in
#'   kilograms (kg), and linear measurements (height vs. length) in centimeters (cm).
#' @param recover.unit.error Indicates whether the cleaning algorithm should
#' attempt to identify unit errors (I.e. inches vs. cm, lbs vs. kg). If unit
#' errors are identified, the value will be corrected and retained within the
#' cleaning algorithm as a valid measurement.  Defaults to FALSE.
#' @param sd.extreme Measurements more than sd.extreme standard deviations from
#' the mean (either above or below) will be flagged as invalid. Defaults to 25.
#' @param z.extreme Measurements with an absolute z-score greater than
#' z.extreme will be flagged as invalid. Defaults to 25.
#' @param lt3.exclude.mode Determines type of exclusion procedure to use for 1 or 2 measurements of one type without
#' matching same ageday measurements for the other parameter. Options include "default" (standard growthcleanr approach),
#' and "flag.both" (in case of two measurements of one type without matching values for the other parameter, flag both
#' for exclusion if beyond threshold)
#' @param error.load.mincount minimum count of exclusions on parameter before
#' considering excluding all measurements. Defaults to 2.
#' @param error.load.threshold threshold of percentage of excluded measurement count to included measurement
#' count that must be exceeded before excluding all measurements of either parameter. Defaults to 0.5.
#' @param sd.recenter Data frame or table with median SD-scores per day of life
#' by gender and parameter. Columns in the table must include param, sex,
#' agedays, and sd.median.  If not supplied, the median values will be
#' calculated using the growth data that is being cleaned. Defaults to NA.
#' @param sdmedian.filename Name of file to save sd.median data calculated on the input dataset to as CSV.
#' Defaults to "", for which this data will not be saved. Use for extracting medians for parallel processing
#' scenarios other than the built-in parallel option.
#' @param sdrecentered.filename Name of file to save re-centered data to as CSV. Defaults to "", for which this
#' data will not be saved. Useful for post-processing and debugging.
#' @param include.carryforward Determines whether Carry-Forward values are kept in the output. Defaults to False.
#' @param ewma.exp Exponent to use for weighting measurements in the
#' exponentially weighted moving average calculations. Defaults to -1.5.
#' This exponent should be negative in order to weight growth measurements
#' closer to the measurement being evaluated more strongly. Exponents that are
#' further from zero (e.g. -3) will increase the relative influence of
#' measurements close in time to the measurement being evaluated compared to
#' using the default exponent.
#' @param ref.data.path Path to reference data. If not supplied, the year 2000
#' Centers for Disease Control (CDC) reference data will be used.
#' @param log.path Path to log file output when running in parallel (non-quiet mode). Default is ".". A new
#' directory will be created if necessary.
#' @param parallel Determines if function runs in parallel.  Defaults to FALSE.
#' @param num.batches Specify the number of batches to run in parallel. Only
#' applies if parallel is set to TRUE. Defaults to the number of workers
#' returned by the getDoParWorkers function in the foreach package.
#' @param quietly Determines if function messages are to be displayed and if log files (parallel only) are to be generated.
#' Defaults to TRUE
#'
#' @return Vector of exclusion codes for each of the input measurements.
#'
#'    Possible values for each code are:
#'
#' * 'Include', 'Unit-Error-High', 'Unit-Error-Low', 'Swapped-Measurements', 'Missing',
#' *  'Exclude-Carried-Forward', 'Exclude-SD-Cutoff', 'Exclude-EWMA-Extreme', 'Exclude-EWMA-Extreme-Pair',
#' *  'Exclude-Duplicate',
#' *  'Exclude-EWMA-8', 'Exclude-EWMA-9', 'Exclude-EWMA-10', 'Exclude-EWMA-11', 'Exclude-EWMA-12', 'Exclude-EWMA-13', 'Exclude-EWMA-14',
#' *  'Exclude-Min-Height-Change', 'Exclude-Max-Height-Change',
#' *  'Exclude-Pair-Delta-17', 'Exclude-Pair-Delta-18', 'Exclude-Pair-Delta-19',
#' *  'Exclude-Single-Outlier', 'Exclude-Too-Many-Errors', 'Exclude-Too-Many-Errors-Other-Parameter'
#' @md
#'
#' @export
#' @import data.table
#' @rawNamespace import(plyr, except = c(failwith, id, summarize, count, desc, mutate, arrange, rename, is.discrete, summarise, summarize))
#' @import foreach
#' @import doParallel
#' @examples
#' # Run calculation using a small subset of given data
#' df_stats <- as.data.frame(syngrowth)
#' df_stats <- df_stats[df_stats$subjid %in% unique(df_stats[, "subjid"])[1:5], ]
#'
#' clean_stats <- cleangrowth(
#'   subjid = df_stats$subjid,
#'   param = df_stats$param,
#'   agedays = df_stats$agedays,
#'   sex = df_stats$sex,
#'   measurement = df_stats$measurement
#' )
#'
#' # Once processed you can filter data based on result value
#' df_stats <- cbind(df_stats, "clean_result" == clean_stats)
#' clean_df_stats <- df_stats[, df_stats$clean_result == "Include"]
#'
#' # Parallel processing: run using 3 cores and batches
#' df_stats <- cleangrowth(
#'   subjid = df_stats$subjid,
#'   param = df_stats$param,
#'   agedays = df_stats$agedays,
#'   sex = df_stats$sex,
#'   measurement = df_stats$measurement,
#'   parallel = TRUE,
#'   num.batches = 2
#' )
cleangrowth <- function(subjid,
                        param,
                        agedays,
                        sex,
                        measurement,
                        recover.unit.error = FALSE,
                        sd.extreme = 25,
                        z.extreme = 25,
                        lt3.exclude.mode = "default",
                        height.tolerance.cm = 2.5,
                        error.load.mincount = 2,
                        error.load.threshold = 0.5,
                        sd.recenter = NA,
                        sdmedian.filename = "",
                        sdrecentered.filename = "",
                        include.carryforward = FALSE,
                        ewma.exp = -1.5,
                        ref.data.path = "",
                        log.path = ".",
                        parallel = FALSE,
                        num.batches = NA,
                        quietly = TRUE) {
  # organize data into a dataframe along with a line "index" so the original data order can be recovered
  data.all <- data.table(
    line = seq_along(measurement),
    subjid = as.factor(subjid),
    param,
    agedays = as.integer(agedays),
    v = ifelse(measurement == 0, NaN, measurement),
    sex = as.integer(ifelse(
      sex %in% c(0, "m", "M"), 0, ifelse(sex %in% c(1, "f", "F"), 1, NA)
    ))
  )

  # if parallel processing is desired, load additional modules
  if (parallel) {
    registerDoParallel(cores = num.batches)
    if (is.na(num.batches)) {
      num.batches <- getDoParWorkers()
    }
  } else {
    if (is.na(num.batches)) {
      num.batches <- 1
    }
  }

  setkeyv(data.all, "subjid")
  subjid.unique <- data.all[j = unique(subjid)]
  batches.all <- data.table(
    subjid = subjid.unique,
    batch = sample(num.batches, length(subjid.unique), replace = TRUE),
    key = "subjid"
  )
  data.all <- batches.all[data.all]

  # load tanner height velocity data. sex variable is defined such that 0=male and 1=female
  # recode column names to match syntactic style ("." rather than "_" in variable names)
  tanner_ht_vel_path <- ifelse(
    ref.data.path == "",
    system.file("extdata/tanner_ht_vel.csv", package = "growthcleanr"),
    paste0(ref.data.path, "tanner_ht_vel.csv")
  )

  tanner.ht.vel <- fread(tanner_ht_vel_path)

  setnames(
    tanner.ht.vel,
    colnames(tanner.ht.vel),
    gsub("_", ".", colnames(tanner.ht.vel))
  )
  setkeyv(tanner.ht.vel, c("sex", "tanner.months"))
  # keep track of column names in the tanner data
  tanner.fields <- colnames(tanner.ht.vel)
  tanner.fields <- tanner.fields[!tanner.fields %in% c("sex", "tanner.months")]

  who_max_ht_vel_path <- ifelse(
    ref.data.path == "",
    system.file("extdata/who_ht_maxvel_3sd.csv", package = "growthcleanr"),
    paste0(ref.data.path, "who_ht_maxvel_3sd.csv")
  )

  who_ht_vel_3sd_path <- ifelse(
    ref.data.path == "",
    system.file("extdata/who_ht_vel_3sd.csv", package = "growthcleanr"),
    paste0(ref.data.path, "who_ht_vel_3sd.csv")
  )
  who.max.ht.vel <- fread(who_max_ht_vel_path)
  who.ht.vel <- fread(who_ht_vel_3sd_path)
  setkeyv(who.max.ht.vel, c("sex", "whoagegrp_ht"))
  setkeyv(who.ht.vel, c("sex", "whoagegrp_ht"))
  who.ht.vel <- as.data.table(dplyr::full_join(who.ht.vel, who.max.ht.vel, by = c("sex", "whoagegrp_ht")))

  setnames(who.ht.vel, colnames(who.ht.vel), gsub("_", ".", colnames(who.ht.vel)))
  setkeyv(who.ht.vel, c("sex", "whoagegrp.ht"))
  # keep track of column names in the who growth velocity data
  who.fields <- colnames(who.ht.vel)
  who.fields <- who.fields[!who.fields %in% c("sex", "whoagegrp.ht")]

  # 1.  General principles
  # a.	All steps are done separately for each parameter unless otherwise noted
  # b.	All steps are done sorted by subject, parameter, and age (in days) for nonexcluded and nonmissing values only unless otherwise noted. This is very important.
  #     Sorting needs to be redone with each step to account for excluded and transformed  values.
  # c.	The next value refers to the value with the next highest age for the same parameter and the same subject, and the previous value refers to the value with the
  #     next lowest age for the same parameter and the same subject.
  # d.	You will need to set up a method for keeping track of whether a value is missing or excluded (and in what step). I use variables called exc_* that are =0
  #     if a value is to be included, =1 if missing, and =2 or higher if it is to be excluded, with each number indicating a different step. I also set up
  #     parameter-specific subjid_* variables that are = to subjid for included values and are blank if the value is missing or should be excluded. These subjid_*
  #     variables need to be updated with each step.
  # e.  All steps assume that data are sorted by subjid_*, parameter, and age (in days) for nonexcluded and nonmissing values only
  #     unless otherwise noted. Sorting needs to be redone after any transformations or exclusions to account for excluded and
  #     transformed values.
  # f.  The next value refers to the nonexcluded nonmissing value with the next highest age for the same parameter and the same
  #     subject, and the previous value refers to the nonexcluded nonmissing value with the next lowest age for the same parameter
  #     and the same subject.
  # g.  exc_* should only be replaced with a  higher value if exc_*==0 at the time of replacement, unless otherwise specified.


  # NOTE: in the R code below exclusion is documented as a series of factor levels, where all levels occuring before 'Exclude' in the sequence are considered
  # to be valid measurements.  We use the built in sorting of the data.table object and subsets rather than re-sorting at each step
  # to ensure that only valid measurements are used at the beginning of each step.
  # Also, unlike the Stata code, the measurement parameter (weight vs. height) is recorded as a factor in the data frame, rather than as a variable name

  # 2.  Data set-up
  # a.	I always code sex as 0=Male, 1=Female, so I recoded the variable sex that way and left a variable sexorigcode the way the data was sent to me (1=Female 2=Male)
  # b.	Remove rows that are duplicates for subjid, param, and measurement from further analysis
  #     NOTE: this step is not needed -- handled automatically by "temporary duplicate" step.
  # c.  I generated separate variables for weight (wt) and height (ht), as well as exc_* and subjid_* variables. Set exc_*=0 if value is not missing
  #     and exc_*=1 if value is missing. In all future steps, exc_* should only be changed if it is 0. This helps to keep track of which step excluded a value.
  #     I also kept the measurement variable there and untouched because sometimes wt and ht got transformed to something else.
  # d.	I made tables based on CDC growth curve parameters that include data for each day that I will send separately. The LMS parameters for each day are
  #     cubically interpolated from the values by month available on the CDC website. Create wt and ht z-scores for each value of each parameter (Z: WtZ and HtZ).
  # e.	There are variables in the table labelled cdc_*_csd_pos and cdc_*_csd_neg. For each age and sex, these correspond to ½ of the absolute value of the
  #     median and the value with a z-score of +2 (csd_pos) and -2 (csd_neg). These can be created to generate a score similar to the z-score but with an
  #     important difference. The z-scores created using the LMS method account for skewness in the distribution of the parameters (particularly weight), which
  #     can lead to small changes in z-score with large changes in weight in subjects with very high weight, and relatively large changes in z-score for smaller
  #     changes in weight in subjects with low weights.  The score we will create can be called an SD-score (SDorig: WtSDorig and HtSDorig that is calculated by
  #     dividing the difference between the value and the median by the SD score (use csd_pos if the value is above the median, csd_neg if the value is below the
  #     median). These SD-scores, rather than z-scores, now form the basis for the algorithm.

  # calculate z scores
  if (!quietly) {
    cat(sprintf("[%s] Calculating z-scores...\n", Sys.time()))
  }
  measurement.to.z <- read.anthro(ref.data.path, cdc.only = TRUE)
  data.all[, z.orig := measurement.to.z(param, agedays, sex, v)]

  # calculate "standard deviation" scores
  if (!quietly) {
    cat(sprintf("[%s] Calculating SD-scores...\n", Sys.time()))
  }
  data.all[, sd.orig := measurement.to.z(param, agedays, sex, v, TRUE)]

  # sort by subjid, param, agedays
  setkeyv(data.all, c("subjid", "param", "agedays"))

  # add a new convenience index for bookkeeping
  data.all[, index := 1:.N]

  # enumerate the different exclusion levels
  exclude.levels <- c(
    "Include",
    "Unit-Error-High",
    "Unit-Error-Low",
    "Unit-Error-Possible",
    "Swapped-Measurements",
    "Exclude",
    "Missing",
    "Exclude-Temporary-Duplicate",
    "Exclude-Carried-Forward",
    "Exclude-SD-Cutoff",
    "Exclude-EWMA-Extreme",
    "Exclude-EWMA-Extreme-Pair",
    "Exclude-Duplicate",
    "Exclude-EWMA-8",
    "Exclude-EWMA-9",
    "Exclude-EWMA-10",
    "Exclude-EWMA-11",
    "Exclude-EWMA-12",
    "Exclude-EWMA-13",
    "Exclude-EWMA-14",
    "Exclude-Min-Height-Change",
    "Exclude-Max-Height-Change",
    "Exclude-Pair-Delta-17",
    "Exclude-Pair-Delta-18",
    "Exclude-Pair-Delta-19",
    "Exclude-Single-Outlier",
    "Exclude-Too-Many-Errors",
    "Exclude-Too-Many-Errors-Other-Parameter"
  )

  # Mark missing values for exclusion
  data.all[,
   exclude := factor(with(data.all, ifelse(
        is.na(v) |
          agedays < 0, "Missing", "Include"
      )),
      levels = exclude.levels,
      ordered = TRUE
    )
  ]

  # after calculating z scores, for convenience, recategorize linear parameters as 'HEIGHTCM'
  data.all[param == "LENGTHCM", param := "HEIGHTCM"]

  # define field names needed by helper functions
  ewma.fields <- c("ewma.all", "ewma.before", "ewma.after")

  # 3.  SD-score recentering: Because the basis of the method is comparing SD-scores over time, we need to account for the fact that
  #     the mean SD-score for the population changes with age.
  # a.	Determine the median cdc*sd for each parameter by year of age (with sexes combined): median*sd.
  # b.	The median*sd should be considered to apply to midyear-age, defined as the age in days with the same value as the integer
  #     portion of (365.25*year + 365.25/2).
  # c.	Linearly interpolate median*sd for each parameter between each midyear-age, naming the interpolated values rc*sd.
  # d.	For ages below the first midyear-age, let rc*sd equal the median*sd for the earliest year.
  #     For ages above the last midyear_age, let rc*sd equal the median*sd for the last year.
  # e.	Subtract rcsd_* from SDorig to create the recentered SD-score.  This recentered SD-score, labeled tbc*sd
  #     (stands for "to be cleaned") will be used for most of the rest of the analyses.
  # f.	In future steps I will sometimes refer to measprev and measnext which refer to the previous or next wt or ht measurement
  #     for which exc_*==0 for the subject and parameter, when the data are sorted by subject, parameter, and agedays. SDprev and SDnext refer to the tbc*sd of the previous or next measurement.

  if (!quietly) {
    cat(sprintf("[%s] Re-centering data...\n", Sys.time()))
  }

  # see function definition below for explanation of the re-centering process
  # returns a data table indexed by param, sex, agedays
  if (!is.data.table(sd.recenter)) {
    sd.recenter <- data.all[exclude < "Exclude", sd.median(param, sex, agedays, sd.orig)]
    if (sdmedian.filename != "") {
      write.csv(sd.recenter, sdmedian.filename, row.names = FALSE)
      if (!quietly) {
        cat(
          sprintf(
            "[%s] Wrote re-centering medians to %s...\n",
            Sys.time(),
            sdmedian.filename
          )
        )
      }
    }
  } else {
    # ensure passed-in medians are sorted correctly
    setkeyv(sd.recenter, c("param", "sex", "agedays"))
  }
  # add sd.recenter to data, and recenter
  setkeyv(data.all, c("param", "sex", "agedays"))
  data.all <- sd.recenter[data.all]
  setkeyv(data.all, c("subjid", "param", "agedays"))
  data.all[, tbc.sd := sd.orig - sd.median]

  if (sdrecentered.filename != "") {
    write.csv(data.all, sdrecentered.filename, row.names = FALSE)
    if (!quietly) {
      cat(
        sprintf(
          "[%s] Wrote re-centered data to %s...\n",
          Sys.time(),
          sdrecentered.filename
        )
      )
    }
  }

  # safety check: treat observations where tbc.sd cannot be calculated as missing
  data.all[is.na(tbc.sd), exclude := "Missing"]

  # NOTE: the rest of cleangrowth's steps are done through cleanbatch().

  # optionally process batches in parallel
  if (!quietly) {
    cat(sprintf(
      "[%s] Cleaning growth data in %d batch(es)...\n",
      Sys.time(),
      num.batches
    ))
  }
  if (num.batches == 1) {
    ret.df <- cleanbatch(data.all,
      log.path = log.path,
      quietly = quietly,
      parallel = parallel,
      measurement.to.z = measurement.to.z,
      ewma.fields = ewma.fields,
      ewma.exp = ewma.exp,
      recover.unit.error = recover.unit.error,
      include.carryforward = include.carryforward,
      sd.extreme = sd.extreme,
      z.extreme = z.extreme,
      exclude.levels = exclude.levels,
      tanner.ht.vel = tanner.ht.vel,
      who.ht.vel = who.ht.vel,
      lt3.exclude.mode = lt3.exclude.mode,
      error.load.threshold = error.load.threshold,
      error.load.mincount = error.load.mincount
    )
  } else {
    # create log directory if necessary
    if (!quietly) {
      cat(sprintf("[%s] Writing batch logs to '%s'...\n", Sys.time(), log.path))
    }
    ifelse(!dir.exists(log.path), dir.create(log.path), FALSE)

    ret.df <- ddply(
      data.all,
      .(batch),
      cleanbatch,
      .parallel = parallel,
      .paropts = list(.packages = "data.table"),
      log.path = log.path,
      quietly = quietly,
      parallel = parallel,
      measurement.to.z = measurement.to.z,
      ewma.fields = ewma.fields,
      ewma.exp = ewma.exp,
      recover.unit.error = recover.unit.error,
      include.carryforward = include.carryforward,
      sd.extreme = sd.extreme,
      z.extreme = z.extreme,
      exclude.levels = exclude.levels,
      tanner.ht.vel = tanner.ht.vel,
      who.ht.vel = who.ht.vel,
      lt3.exclude.mode = lt3.exclude.mode,
      error.load.threshold = error.load.threshold,
      error.load.mincount = error.load.mincount
    )
    stopImplicitCluster()
  }
  if (!quietly) {
    cat(sprintf("[%s] Done!\n", Sys.time()))
  }

  return(ret.df$exclude[order(ret.df$line)])
}

#' Function to calculate z-scores and csd-scores based on anthro tables.
#'
#' @param path Path to supplied reference anthro data. Defaults to package anthro tables.
#' @param cdc.only Whether or not only CDC data should be used. Defaults to false.
#'
#' @return Function for calculating BMI based on measurement, age in days, sex, and measurement value.
#' @export
#' @import data.table
#' @examples
#' # Return calculating function with all defaults
#' afunc <- read.anthro()
#'
#' # Return calculating function while specifying a path and using only CDC data
#' afunc <- read.anthro(
#'   path = system.file("extdata", package = "growthcleanr"),
#'   cdc.only = TRUE
#' )
read.anthro <- function(path = "", cdc.only = FALSE) {
  # set correct path based on input reference table path (if any)
  weianthro_path <- ifelse(
    path == "",
    system.file(file.path("extdata", "weianthro.txt"), package = "growthcleanr"),
    file.path(path, "weianthro.txt")
  )
  lenanthro_path <- ifelse(
    path == "",
    system.file(file.path("extdata", "lenanthro.txt"), package = "growthcleanr"),
    file.path(path, "lenanthro.txt")
  )
  bmianthro_path <- ifelse(
    path == "",
    system.file(file.path("extdata", "bmianthro.txt"), package = "growthcleanr"),
    file.path(path, "bmianthro.txt")
  )
  growth_cdc_ext_path <- ifelse(
    path == "",
    system.file(file.path("extdata", "growthfile_cdc_ext.csv"), package = "growthcleanr"),
    file.path(path, "growthfile_cdc_ext.csv")
  )


  growth_cdc_ext <- read.csv(growth_cdc_ext_path)

  l <- list(
    with(
      read.table(weianthro_path, header = TRUE),
      data.frame(
        src = "WHO",
        param = "WEIGHTKG",
        sex = sex - 1,
        age,
        l,
        m,
        s,
        csdpos = as.double(NA),
        csdneg = as.double(NA)
      )
    ),
    with(
      read.table(lenanthro_path, header = TRUE),
      data.frame(
        src = "WHO",
        param = "HEIGHTCM",
        sex = sex - 1,
        age,
        l,
        m,
        s,
        csdpos = as.double(NA),
        csdneg = as.double(NA)
      )
    ),
    with(
      read.table(bmianthro_path, header = TRUE),
      data.frame(
        src = "WHO",
        param = "BMI",
        sex = sex - 1,
        age,
        l,
        m,
        s,
        csdpos = as.double(NA),
        csdneg = as.double(NA)
      )
    ),
    with(
      growth_cdc_ext,
      data.frame(
        src = "CDC",
        param = "WEIGHTKG",
        sex,
        age = agedays,
        l = cdc_wt_l,
        m = cdc_wt_m,
        s = cdc_wt_s,
        csdpos = cdc_wt_csd_pos,
        csdneg = cdc_wt_csd_neg
      )
    ),
    with(
      growth_cdc_ext,
      data.frame(
        src = "CDC",
        param = "HEIGHTCM",
        sex,
        age = agedays,
        l = cdc_ht_l,
        m = cdc_ht_m,
        s = cdc_ht_s,
        csdpos = cdc_ht_csd_pos,
        csdneg = cdc_ht_csd_neg
      )
    ),
    with(
      growth_cdc_ext,
      data.frame(
        src = "CDC",
        param = "BMI",
        sex,
        age = agedays,
        l = cdc_bmi_l,
        m = cdc_bmi_m,
        s = cdc_bmi_s,
        csdpos = cdc_bmi_csd_pos,
        csdneg = cdc_bmi_csd_neg
      )
    )
  )

  anthro <- rbindlist(l)


  setkeyv(anthro, c("src", "param", "sex", "age"))

  return(function(param, agedays, sex, measurement, csd = FALSE) {
    # For now, we will only use CDC growth reference data, note that the cubically interpolated file
    # we are using has linear measurments derived from length data for children < 731 days, and height thereafter
    src <- ifelse(agedays < 731 & !cdc.only, "WHO", "CDC")

    # keep column sequence the same fo efficient join
    dt <- data.table(src, param, sex, agedays, measurement)
    dt <- anthro[dt]

    dt[, ret := as.double(NA)]
    if (csd) {
      dt[measurement < m, ret := (measurement - m) / csdneg]
      dt[measurement >= m, ret := (measurement - m) / csdpos]
    } else {
      dt[l == 0, ret := log(measurement / m) / s]
      dt[l != 0, ret := (((measurement / m)^l) - 1) / (l * s)]
    }

    return(dt$ret)
  })
}

#' Exponentially Weighted Moving Average (EWMA)
#'
#' \code{ewma} calculates the exponentially weighted moving average (EWMA) for a set of numeric observations over time.
#'
#' @param agedays Vector of age in days for each z score (potentially transformed to adjust weighting).
#'
#' @param z Input vector of numeric z-score data.
#'
#' @param ewma.exp Exponent to use for weighting.
#'
#' @param ewma.adjacent Specify whether EWMA values excluding adjacent measurements should be calculated.  Defaults to TRUE.
#'
#' @return Data frame with 3 variables:
#' * The first variable (ewma.all) contains the EWMA at observation time
#'   excluding only the actual observation for that time point.
#' * The second variable (ewma.before) contains the EWMA for each observation excluding both the actual observation
#'   and the immediate prior observation.
#' * The third variable (ewma.after) contains the EWMA for each observation excluding both the actual observation
#'   and the subsequent observation.
#' @md
#'
#' @export
#' @examples
#' # Run on 1 subject, 1 type of parameter
#' df_stats <- as.data.frame(syngrowth)
#' df_stats <- df_stats[df_stats$subjid == df_stats$subjid[1] &
#'   df_stats$param == "HEIGHTCM", ]
#'
#' # Get the uncentered z-scores
#' measurement_to_z <- read.anthro(cdc.only = TRUE)
#' sd <- measurement_to_z(
#'   df_stats$param,
#'   df_stats$agedays,
#'   df_stats$sex,
#'   df_stats$measurement,
#'   TRUE
#' )
#'
#' # Calculate exponentially weighted moving average
#' e_df <- ewma(df_stats$agedays, sd, ewma.exp = -1.5)
ewma <- function(agedays, z, ewma.exp, ewma.adjacent = TRUE) {
  # 6.  EWMA calculation description: Most of the next steps will involve calculating the exponentially weighted moving average for each subject and parameter. I will
  #     describe how to calculate EWMASDs, and will describe how it needs to be varied in subsequent steps.
  # a.	The overall goal of the EWMASD calculation is to identify the difference between the SD-score and what we might predict that DS-score should be, in order to
  #     determine whether it should be excluded.
  # b.	Only nonmissing SD-scores for a parameter that are not designated for exclusion are included in the following calculations.
  # c.	For each SD-score SDi and associated agedaysi calculate the following for every other z-score (SDj…SDn) and associated agedays (agedaysj…agedaysn)  for the
  #     same subject and parameter
  #   i.	ΔAgej=agedaysj-agedaysi
  #   ii.	EWMAZ=SDi=[Σj→n(SDj*((5+ΔAgej)^-1.5))]/[ Σj→n((5+ΔAgej)^-1.5)]
  #   iii.	For most EWMASD calculations, there are 3 EWMASDs that need to be calculated. I will note if not all of these need to be done for a given step.
  #     1.	EWMASDall calculated as above
  #     2.	EWMAZbef calculated excluding the SD-score just before the SD-score of interest (sorted by agedays). For the first observation for a parameter for a
  #         subject, this should be identical to EWMASDall rather than missing.
  #     3.	EWMAZaft calculated excluding the z-score just after the SD-score of interest (sorted by agedays). For the lastobservation for a parameter for a subject,
  #         this should be identical to EWMASDall rather than missing.
  #   iv.	For each of the three EWMASDs, calculate the dewma_*=SD-EWMASD
  # d.	EWMASDs and ΔEWMASDs will change if a value is excluded or manipulated using one of the methods below, therefore EWMASDs and ΔEWMASDs be recalculated for each
  #     step where they are needed.
  # e.	For these calculations, use variables that allow for precise storage of numbers (in Stata this is called 'double') because otherwise rounding errors can cause
  #     problems in a few circumstances

  n <- length(agedays)
  # initialize response variables
  ewma.all <- ewma.before <- ewma.after <- vector("numeric", 0)
  if (n > 0) {
    # organize into data frame and sort into order of increasing age,
    # but retain original sort order information in index
    if (!all(agedays == cummax(agedays))) {
      warning("EWMA ordering is not sorted; double check")
    } # add in a check to make sure the inputs are already sorted (they should be)
    index <- order(agedays)


    # calculate matrix of differences in age, and add 5 to each delta per Daymont algorithm
    delta <- as_matrix_delta(agedays)
    delta <- ifelse(delta == 0, 0, (delta + 5)^ewma.exp)

    # calculate EWMAs, and return in order of original data
    ewma.all[index] <- delta %*% z / apply(delta, 1, sum)

    if (ewma.adjacent) {
      if (n > 2) {
        delta2 <- delta
        delta2[col(delta2) == row(delta2) - 1] <- 0
        ewma.before[index] <- delta2 %*% z / apply(delta2, 1, sum)
        delta3 <- delta
        delta3[col(delta3) == row(delta3) + 1] <- 0
        ewma.after[index] <- delta3 %*% z / apply(delta3, 1, sum)
      } else {
        ewma.before <- ewma.after <- ewma.all
      }
    }
  }
  # return all 3 EWMAs as a data frame
  if (ewma.adjacent) {
    data.frame(ewma.all, ewma.before, ewma.after)
  } else {
    data.frame(ewma.all)
  }
}

#' Function to convert vector of ages, in days, into matrix
#'
#' @keywords internal
#' @noRd
as_matrix_delta <- function(agedays) {
  n <- length(agedays)
  abs(matrix(rep(agedays, n), n, byrow = TRUE) - agedays)
}

#' Calculate median SD score by age for each parameter.
#'
#' @param param Vector identifying each measurement, may be 'WEIGHTKG', or 'HEIGHTCM'.
#' @param agedays Numeric vector containing the age in days at each measurement.
#' @param sex Vector identifying the gender of the subject, may be 'M', 'm', or 0 for males, vs. 'F', 'f' or 1 for females.
#' @param sd.orig Vector of previously calculated standard deviation (SD) scores for each measurement before re-centering.
#'
#' @return Table of data with median SD-scores per day of life by gender and parameter.
#'
#' @export
#' @import data.table
#' @examples
#' # Run on 1 subject
#' df_stats <- as.data.frame(syngrowth)
#' df_stats <- df_stats[df_stats$subjid == df_stats$subjid[1], ]
#'
#' # Get the original standard deviations
#' measurement_to_z <- read.anthro(cdc.only = TRUE)
#' sd.orig <- measurement_to_z(
#'   df_stats$param,
#'   df_stats$agedays,
#'   df_stats$sex,
#'   df_stats$measurement,
#'   TRUE
#' )
#'
#' # Calculate median standard deviations
#' sd.m <- sd.median(
#'   df_stats$param,
#'   df_stats$sex,
#'   df_stats$agedays,
#'   sd.orig
#' )
sd.median <- function(param, sex, agedays, sd.orig) {
  # 3.  SD-score recentering: Because the basis of the method is comparing SD-scores over time, we need to account for the fact that
  #     the mean SD-score for the population changes with age.
  # a.  Determine the median cdc*sd for each parameter by year of age (with sexes combined): median*sd.
  # b.	The median*sd should be considered to apply to midyear-age, defined as the age in days with the same value as the integer
  #     portion of (365.25*year + 365.25/2).
  # c.	Linearly interpolate median*sd for each parameter between each midyear-age, naming the interpolated values rc*sd.
  # d.	For ages below the first midyear-age, let rc*sd equal the median*sd for the earliest year.
  #     For ages above the last midyear_age, let rc*sd equal the median*sd for the last year.
  # e.	Subtract rcsd_* from SDorig to create the recentered SD-score.  This recentered SD-score, labeled tbc*sd
  #     (stands for "to be cleaned") will be used for most of the rest of the analyses.
  # f.	In future steps I will sometimes refer to measprev and measnext which refer to the previous or next wt or ht measurement
  #     for which exc_*==0 for the subject and parameter, when the data are sorted by subject, parameter, and agedays. SDprev and SDnext refer to the tbc*sd of the previous or next measurement.
  dt <- data.table(param, sex, agedays, ageyears = floor(agedays / 365.25), sd.orig)
  setkeyv(dt, c("param", "sex", "agedays"))
  # determine ages (in days) we need to cover from min to max age in years
  agedays.to.cover <- dt[, (floor(min(ageyears) * 365.25):floor(max(ageyears + 1) * 365.25))]
  # group all measurements above age 19 together (copied from CD stata code e-mailed 4/3/15)
  dt[ageyears > 19, ageyears := 19]
  dt.median <- dt[
    !is.na(sd.orig),
    list(sex = c(0, 1), sd.median = rep(median(sd.orig), 2)),
    by = .(param, ageyears)
  ]
  dt.median <- dt.median[,
    list(
      agedays = agedays.to.cover,
      sd.median = approx(
        floor((ageyears + 0.5) * 365.25),
        sd.median,
        xout = agedays.to.cover,
        rule = 2
      )$y
    ),
    by = .(param, sex)
  ]
  setkeyv(dt.median, c("param", "sex", "agedays"))
  return(dt.median)
}
